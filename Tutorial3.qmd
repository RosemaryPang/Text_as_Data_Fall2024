---
title: "Tutorial 3 Web Scraping in R"
pagetitle: Tutorial_3
---

This tutorial walks you through the process of using the package `rvest` to scrape websites using R. rvest (sounds like "harvest", get it?) is the workhorse package for text scraping in R and contains all of the functionality for basic scraping applications. We'll start by loading the package and tidyverse more generally so that we can use many of the tidyverse functions later on.

```{r}
#install.packages("rvest")
library(rvest)
library(tidyverse)
```

# Nicely Formatted Data

Let's start with a simple example. Let's say you are interested in top 2024 movies at the Worldwide Box Office on [this website](https://www.the-numbers.com/box-office-records/worldwide/all-movies/cumulative/released-in-2024). But how can we get this into a format for analysis? Time to waste a lot of hours on manually entering the data or maybe we have some spare funds to hire an undergrad...

Fortunately, that's not true. With `rvest` this is arbitrarily easy. First, we can tell R the webpage we are looking at.

```{r}
# identify the url
url <- "https://www.the-numbers.com/box-office-records/worldwide/all-movies/cumulative/released-in-2024"
```

Of course, we can't just read that webpage in because webpages are filled with all variety of --- from our perspective --- junk.

```{r}
# what happens when we just read that in; it's a full webpage
read_html(url)
```

Yeah, that's not helpful. Instead, we need to identify just the table. This is the hardest part, but it's not really too difficult. If you are familiar with HTML / CSS, then you might be able to just inspect the HTML to identify the correct selector. If you are in a Chrome browser, for instance, you can right click then select "Inspect".

```{r}
css_selector <- "table"
```

Once we have the correct selector, we still need to (1) read the full html, (2) select out the portion that we are interested in, and (3) start formatting. That third step is really particularly easy when what we are pulling is already in tabular format. So, for example, here's how we could read our table of data on famines into R.

```{r}
# pull the table
url %>%
  read_html() %>%
  html_nodes(css = css_selector) %>%
  html_table()
```

The output returns ALL table format data on the webpage. Of course we can select the tibble using

```{r}
alltables <- url %>%
  read_html() %>%
  html_nodes(css = css_selector) %>%
  html_table()

topmovie <- alltables[[2]]  
```

If you are less familiar, you can use SelectorGadget, a Chrome add-on that lets you --- for whatever webpage you are visiting --- "select" the portion of a page you are interested in scraping. SelectorGadget will then highlight whatever the Selector (in the bar at the bottom) would return. Once you've identified the Selector that works for the element of interest to you, you can add that to our code and you are off and running.

Note that, especially if you aren't familiar, this is going to take some time to get used to and require some patience and practice. Take a moment now to install the SelectorGadget Chrome extension, then play around with SelectorGadget on the [website](https://www.the-numbers.com/box-office-records/worldwide/all-movies/cumulative/released-in-2024) until you can correctly identify the CSS selector for the table.

```{r}
# use SelectorGadget to find the info
css_selector <- "#page_filling_chart td , th"

# pull the table
url %>%
  read_html() %>%
  html_node(css = css_selector) %>%
  html_node(xpath = "ancestor::table") %>%
  html_table()
```

That's great, but as you can see it only pulls the first 100 movies. We'd ideally like to have all 216 top movies. To do so, we need to iterate through the pages. This takes some understanding of how the website is set up. Here's the URL we were using

```{r}
url
```

How might we get the second page? The easiest way would be if we could identify some standard language that the website is using that we can then leverage to "loop" through each page (that is, run the same operation as above for each page). To see how the website lays everything out, click through to the second page of movies. Here's the URL for that page, and a comparison of the two URLs.

```{r}
new_url <- "https://www.the-numbers.com/box-office-records/worldwide/all-movies/cumulative/released-in-2024/101"

# print these next to each other
url
new_url
```

Aha! That shows us how they are indexing the pages, with a little statement :"/101". The indexing here is kind of weird but if you click on the next set, you'll see that 101 jumps up to 201. So they are counting by 100 and creating a new page each time. We can use that to set up a loop. Ostensibly, you could identify the total number of pages is 3 pages. The first page is --- as we just saw --- unnumbered, and then each thereafter is indexed by 101. Because the first is unnumbered, we'll subtract one from our number of pages to loop through.

```{r}
# create the indices
pageNumber <- 100 * c(1:2) +1
# get an idea of what we just created
head(pageNumber)
length(pageNumber)

# set up a new vector to store the urls
urls <- url

# loop through the page numbers and create the new urls
for (i in 1:length(pageNumber)){
  urls <- c(urls, paste("https://www.the-numbers.com/box-office-records/worldwide/all-movies/cumulative/released-in-2024/",pageNumber[i], sep = ""))
}

# look at the first few
head(urls)
```

Now that we have all of the urls, we can loop through each pulling the table from each page. To do so, we need to create a new list that will store all of the reviews; otherwise, we'll just be overwriting our object in each loop.

```{r}
# set up an empty vector to store tables
movies <- list()

# loop through urls
for (i in 1:length(urls)){
  # extract table for this url
  movies_single <- urls[i] %>%
  read_html() %>%
  html_node(css = css_selector) %>%
  html_node(xpath = "ancestor::table") %>%
  html_table()

  # store each tibble in the list
  movies[[i]] <- movies_single
}

# combine all tables into one tibble
movies_all <- bind_rows(movies)

movies_all
```

# Less Nicely Formatted Data

Of course, more likely than not we will be encountering data that aren't so nice and tidy. This is particularly true for web scraping when the item of interest is text data, which rarely appear in tables like the one above.

In previous years, we used to scrape Yelp reviews for practice. But we have to stop doing that. Because we want to be polite web scrapers, and Yelp no longer allows web scraping. How did we know?

```{r}
#install.packages("polite")
library(polite)
bow("https://www.yelp.com/biz/hotel-umass-amherst?osq=hotel+umass")
```

Let's try out another website that allows scraping: Rotten Tomatoes.

```{r}
bow("https://www.rottentomatoes.com/m/inside_out_2/reviews")
```

Weâ€™ll look at reviews for Inside Out 2 (where Anxiety, Sadness, and Anger all control our brain whenever web scraping refuses to cooperate).

```{r}
# start by defining the url we want
url <- "https://www.rottentomatoes.com/m/inside_out_2/reviews"

# now define the field we want
css_selector <- ".review-text"

reviews <- url %>%
  read_html() %>%
  html_nodes(css = css_selector) %>%
  html_text()

reviews
```

There are 20 reviews on each page. Unlike the example earlier, where we can click other pages, Rotten Tomatoes has a "LOAD MORE" button. How can we scrape more reviews? Let's go to "RSelenium_RT.r" for more information. 

# A Quick Analysis

With the text scraped from the site, we can use a bit of what we've done in past tutorials (and will continue to do going forward) to take a look at what folks are saying about Inside Out 2.

```{r}
library(quanteda)
library(quanteda.textplots)

# convert to corpus
review <- read_csv("reviews.csv")
review_corpus <- corpus(review$text)

# create a word cloud
review_dfm <- tokens(review_corpus, remove_punct=TRUE) %>%
          tokens_select(pattern=stopwords("en"),
          selection="remove") %>%
          dfm()

textplot_wordcloud(review_dfm)
```

It's hard to find a pattern. Since this is a review about Inside out 2, we find "inside", "new", "2" at the center. We also see common words such as "film", "pixar", "takes", etc. Let's see if we can remove these words.

```{r}
review_dfm2 <- tokens(review_corpus,
                     remove_punct=TRUE,
                     remove_numbers = TRUE) %>%
  tokens_select(pattern=c(stopwords("en"),
          "inside","out","new","sequel","film", "takes", "pixar", "can", "review"),selection="remove") %>%
             dfm()

textplot_wordcloud(review_dfm2)

```

Now the reviews are pretty obvious! 

# Conclusion

Everything here worked well. We'll leave more complicated RSelenium application and using API for another tutorial.
