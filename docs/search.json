[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome Page",
    "section": "",
    "text": "Welcome to DACSS758 Text as Data. All classroom material, including recordings, slides, tutorials, and assignment descriptions, will be posted in Google Classroom. Be sure you are logged into your UMass Google account.\nThis tutorial website contains the R code and output of weekly tutorials.\n Weekly Tutorials \nWeek 1 Introduction\nWeek 2 Text as Data\nWeek 3 Web Scraping in R\nWeek 4 Natural Language Processing\nWeek 5 Preprocessing\nWeek 6 Text Representation I\nWeek 7 Text Representation II"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Tutorial1.html",
    "href": "Tutorial1.html",
    "title": "Tutorial 1 Introduction to R",
    "section": "",
    "text": "In this tutorial, you’ll learn (or refresh your memory) about working in R, how to get your R workspace set up, how to do some basic work with data in R, and we’ll introduce a quick version of some (pretty though not necessarily useful) text analysis.\nBy the end of this tutorial, you should be familiar with the following:\n1. Load packages: install.packages(), library()\n2. Working directories: getwd(), setwd()\n3. Read in some data: read.csv()\n4. Look at data: str(), head(), tail()\n5. Piping: %&gt;% or |&gt;\n6. Manipulate data: group_by, summarise()\n\nBackground\nEach week, the tutorial will include all of the code to demonstrate some of the fundamental aspects of the work we are doing. The tutorials on the website include R code and output. If you would like to execute & edit the code, please download the .qmd file from Google Classroom, and execute & edit the code in RStudio.\nYou’ll do code inside of cells that look like this:\n\nx &lt;- 5\nx + 10\n\n[1] 15\n\n\nYou can run the code inside the cell by hitting the play button in the upper righthand side of the cell. When the code is running, you’ll notice that the play button transforms, indicating that the operation is being performed.\n\n\nFront-end Matters\nR itself has a base set of pre-programmed functions and operations that are included as soon as you open the program up. However, the real magic of R is in the user-contributed packages, which are all freely available online and easy --- generally speaking --- to add on to the base functionality. You can think of these as like the apps you add to your smart phone. While you can do a lot with your smart phone without doing anything, those apps provide a world of different ways of employing your phone. For R, packages serve the same purpose. When you start R up, then, you need to get the packages loaded. If you have never installed the packages before, you need to do that first:\n(since I have these packages already, I put a # mark in front of the code to stop it from running. If you don’t have these packages, make sure you delete the # mark before running the code.)\n\n#install.packages(\"quanteda\")\n#install.packages(\"quanteda.textplots\")\n\nOnce the packages are installed, you can load them as follows:\n\nlibrary(\"quanteda\")\n\nPackage version: 4.0.2\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(\"quanteda.textplots\")\n\nOccasionally, you’ll see conflicts between packages, and notes about a function in one program being masked from another program. For now, don’t worry about this. It arises because different programs will have similar functions with the same name. To return to the metaphor of the phone: it’s a bit like you may have multiple web browsers and you can choose from any of them to surf the web when, say, you click a link from a friend. There are some dangers associated with this, as some of those functions may be named the same thing but may actually do something slightly different. Again, though, this is a topic for another day.\n\n\nWorking Directories\nDirectories are the folders on a computer. Just as you navigate to different folders when you are looking for that ever elusive file that you can’t remember where you saved, you need to tell R where on your computer to look for data or other files. For new programmers, not understanding working directories (or the directory that R is currently working in) is one of the primary time sucks and sources of frustration. You can set the working directory using the drop down menu. You can also always check your working directory by entering getwd() (i.e., “get working directory”), and can change the working directory using setwd(). The latter requires that you enter the specific address of the folder on your machine (so, something like “~” on a Mac, or “c:” on a PC).\n\n\nReading in Data\nEach tutorial will be designed to work with a dataset that I provide and link to. At the most basic level, many packages include datasets as illustrative examples that we can pull on to explore the basics of that package. However, the code that we cover should — generally speaking — work with the data that you aim to work with this semester.\nWe’ll spend a lot of time during the Web Scraping week discussing the import of text data into R. For now, our focus will just be on getting a dataset into R that we can try out a few basic exploratory functions with. To do that, we can call down data directly from the web as so:\n\n# set up temporary file\ntemp &lt;- tempfile()\n\n# download file and store it in the temporary file\ndownload.file(\"http://scdb.wustl.edu/_brickFiles/2020_01/SCDB_2020_01_caseCentered_Citation.csv.zip\", temp)\n\n# the file is compressed, so we need to unzip it first\ndata &lt;- read.csv(unz(temp, \"SCDB_2020_01_caseCentered_Citation.csv\"))\n\n# if that all works, then you can check the number of observations or the number of rows by\nnrow(data)\n\n[1] 9030\n\n\n\n\nLooking at Data\nOnce the data is in, we can start to work with it. There’s really no limit on us from this point on. In the code chunk below, we’ll cover a few of the basics. Now that we are working with data, we’ll load the tidyverse. The tidyverse describes itself as “an opinionated collection of R packages designed for data science.” That’s about right; the packages are designed to improve on the base functionality of R in order to provide a better experience in working with data, conducting analyses, and creating visualizations. (make sure to install tidyverse if you haven’t done so.)\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# convert to tibble, the tidyverse dataframe format\ndata &lt;- as_tibble(data)\n\n# inspect the data\nstr(data)\n\ntibble [9,030 × 53] (S3: tbl_df/tbl/data.frame)\n $ caseId                  : chr [1:9030] \"1946-001\" \"1946-002\" \"1946-003\" \"1946-004\" ...\n $ docketId                : chr [1:9030] \"1946-001-01\" \"1946-002-01\" \"1946-003-01\" \"1946-004-01\" ...\n $ caseIssuesId            : chr [1:9030] \"1946-001-01-01\" \"1946-002-01-01\" \"1946-003-01-01\" \"1946-004-01-01\" ...\n $ voteId                  : chr [1:9030] \"1946-001-01-01-01\" \"1946-002-01-01-01\" \"1946-003-01-01-01\" \"1946-004-01-01-01\" ...\n $ dateDecision            : chr [1:9030] \"11/18/1946\" \"11/18/1946\" \"11/18/1946\" \"11/25/1946\" ...\n $ decisionType            : int [1:9030] 1 1 1 7 1 1 1 1 1 1 ...\n $ usCite                  : chr [1:9030] \"329 U.S. 1\" \"329 U.S. 14\" \"329 U.S. 29\" \"329 U.S. 40\" ...\n $ sctCite                 : chr [1:9030] \"67 S. Ct. 6\" \"67 S. Ct. 13\" \"67 S. Ct. 1\" \"67 S. Ct. 167\" ...\n $ ledCite                 : chr [1:9030] \"91 L. Ed. 3\" \"91 L. Ed. 12\" \"91 L. Ed. 22\" \"91 L. Ed. 29\" ...\n $ lexisCite               : chr [1:9030] \"1946 U.S. LEXIS 1724\" \"1946 U.S. LEXIS 1725\" \"1946 U.S. LEXIS 3037\" \"1946 U.S. LEXIS 1696\" ...\n $ term                    : int [1:9030] 1946 1946 1946 1946 1946 1946 1946 1946 1946 1946 ...\n $ naturalCourt            : int [1:9030] 1301 1301 1301 1301 1301 1301 1301 1301 1301 1301 ...\n $ chief                   : chr [1:9030] \"Vinson\" \"Vinson\" \"Vinson\" \"Vinson\" ...\n $ docket                  : chr [1:9030] \"24\" \"12\" \"21\" \"26\" ...\n $ caseName                : chr [1:9030] \"HALLIBURTON OIL WELL CEMENTING CO. v. WALKER et al., DOING BUSINESS AS DEPTHOGRAPH CO.\" \"CLEVELAND v. UNITED STATES\" \"CHAMPLIN REFINING CO. v. UNITED STATES ET AL.\" \"UNITED STATES v. ALCEA BAND OF TILLAMOOKS ET AL.\" ...\n $ dateArgument            : chr [1:9030] \"1/9/1946\" \"10/10/1945\" \"11/8/1945\" \"1/31/1946\" ...\n $ dateRearg               : chr [1:9030] \"10/23/1946\" \"10/17/1946\" \"10/18/1946\" \"10/25/1946\" ...\n $ petitioner              : int [1:9030] 198 100 209 27 27 198 148 189 4 135 ...\n $ petitionerState         : int [1:9030] NA NA NA NA NA NA NA NA 2 NA ...\n $ respondent              : int [1:9030] 172 27 27 170 176 4 405 189 248 114 ...\n $ respondentState         : int [1:9030] NA NA NA NA NA 6 NA NA NA NA ...\n $ jurisdiction            : int [1:9030] 6 1 2 1 1 2 1 1 1 1 ...\n $ adminAction             : int [1:9030] NA NA 66 67 NA 117 105 24 117 NA ...\n $ adminActionState        : int [1:9030] NA NA NA NA NA 6 NA NA 2 NA ...\n $ threeJudgeFdc           : int [1:9030] 0 0 1 0 0 0 0 0 0 0 ...\n $ caseOrigin              : int [1:9030] 51 123 107 3 3 302 21 54 44 73 ...\n $ caseOriginState         : int [1:9030] 6 52 42 NA NA 6 NA 9 2 21 ...\n $ caseSource              : int [1:9030] 29 30 107 3 3 300 21 23 29 26 ...\n $ caseSourceState         : int [1:9030] NA NA 42 NA NA 6 NA NA NA NA ...\n $ lcDisagreement          : int [1:9030] 0 0 0 0 0 1 0 0 1 0 ...\n $ certReason              : int [1:9030] 11 4 1 10 2 1 10 12 10 10 ...\n $ lcDisposition           : int [1:9030] 2 2 NA NA NA 3 2 5 2 3 ...\n $ lcDispositionDirection  : int [1:9030] 1 1 2 2 2 2 2 2 2 2 ...\n $ declarationUncon        : int [1:9030] 1 1 1 1 1 3 1 1 1 1 ...\n $ caseDisposition         : int [1:9030] 3 2 2 2 3 3 2 5 7 2 ...\n $ caseDispositionUnusual  : int [1:9030] 0 0 0 0 0 0 0 0 0 0 ...\n $ partyWinning            : int [1:9030] 1 0 0 0 1 1 0 1 1 0 ...\n $ precedentAlteration     : int [1:9030] 1 0 0 0 0 0 0 0 0 0 ...\n $ voteUnclear             : int [1:9030] 0 0 0 0 0 0 0 0 0 0 ...\n $ issue                   : int [1:9030] 80180 10500 80250 20150 80060 80100 80120 90200 70190 80030 ...\n $ issueArea               : int [1:9030] 8 1 8 2 8 8 8 9 7 8 ...\n $ decisionDirection       : int [1:9030] 2 1 2 2 2 1 2 1 1 2 ...\n $ decisionDirectionDissent: int [1:9030] 0 0 0 0 0 0 0 0 0 0 ...\n $ authorityDecision1      : int [1:9030] 4 4 1 4 7 2 1 4 4 4 ...\n $ authorityDecision2      : int [1:9030] NA NA NA NA NA NA NA NA 5 NA ...\n $ lawType                 : int [1:9030] 6 6 2 6 NA 1 6 6 5 3 ...\n $ lawSupp                 : int [1:9030] 600 600 207 600 NA 129 600 600 512 307 ...\n $ lawMinor                : chr [1:9030] \"35 U.S.C. \\xa7 33\" \"18 U.S.C. \\xa7 398\" \"\" \"49 Stat. 801\" ...\n $ majOpinWriter           : int [1:9030] 78 81 84 87 78 81 82 87 87 78 ...\n $ majOpinAssigner         : int [1:9030] 78 87 78 87 87 87 74 87 87 87 ...\n $ splitVote               : int [1:9030] 1 1 1 1 1 1 1 1 1 1 ...\n $ majVotes                : int [1:9030] 8 6 5 5 6 7 6 9 9 8 ...\n $ minVotes                : int [1:9030] 1 3 4 3 3 1 0 0 0 0 ...\n\n\nThat’s pretty informative, but we have other ways to look at the data as well. If you want to see just the first few rows, you can use head(), and if you want to see the last few rows, you can use tail().\n\nhead(data)\n\n# A tibble: 6 × 53\n  caseId   docketId caseIssuesId voteId dateDecision decisionType usCite sctCite\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;               &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;  \n1 1946-001 1946-00… 1946-001-01… 1946-… 11/18/1946              1 329 U… 67 S. …\n2 1946-002 1946-00… 1946-002-01… 1946-… 11/18/1946              1 329 U… 67 S. …\n3 1946-003 1946-00… 1946-003-01… 1946-… 11/18/1946              1 329 U… 67 S. …\n4 1946-004 1946-00… 1946-004-01… 1946-… 11/25/1946              7 329 U… 67 S. …\n5 1946-005 1946-00… 1946-005-01… 1946-… 11/25/1946              1 329 U… 67 S. …\n6 1946-006 1946-00… 1946-006-01… 1946-… 11/25/1946              1 329 U… 67 S. …\n# ℹ 45 more variables: ledCite &lt;chr&gt;, lexisCite &lt;chr&gt;, term &lt;int&gt;,\n#   naturalCourt &lt;int&gt;, chief &lt;chr&gt;, docket &lt;chr&gt;, caseName &lt;chr&gt;,\n#   dateArgument &lt;chr&gt;, dateRearg &lt;chr&gt;, petitioner &lt;int&gt;,\n#   petitionerState &lt;int&gt;, respondent &lt;int&gt;, respondentState &lt;int&gt;,\n#   jurisdiction &lt;int&gt;, adminAction &lt;int&gt;, adminActionState &lt;int&gt;,\n#   threeJudgeFdc &lt;int&gt;, caseOrigin &lt;int&gt;, caseOriginState &lt;int&gt;,\n#   caseSource &lt;int&gt;, caseSourceState &lt;int&gt;, lcDisagreement &lt;int&gt;, …\n\ntail(data)\n\n# A tibble: 6 × 53\n  caseId   docketId caseIssuesId voteId dateDecision decisionType usCite sctCite\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;               &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;  \n1 2019-069 2019-06… 2019-069-01… 2019-… 6/15/2020               2 \"\"     140 S.…\n2 2019-070 2019-07… 2019-070-01… 2019-… 6/25/2020               1 \"\"     140 S.…\n3 2019-071 2019-07… 2019-071-01… 2019-… 6/29/2020               1 \"\"     140 S.…\n4 2019-072 2019-07… 2019-072-01… 2019-… 6/22/2020               1 \"\"     140 S.…\n5 2019-073 2019-07… 2019-073-01… 2019-… 7/6/2020                6 \"\"     140 S.…\n6 2019-074 2019-07… 2019-074-01… 2019-… 7/9/2020                1 \"\"     140 S.…\n# ℹ 45 more variables: ledCite &lt;chr&gt;, lexisCite &lt;chr&gt;, term &lt;int&gt;,\n#   naturalCourt &lt;int&gt;, chief &lt;chr&gt;, docket &lt;chr&gt;, caseName &lt;chr&gt;,\n#   dateArgument &lt;chr&gt;, dateRearg &lt;chr&gt;, petitioner &lt;int&gt;,\n#   petitionerState &lt;int&gt;, respondent &lt;int&gt;, respondentState &lt;int&gt;,\n#   jurisdiction &lt;int&gt;, adminAction &lt;int&gt;, adminActionState &lt;int&gt;,\n#   threeJudgeFdc &lt;int&gt;, caseOrigin &lt;int&gt;, caseOriginState &lt;int&gt;,\n#   caseSource &lt;int&gt;, caseSourceState &lt;int&gt;, lcDisagreement &lt;int&gt;, …\n\n\nWe can also start to run basic analyses. Say you are interested in the average size of the majority coalition from a Supreme Court decision, or the mean number of majority votes in Supreme Court cases. We can calculate that as follows:\n\ndata %&gt;% \n  summarise(meanVotes = mean(majVotes))\n\n# A tibble: 1 × 1\n  meanVotes\n      &lt;dbl&gt;\n1      7.10\n\n\nFor folks that have worked with R and the tidyverse before, that should look familiar. But for others, here’s the idea of what we did there. First, we take the dataset (data) and pass it to a function via the pipe operator (%&gt;% or |&gt;). Then, we call a group of potential summary functions through summarise(), create a new variable called meanVotes, and set it’s value equal to the mean() of majVotes via mean(majVotes). We can see from R that the mean is approximately 7.1 during the entire time period under study here. We are probably more interested, though, in how divisiveness changes. For instance, has the divisiveness of the Court increased or decreased over time? We can do that by just adding another line to what we had above, as so:\n\ndata %&gt;% \n  group_by(term) %&gt;% \n  summarise(meanVotes = mean(majVotes))\n\n# A tibble: 74 × 2\n    term meanVotes\n   &lt;int&gt;     &lt;dbl&gt;\n 1  1946      7.20\n 2  1947      6.91\n 3  1948      6.69\n 4  1949      6.56\n 5  1950      6.88\n 6  1951      6.61\n 7  1952      6.79\n 8  1953      6.98\n 9  1954      6.77\n10  1955      7.13\n# ℹ 64 more rows\n\n\nThat’s cool but it’s hard to see exactly what’s happening. Instead, let’s plot it out to have a look. Within the tidyverse, and probably it’s most popular package, is ggplot, or the “Grammar of Graphics” (i.e., gg). The underlying intuition is that we’ll be “layering” graphics; first you create a blank plot, then you start adding features to the plot. Here’s a basic plot for us:\n\n# pass the work we did above to ggplot\ndata %&gt;% \n  group_by(term) %&gt;% \n  summarise(meanVotes = mean(majVotes)) %&gt;%\n  # then create the ggplot \"canvas\", specify our variables, then start \"adding\" (+) layers\n  ggplot(mapping = aes(x=term, y=meanVotes)) +\n    # our first layer creates a scatterplot\n    geom_point() + \n    # our second layer creates a smoothed fit across the points, a sort of moving average to give us a sense of how \n    # things have changed over time\n    geom_smooth() +\n    # our last layer is going to change the default style of the panel to a black and white theme\n    theme_bw()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nWhere We Are Headed\nOf course, all of this is just working with formatted data, variables that others have already coded. What we are really interested in --- and the reason you are taking this class --- is to work with text-as-data. So what does the work that we’ll be doing look like?\nLet’s do a quick example. We’ll use the same piping command as before along with a host of functions from quanteda, the best text-as-data package out there for R. You can learn (a lot) more about quanteda here at the package website. We’ll be working with quanteda throughout the semester, and here’s a quick example of some basics of what we’ll do:\n\n# quanteda comes with a corpus of presidential inaugural speeches\n# this first line subsets that corpus to speeches later than 1953\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &gt;= 1954) |&gt;\n    # notice we are using the piping operator again.\n    # this time, we pipe the corpus to tokens, which is a single unit of text\n    # in creating it, we remove stop words (\"a\", \"it\", \"they\") and punctuation\n  tokens(remove_punct=TRUE) |&gt;\n    # now we pipe it to DFM, which creates a document-feature matrix\n    dfm() |&gt;\n    dfm_remove(stopwords('english')) |&gt;\n    # then we trim words that appear fewer than 10 times\n    dfm_trim(min_termfreq = 10, verbose = FALSE)\n\ntextplot_wordcloud(dfm_inaug)\n\n\n\n\n\n\n\n\nNow you have a nice word cloud! Don’t worry if you are unfamiliar with terms like corpus, document-feature matrix, and stop words. We will be covering and learning about these concepts throughout the semester. Here’s more exciting text analysis!"
  },
  {
    "objectID": "Tutorial2.html",
    "href": "Tutorial2.html",
    "title": "Tutorial 2 Text as Data",
    "section": "",
    "text": "This is our second tutorial for running R. In this tutorial, we’ll start working with texts. As we discussed in class, there’s a huge variance in what counts as a “text”, running the gamut from sentences or tweets to entire novels like War & Peace.\nIn this tutorial, you’ll learn how to start working with text-as-data in R. That includes storage formats, manipulation, counting, subsetting, and editing of the texts. There’s a lot we can do once we have the texts in R!\nBy the end of this tutorial, you should be familiar with the following:\n1. Character vectors: c()\n2. Corpus: corpus(), summary()\n3. Metadata: docvars()\n4. Subsetting corpora: corpus_subset()\n5. Number of documents: ndoc()\n6. Tokenization: tokens()\n7. Contextual analysis: kwic()\n8. N-grams: tokens_ngrams()"
  },
  {
    "objectID": "Tutorial2.html#front-end-matters",
    "href": "Tutorial2.html#front-end-matters",
    "title": "Tutorial 2 Text as Data",
    "section": "Front-end Matters",
    "text": "Front-end Matters\nThis week, we’ll start by looking at the Harry Potter series. First things first, we need to install and load the packages for today’s notebook.\n\nlibrary(tidytext)\nlibrary(plyr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::arrange()   masks plyr::arrange()\n✖ purrr::compact()   masks plyr::compact()\n✖ dplyr::count()     masks plyr::count()\n✖ dplyr::desc()      masks plyr::desc()\n✖ dplyr::failwith()  masks plyr::failwith()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::id()        masks plyr::id()\n✖ dplyr::lag()       masks stats::lag()\n✖ dplyr::mutate()    masks plyr::mutate()\n✖ dplyr::rename()    masks plyr::rename()\n✖ dplyr::summarise() masks plyr::summarise()\n✖ dplyr::summarize() masks plyr::summarize()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(quanteda)\n\nPackage version: 4.0.2\nUnicode version: 14.0\nICU version: 71.1\nParallel computing: disabled\nSee https://quanteda.io for tutorials and examples."
  },
  {
    "objectID": "Tutorial2.html#character-vectors",
    "href": "Tutorial2.html#character-vectors",
    "title": "Tutorial 2 Text as Data",
    "section": "Character Vectors",
    "text": "Character Vectors\nNow you should see the seven books our workspace/environment. These are:\n1. philosophers_stone: Harry Potter and the Philosophers Stone (1997)\n2. chamber_of_secrets: Harry Potter and the Chamber of Secrets (1998)\n3. prisoner_of_azkaban: Harry Potter and the Prisoner of Azkaban (1999)\n4. goblet_of_fire: Harry Potter and the Goblet of Fire (2000)\n5. order_of_the_phoenix: Harry Potter and the Order of the Phoenix\n6. half_blood_price: Harry Potter and the Half-Blood Prince (2005)\n7. deathly_hallows: Harry Potter and the Deathly Hallows (2007)\nEach is stored as a character vector. A character vector is a collection of elements, where each element is a string. You could create your own character vector as something like:\n\nmy_char_vec &lt;- c(\"Rosemary's\", \"favorite\",\"horror movie\",\"director\", \"is\", \"James Wan.\")\nprint(my_char_vec)\n\n[1] \"Rosemary's\"   \"favorite\"     \"horror movie\" \"director\"     \"is\"          \n[6] \"James Wan.\"  \n\n\nEach element of a vector has an index; starting at 1, count from left-to-right. You can call to particular elements from the character vector using that indexing. So, if I wanted the third element from the above, I could type:\n\nmy_char_vec[3]\n\n[1] \"horror movie\"\n\n\nAs you can see in the example, an element doesn’t have to be a single word. Each element can be as long as you like. For instance, a character vector could also be:\n\nmy_char_vec2 &lt;- c(\"Rosemary's favorite horror movie director is James Wan.\", \"She has a James Wan figure in her office.\", \"She likes the Conjuring series the most.\")\nprint(my_char_vec2)\n\n[1] \"Rosemary's favorite horror movie director is James Wan.\"\n[2] \"She has a James Wan figure in her office.\"              \n[3] \"She likes the Conjuring series the most.\"               \n\n\nThe storage of the Harry Potter books follows this intuition. Each book is a character vector, and each chapter is an element in that book’s character vector. So, for philosophers_stone, we can see the first chapter via:\n\nphilosophers_stone[1]\n\n[1] \"THE BOY WHO LIVED  Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.  Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.  The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursley's sister, but they hadn't met for several years; in fact, Mrs. Dursley pretended she didn't have a sister, because her sister and her good-for-nothing husband were as unDursleyish as it was possible to be. The Dursleys shuddered to think what the neighbors would say if the Potters arrived in the street. The Dursleys knew that the Potters had a small son, too, but they had never even seen him. This boy was another good reason for keeping the Potters away; they didn't want Dudley mixing with a child like that.  When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story starts, there was nothing about the cloudy sky outside to suggest that strange and mysterious things would soon be happening all over the country. Mr. Dursley hummed as he picked out his most boring tie for work, and Mrs. Dursley gossiped away happily as she wrestled a screaming Dudley into his high chair.  None of them noticed a large, tawny owl flutter past the window.  At half past eight, Mr. Dursley picked up his briefcase, pecked Mrs. Dursley on the cheek, and tried to kiss Dudley good-bye but missed, because Dudley was now having a tantrum and throwing his cereal at the walls. `Little tyke,` chortled Mr. Dursley as he left the house. He got into his car and backed out of number four's drive.  It was on the corner of the street that he noticed the first sign of something peculiar -- a cat reading a map. For a second, Mr. Dursley didn't realize what he had seen -- then he jerked his head around to look again. There was a tabby cat standing on the corner of Privet Drive, but there wasn't a map in sight. What could he have been thinking of? It must have been a trick of the light. Mr. Dursley blinked and stared at the cat. It stared back. As Mr. Dursley drove around the corner and up the road, he watched the cat in his mirror. It was now reading the sign that said Privet Drive -- no, looking at the sign; cats couldn't read maps or signs. Mr. Dursley gave himself a little shake and put the cat out of his mind. As he drove toward town he thought of nothing except a large order of drills he was hoping to get that day.  But on the edge of town, drills were driven out of his mind by something else. As he sat in the usual morning traffic jam, he couldn't help noticing that there seemed to be a lot of strangely dressed people about. People in cloaks. Mr. Dursley couldn't bear people who dressed in funny clothes -- the getups you saw on young people! He supposed this was some stupid new fashion. He drummed his fingers on the steering wheel and his eyes fell on a huddle of these weirdos standing quite close by. They were whispering excitedly together. Mr. Dursley was enraged to see that a couple of them weren't young at all; why, that man had to be older than he was, and wearing an emerald-green cloak! The nerve of him! But then it struck Mr. Dursley that this was probably some silly stunt -- these people were obviously collecting for something... yes, that would be it. The traffic moved on and a few minutes later, Mr. Dursley arrived in the Grunnings parking lot, his mind back on drills.  Mr. Dursley always sat with his back to the window in his office on the ninth floor. If he hadn't, he might have found it harder to concentrate on drills that morning. He didn't see the owls swoop ing past in broad daylight, though people down in the street did; they pointed and gazed open- mouthed as owl after owl sped overhead. Most of them had never seen an owl even at nighttime. Mr. Dursley, however, had a perfectly normal, owl-free morning. He yelled at five different people. He made several important telephone calls and shouted a bit more. He was in a very good mood until lunchtime, when he thought he'd stretch his legs and walk across the road to buy himself a bun from the bakery. He'd forgotten all about the people in cloaks until he passed a group of them next to the baker's. He eyed them angrily as he passed. He didn't know why, but they made him uneasy. This bunch were whispering excitedly, too, and he couldn't see a single collecting tin. It was on his way back past them, clutching a large doughnut in a bag, that he caught a few words of what they were saying.   `The Potters, that's right, that's what I heard yes, their son, Harry`  Mr. Dursley stopped dead. Fear flooded him. He looked back at the whisperers as if he wanted to say something to them, but thought better of it.  He dashed back across the road, hurried up to his office, snapped at his secretary not to disturb him, seized his telephone, and had almost finished dialing his home number when he changed his mind. He put the receiver back down and stroked his mustache, thinking... no, he was being stupid. Potter wasn't such an unusual name. He was sure there were lots of people called Potter who had a son called Harry. Come to think of it, he wasn't even sure his nephew was called Harry. He'd never even seen the boy. It might have been Harvey. Or Harold. There was no point in worrying Mrs. Dursley; she always got so upset at any mention of her sister. He didn't blame her -- if he'd had a sister like that... but all the same, those people in cloaks...  He found it a lot harder to concentrate on drills that afternoon and when he left the building at five o'clock, he was still so worried that he walked straight into someone just outside the door.  `Sorry,` he grunted, as the tiny old man stumbled and almost fell. It was a few seconds before Mr. Dursley realized that the man was wearing a violet cloak. He didn't seem at all upset at being almost knocked to the ground. On the contrary, his face split into a wide smile and he said in a squeaky voice that made passersby stare, `Don't be sorry, my dear sir, for nothing could upset me today! Rejoice, for You-Know-Who has gone at last! Even Muggles like yourself should be celebrating, this happy, happy day!`  And the old man hugged Mr. Dursley around the middle and walked off.  Mr. Dursley stood rooted to the spot. He had been hugged by a complete stranger. He also thought he had been called a Muggle, whatever that was. He was rattled. He hurried to his car and set off for home, hoping he was imagining things, which he had never hoped before, because he didn't approve of imagination.  As he pulled into the driveway of number four, the first thing he saw -- and it didn't improve his mood -- was the tabby cat he'd spotted that morning. It was now sitting on his garden wall. He was sure it was the same one; it had the same markings around its eyes. `Shoo!` said Mr. Dursley loudly. The cat didn't move. It just gave him a stern look. Was this normal cat behavior? Mr. Dursley wondered. Trying to pull himself together, he let himself into the house. He was still determined not to mention anything to his wife. Mrs. Dursley had had a nice, normal day. She told him over dinner all about Mrs. Next Door's problems with her daughter and how Dudley had learned a new word (`Won't!`). Mr. Dursley tried to act normally. When Dudley had been put to bed, he went into the living room in time to catch the last report on the evening news:  `And finally, bird-watchers everywhere have reported that the nation's owls have been behaving very unusually today. Although owls normally hunt at night and are hardly ever seen in daylight, there have been hundreds of sightings of these birds flying in every direction since sunrise. Experts are unable to explain why the owls have suddenly changed their sleeping pattern.` The newscaster allowed himself a grin. `Most mysterious. And now, over to Jim McGuffin with the weather. Going to be any more showers of owls tonight, Jim?`  `Well, Ted,` said the weatherman, `I don't know about that, but it's not only the owls that have been acting oddly today. Viewers as far apart as Kent, Yorkshire, and Dundee have been phoning in to tell me that instead of the rain I promised yesterday, they've had a downpour of shooting stars! Perhaps people have been celebrating Bonfire Night early -- it's not until next week, folks! But I can promise a wet night tonight.`  Mr. Dursley sat frozen in his armchair. Shooting stars all over Britain? Owls flying by daylight? Mysterious people in cloaks all over the place? And a whisper, a whisper about the Potters...  Mrs. Dursley came into the living room carrying two cups of tea. It was no good. He'd have to say something to her. He cleared his throat nervously. `Er -- Petunia, dear -- you haven't heard from your sister lately, have you?` As he had expected, Mrs. Dursley looked shocked and angry. After all, they normally pretended she didn't have a sister.  `No,` she said sharply. `Why?`  `Funny stuff on the news,` Mr. Dursley mumbled. `Owls... shooting stars... and there were a lot of funny-looking people in town today...`  `So?` snapped Mrs. Dursley.  `Well, I just thought... maybe... it was something to do with... you know... her crowd.` Mrs. Dursley sipped her tea through pursed lips. Mr. Dursley wondered whether he dared tell her he'd heard the name `Potter.` He decided he didn't dare. Instead he said, as casually as he could, `Their son -- he'd be about Dudley's age now, wouldn't he?`  `I suppose so,` said Mrs. Dursley stiffly.  `What's his name again? Howard, isn't it?`  `Harry. Nasty, common name, if you ask me.`  `Oh, yes,` said Mr. Dursley, his heart sinking horribly. `Yes, I quite agree.` He didn't say another word on the subject as they went upstairs to bed. While Mrs. Dursley was in the bathroom, Mr. Dursley crept to the bedroom window and peered down into the front garden. The cat was still there. It was staring down Privet Drive as though it were waiting for something.  Was he imagining things? Could all this have anything to do with the Potters? If it did... if it got out that they were related to a pair of -- well, he didn't think he could bear it.  The Dursleys got into bed. Mrs. Dursley fell asleep quickly but Mr. Dursley lay awake, turning it all over in his mind. His last, comforting thought before he fell asleep was that even if the Potters were involved, there was no reason for them to come near him and Mrs. Dursley. The Potters knew very well what he and Petunia thought about them and their kind.... He couldn't see how he and Petunia could get mixed up in anything that might be going on -- he yawned and turned over -- it couldn't affect them....  How very wrong he was.  Mr. Dursley might have been drifting into an uneasy sleep, but the cat on the wall outside was showing no sign of sleepiness. It was sitting as still as a statue, its eyes fixed unblinkingly on the far corner of Privet Drive. It didn't so much as quiver when a car door slammed on the next street, nor when two owls swooped overhead. In fact, it was nearly midnight before the cat moved at all.  A man appeared on the corner the cat had been watching, appeared so suddenly and silently you'd have thought he'd just popped out of the ground. The cat's tail twitched and its eyes narrowed.  Nothing like this man had ever been seen on Privet Drive. He was tall, thin, and very old, judging by the silver of his hair and beard, which were both long enough to tuck into his belt. He was wearing long robes, a purple cloak that swept the ground, and high-heeled, buckled boots. His blue eyes were light, bright, and sparkling behind half-moon spectacles and his nose was very long and crooked, as though it had been broken at least twice. This man's name was Albus Dumbledore.  Albus Dumbledore didn't seem to realize that he had just arrived in a street where everything from his name to his boots was unwelcome. He was busy rummaging in his cloak, looking for something. But he did seem to realize he was being watched, because he looked up suddenly at the cat, which was still staring at him from the other end of the street. For some reason, the sight of the cat seemed to amuse him. He chuckled and muttered, `I should have known.` He found what he was looking for in his inside pocket. It seemed to be a silver cigarette lighter. He flicked it open, held it up in the air, and clicked it. The nearest street lamp went out with a little pop. He clicked it again -- the next lamp flickered into darkness. Twelve times he clicked the Put-Outer, until the only lights left on the whole street were two tiny pinpricks in the distance, which were the eyes of the cat watching him. If anyone looked out of their window now, even beady-eyed Mrs. Dursley, they wouldn't be able to see anything that was happening down on the pavement. Dumbledore slipped the Put-Outer back inside his cloak and set off down the street toward number four, where he sat down on the wall next to the cat. He didn't look at it, but after a moment he spoke to it.  `Fancy seeing you here, Professor McGonagall.`  He turned to smile at the tabby, but it had gone. Instead he was smiling at a rather severe-looking woman who was wearing square glasses exactly the shape of the markings the cat had had around its eyes. She, too, was wearing a cloak, an emerald one. Her black hair was drawn into a tight bun. She looked distinctly ruffled.  `How did you know it was me?` she asked.  `My dear Professor, I 've never seen a cat sit so stiffly.`  `You'd be stiff if you'd been sitting on a brick wall all day,` said Professor McGonagall.  `All day? When you could have been celebrating? I must have passed a dozen feasts and parties on my way here.`  Professor McGonagall sniffed angrily. `Oh yes, everyone's celebrating, all right,` she said impatiently. `You'd think they'd be a bit more careful, but no -- even the Muggles have noticed something's going on. It was on their news.` She jerked her head back at the Dursleys' dark living-room window. `I heard it. Flocks of owls... shooting stars.... Well, they're not completely stupid. They were bound to notice something. Shooting stars down in Kent -- I'll bet that was Dedalus Diggle. He never had much sense.`  `You can't blame them,` said Dumbledore gently. `We've had precious little to celebrate for eleven years.`  `I know that,` said Professor McGonagall irritably. `But that's no reason to lose our heads. People are being downright careless, out on the streets in broad daylight, not even dressed in Muggle clothes, swapping rumors.`  She threw a sharp, sideways glance at Dumbledore here, as though hoping he was going to tell her something, but he didn't, so she went on. `A fine thing it would be if, on the very day YouKnow-Who seems to have disappeared at last, the Muggles found out about us all. I suppose he really has gone, Dumbledore?` `It certainly seems so,` said Dumbledore. `We have much to be thankful for. Would you care for a lemon drop?`  `A what?` `A lemon drop. They're a kind of Muggle sweet I'm rather fond of`  `No, thank you,` said Professor McGonagall coldly, as though she didn't think this was the moment for lemon drops. `As I say, even if You-Know-Who has gone -`  `My dear Professor, surely a sensible person like yourself can call him by his name? All this 'You- Know-Who' nonsense -- for eleven years I have been trying to persuade people to call him by his proper name: Voldemort.` Professor McGonagall flinched, but Dumbledore, who was unsticking two lemon drops, seemed not to notice. `It all gets so confusing if we keep saying 'You-Know-Who.' I have never seen any reason to be frightened of saying Voldemort's name.  `I know you haven 't, said Professor McGonagall, sounding half exasperated, half admiring. `But you're different. Everyone knows you're the only one You-Know- oh, all right, Voldemort, was frightened of.`  `You flatter me,` said Dumbledore calmly. `Voldemort had powers I will never have.`  `Only because you're too -- well -- noble to use them.`  `It's lucky it's dark. I haven't blushed so much since Madam Pomfrey told me she liked my new earmuffs.`  Professor McGonagall shot a sharp look at Dumbledore and said, `The owls are nothing next to the rumors that are flying around. You know what everyone's saying? About why he's disappeared? About what finally stopped him?`  It seemed that Professor McGonagall had reached the point she was most anxious to discuss, the real reason she had been waiting on a cold, hard wall all day, for neither as a cat nor as a woman had she fixed Dumbledore with such a piercing stare as she did now. It was plain that whatever `everyone` was saying, she was not going to believe it until Dumbledore told her it was true. Dumbledore, however, was choosing another lemon drop and did not answer.  `What they're saying,` she pressed on, `is that last night Voldemort turned up in Godric's Hollow. He went to find the Potters. The rumor is that Lily and James Potter are -- are -- that they're -- dead. ` Dumbledore bowed his head. Professor McGonagall gasped.  `Lily and James... I can't believe it... I didn't want to believe it... Oh, Albus...` Dumbledore reached out and patted her on the shoulder. `I know... I know...` he said heavily.  Professor McGonagall's voice trembled as she went on. `That's not all. They're saying he tried to kill the Potter's son, Harry. But -- he couldn't. He couldn't kill that little boy. No one knows why, or how, but they're saying that when he couldn't kill Harry Potter, Voldemort's power somehow broke -- and that's why he's gone.  Dumbledore nodded glumly.  `It's -- it's true?` faltered Professor McGonagall. `After all he's done... all the people he's killed... he couldn't kill a little boy? It's just astounding... of all the things to stop him... but how in the name of heaven did Harry survive?`  `We can only guess,` said Dumbledore. `We may never know.`  Professor McGonagall pulled out a lace handkerchief and dabbed at her eyes beneath her spectacles. Dumbledore gave a great sniff as he took a golden watch from his pocket and examined it. It was a very odd watch. It had twelve hands but no numbers; instead, little planets were moving around the edge. It must have made sense to Dumbledore, though, because he put it back in his pocket and said, `Hagrid's late. I suppose it was he who told you I'd be here, by the way?`  `Yes,` said Professor McGonagall. `And I don't suppose you're going to tell me why you're here, of all places?`  `I've come to bring Harry to his aunt and uncle. They're the only family he has left now.`  `You don't mean -- you can't mean the people who live here?` cried Professor McGonagall, jumping to her feet and pointing at number four. `Dumbledore -- you can't. I've been watching them all day. You couldn't find two people who are less like us. And they've got this son -- I saw him kicking his mother all the way up the street, screaming for sweets. Harry Potter come and live here!`  `It's the best place for him,` said Dumbledore firmly. `His aunt and uncle will be able to explain everything to him when he's older. I've written them a letter.`  `A letter?` repeated Professor McGonagall faintly, sitting back down on the wall. `Really, Dumbledore, you think you can explain all this in a letter? These people will never understand him! He'll be famous -- a legend -- I wouldn't be surprised if today was known as Harry Potter day in the future -- there will be books written about Harry -- every child in our world will know his name!`  `Exactly,` said Dumbledore, looking very seriously over the top of his half-moon glasses. `It would be enough to turn any boy's head. Famous before he can walk and talk! Famous for something he won't even remember! CarA you see how much better off he'll be, growing up away from all that until he's ready to take it?` Professor McGonagall opened her mouth, changed her mind, swallowed, and then said, `Yes -- yes, you're right, of course. But how is the boy getting here, Dumbledore?` She eyed his cloak suddenly as though she thought he might be hiding Harry underneath it.  `Hagrid's bringing him.` `You think it -- wise -- to trust Hagrid with something as important as this?`  I would trust Hagrid with my life,` said Dumbledore.  `I'm not saying his heart isn't in the right place,` said Professor McGonagall grudgingly, `but you can't pretend he's not careless. He does tend to -- what was that?`  A low rumbling sound had broken the silence around them. It grew steadily louder as they looked up and down the street for some sign of a headlight; it swelled to a roar as they both looked up at the sky -- and a huge motorcycle fell out of the air and landed on the road in front of them.  If the motorcycle was huge, it was nothing to the man sitting astride it. He was almost twice as tall as a normal man and at least five times as wide. He looked simply too big to be allowed, and so wild - long tangles of bushy black hair and beard hid most of his face, he had hands the size of trash can lids, and his feet in their leather boots were like baby dolphins. In his vast, muscular arms he was holding a bundle of blankets.  `Hagrid,` said Dumbledore, sounding relieved. `At last. And where did you get that motorcycle?`  `Borrowed it, Professor Dumbledore, sit,` said the giant, climbing carefully off the motorcycle as he spoke. `Young Sirius Black lent it to me. I've got him, sir.`  `No problems, were there?`  `No, sir -- house was almost destroyed, but I got him out all right before the Muggles started swarmin' around. He fell asleep as we was flyin' over Bristol.`  Dumbledore and Professor McGonagall bent forward over the bundle of blankets. Inside, just visible, was a baby boy, fast asleep. Under a tuft of jet-black hair over his forehead they could see a curiously shaped cut, like a bolt of lightning.  `Is that where -?` whispered Professor McGonagall.  `Yes,` said Dumbledore. `He'll have that scar forever.`  `Couldn't you do something about it, Dumbledore?`  `Even if I could, I wouldn't. Scars can come in handy. I have one myself above my left knee that is a perfect map of the London Underground. Well -- give him here, Hagrid -- we'd better get this over with.`  Dumbledore took Harry in his arms and turned toward the Dursleys' house.  `Could I -- could I say good-bye to him, sir?` asked Hagrid. He bent his great, shaggy head over Harry and gave him what must have been a very scratchy, whiskery kiss. Then, suddenly, Hagrid let out a howl like a wounded dog.  `Shhh!` hissed Professor McGonagall, `you'll wake the Muggles!`  `S-s-sorry,` sobbed Hagrid, taking out a large, spotted handkerchief and burying his face in it. `But I c-c-can't stand it -- Lily an' James dead -- an' poor little Harry off ter live with Muggles -`  `Yes, yes, it's all very sad, but get a grip on yourself, Hagrid, or we'll be found,` Professor McGonagall whispered, patting Hagrid gingerly on the arm as Dumbledore stepped over the low garden wall and walked to the front door. He laid Harry gently on the doorstep, took a letter out of his cloak, tucked it inside Harry's blankets, and then came back to the other two. For a full minute the three of them stood and looked at the little bundle; Hagrid's shoulders shook, Professor McGonagall blinked furiously, and the twinkling light that usually shone from Dumbledore's eyes seemed to have gone out. `Well,` said Dumbledore finally, `that's that. We've no business staying here. We may as well go and join the celebrations.`  `Yeah,` said Hagrid in a very muffled voice, `I'll be takin' Sirius his bike back. G'night, Professor McGonagall -- Professor Dumbledore, sir.`  Wiping his streaming eyes on his jacket sleeve, Hagrid swung himself onto the motorcycle and kicked the engine into life; with a roar it rose into the air and off into the night.  `I shall see you soon, I expect, Professor McGonagall,` said Dumbledore, nodding to her. Professor McGonagall blew her nose in reply.  Dumbledore turned and walked back down the street. On the corner he stopped and took out the silver Put-Outer. He clicked it once, and twelve balls of light sped back to their street lamps so that Privet Drive glowed suddenly orange and he could make out a tabby cat slinking around the corner at the other end of the street. He could just see the bundle of blankets on the step of number four.  `Good luck, Harry,` he murmured. He turned on his heel and with a swish of his cloak, he was gone.  A breeze ruffled the neat hedges of Privet Drive, which lay silent and tidy under the inky sky, the very last place you would expect astonishing things to happen. Harry Potter rolled over inside his blankets without waking up. One small hand closed on the letter beside him and he slept on, not knowing he was special, not knowing he was famous, not knowing he would be woken in a few hours' time by Mrs. Dursley's scream as she opened the front door to put out the milk bottles, nor that he would spend the next few weeks being prodded and pinched by his cousin Dudley... He couldn't know that at this very moment, people meeting in secret all over the country were holding up their glasses and saying in hushed voices: `To Harry Potter -- the boy who lived!\""
  },
  {
    "objectID": "Tutorial2.html#corpus",
    "href": "Tutorial2.html#corpus",
    "title": "Tutorial 2 Text as Data",
    "section": "Corpus",
    "text": "Corpus\nWhile this is interesting, we want something that’s more straightforward to work with. Therefore, we are going to convert the character vectors to a corpus. A corpus is a stored collection of texts; we can also store corpus meta-data as a dataframe associated with the corpus. This is particularly helpful when we have document-level covariates that we might want to use in analysis of the texts.\n\nphilosophers_stone_corpus &lt;- corpus(philosophers_stone)\nphilosophers_stone_summary &lt;- summary(philosophers_stone_corpus)\nphilosophers_stone_summary\n\nCorpus consisting of 17 documents, showing 17 documents:\n\n   Text Types Tokens Sentences\n  text1  1271   5693       350\n  text2  1066   4154       237\n  text3  1226   4656       297\n  text4  1193   4832       322\n  text5  1818   8446       563\n  text6  1563   8016       566\n  text7  1374   5487       351\n  text8  1095   3608       198\n  text9  1422   6195       411\n text10  1293   5237       334\n text11  1110   4215       277\n text12  1509   6790       447\n text13  1076   3953       262\n text14  1110   4394       308\n text15  1385   6486       459\n text16  1581   8357       591\n text17  1489   7172       506\n\n# how to access to \"Tokens\" in the summary?\nphilosophers_stone_summary$Tokens\n\n [1] 5693 4154 4656 4832 8446 8016 5487 3608 6195 5237 4215 6790 3953 4394 6486\n[16] 8357 7172\n\n# Which chapters have less than 5000 words?\nwhich(philosophers_stone_summary$Tokens &lt; 5000)\n\n[1]  2  3  4  8 11 13 14\n\n\nNotice that each element from the character vector has been treated as a unique text; that is, each chapter is being treated as a separate text. The summary() function provides a breakdown of some basic statistics on each chapter then. Text is an automatically created unique identifier for each text, Types is the number of unique words/tokens in the text, Tokens is the total number of words/tokens in the text (i.e., the length of the chapter), and Sentences is the number of sentences in the chapter."
  },
  {
    "objectID": "Tutorial2.html#metadata",
    "href": "Tutorial2.html#metadata",
    "title": "Tutorial 2 Text as Data",
    "section": "Metadata",
    "text": "Metadata\nFor each book, we don’t have much in the way of metadata. However, this summary gives us a start and is something we can use to add metadata to the corpus we’ve created.\n\n# check for metadata; shouldn't see any\ndocvars(philosophers_stone_corpus)\n\ndata frame with 0 columns and 17 rows\n\n\n\n# add an indicator for the book; this will be useful later when we add all the books together into a single corpus\nphilosophers_stone_summary$book &lt;- \"Philosopher's Stone\"\nphilosophers_stone_summary\n\nCorpus consisting of 17 documents, showing 17 documents:\n\n   Text Types Tokens Sentences                book\n  text1  1271   5693       350 Philosopher's Stone\n  text2  1066   4154       237 Philosopher's Stone\n  text3  1226   4656       297 Philosopher's Stone\n  text4  1193   4832       322 Philosopher's Stone\n  text5  1818   8446       563 Philosopher's Stone\n  text6  1563   8016       566 Philosopher's Stone\n  text7  1374   5487       351 Philosopher's Stone\n  text8  1095   3608       198 Philosopher's Stone\n  text9  1422   6195       411 Philosopher's Stone\n text10  1293   5237       334 Philosopher's Stone\n text11  1110   4215       277 Philosopher's Stone\n text12  1509   6790       447 Philosopher's Stone\n text13  1076   3953       262 Philosopher's Stone\n text14  1110   4394       308 Philosopher's Stone\n text15  1385   6486       459 Philosopher's Stone\n text16  1581   8357       591 Philosopher's Stone\n text17  1489   7172       506 Philosopher's Stone\n\n\n\n# create a chapter indicator\nphilosophers_stone_summary$chapter &lt;- as.numeric(str_extract(philosophers_stone_summary$Text, \"[0-9]+\"))\nphilosophers_stone_summary\n\nCorpus consisting of 17 documents, showing 17 documents:\n\n   Text Types Tokens Sentences                book chapter\n  text1  1271   5693       350 Philosopher's Stone       1\n  text2  1066   4154       237 Philosopher's Stone       2\n  text3  1226   4656       297 Philosopher's Stone       3\n  text4  1193   4832       322 Philosopher's Stone       4\n  text5  1818   8446       563 Philosopher's Stone       5\n  text6  1563   8016       566 Philosopher's Stone       6\n  text7  1374   5487       351 Philosopher's Stone       7\n  text8  1095   3608       198 Philosopher's Stone       8\n  text9  1422   6195       411 Philosopher's Stone       9\n text10  1293   5237       334 Philosopher's Stone      10\n text11  1110   4215       277 Philosopher's Stone      11\n text12  1509   6790       447 Philosopher's Stone      12\n text13  1076   3953       262 Philosopher's Stone      13\n text14  1110   4394       308 Philosopher's Stone      14\n text15  1385   6486       459 Philosopher's Stone      15\n text16  1581   8357       591 Philosopher's Stone      16\n text17  1489   7172       506 Philosopher's Stone      17\n\n\nNow we can assign these to the corpus as document-level metadata as follows:\n\ndocvars(philosophers_stone_corpus) &lt;- philosophers_stone_summary\ndocvars(philosophers_stone_corpus)\n\n     Text Types Tokens Sentences                book chapter\n1   text1  1271   5693       350 Philosopher's Stone       1\n2   text2  1066   4154       237 Philosopher's Stone       2\n3   text3  1226   4656       297 Philosopher's Stone       3\n4   text4  1193   4832       322 Philosopher's Stone       4\n5   text5  1818   8446       563 Philosopher's Stone       5\n6   text6  1563   8016       566 Philosopher's Stone       6\n7   text7  1374   5487       351 Philosopher's Stone       7\n8   text8  1095   3608       198 Philosopher's Stone       8\n9   text9  1422   6195       411 Philosopher's Stone       9\n10 text10  1293   5237       334 Philosopher's Stone      10\n11 text11  1110   4215       277 Philosopher's Stone      11\n12 text12  1509   6790       447 Philosopher's Stone      12\n13 text13  1076   3953       262 Philosopher's Stone      13\n14 text14  1110   4394       308 Philosopher's Stone      14\n15 text15  1385   6486       459 Philosopher's Stone      15\n16 text16  1581   8357       591 Philosopher's Stone      16\n17 text17  1489   7172       506 Philosopher's Stone      17\n\n\nThese document variables can be really useful when we want to subset the corpus to some specific level. With just one book, it doesn’t make a lot of sense to subset right now. But the intuition works later, so let’s look at what we’d do if we wanted to, say, look at only chapters with fewer than 5,000 tokens.\n\nsmall_corpus &lt;- corpus_subset(philosophers_stone_corpus, Tokens &lt; 5000)\nsummary(small_corpus)\n\nCorpus consisting of 7 documents, showing 7 documents:\n\n   Text Types Tokens Sentences   Text Types Tokens Sentences\n  text2  1066   4154       237  text2  1066   4154       237\n  text3  1226   4656       297  text3  1226   4656       297\n  text4  1193   4832       322  text4  1193   4832       322\n  text8  1095   3608       198  text8  1095   3608       198\n text11  1110   4215       277 text11  1110   4215       277\n text13  1076   3953       262 text13  1076   3953       262\n text14  1110   4394       308 text14  1110   4394       308\n                book chapter\n Philosopher's Stone       2\n Philosopher's Stone       3\n Philosopher's Stone       4\n Philosopher's Stone       8\n Philosopher's Stone      11\n Philosopher's Stone      13\n Philosopher's Stone      14\n\n\nChapters offer a natural unit for analysis here. However, we may want to reshape the level of analysis that we are conducting, perhaps moving from the chapter level to the paragraph or sentence level.\n\n# the number of documents (chapters) in our small corpus\nndoc(small_corpus)\n\n[1] 7\n\n# the command to reshape our corpus to the sentence level\nsmall_corpus_sentences &lt;- corpus_reshape(small_corpus, to = \"sentences\")\n# can also be to = \"sentences\", \"paragraphs\", or \"documents\"\n\n#the number of documents (sentences) in our reshaped corpus\nndoc(small_corpus_sentences)\n\n[1] 1901\n\n# a summary of the first 5 texts in the sentence-level corpus\nsummary(small_corpus_sentences, n = 5)\n\nCorpus consisting of 1901 documents, showing 5 documents:\n\n    Text Types Tokens Sentences  Text Types Tokens Sentences\n text2.1    29     32         1 text2  1066   4154       237\n text2.2    45     57         1 text2  1066   4154       237\n text2.3    13     14         1 text2  1066   4154       237\n text2.4    55     70         1 text2  1066   4154       237\n text2.5    17     17         1 text2  1066   4154       237\n                book chapter\n Philosopher's Stone       2\n Philosopher's Stone       2\n Philosopher's Stone       2\n Philosopher's Stone       2\n Philosopher's Stone       2\n\n\nSo we have gone from 7 documents (chapters) to 1,898 documents (sentences). The summary provides some unique new information for us now. The first four columns now relate to summary items for the sentence level (which is why Sentences is always equal to 1) and the next four columns relate to the document. Note that the first Text column now includes an additional term (i.e., .1). These index each sentence within the chapter.\nThe “right” level of analysis is going to be really contingent on your specific research question. There is no single correct level of analysis across all analyses. Think – a lot – about what you are interested in studying, and what level is best for that study."
  },
  {
    "objectID": "Tutorial2.html#tokens",
    "href": "Tutorial2.html#tokens",
    "title": "Tutorial 2 Text as Data",
    "section": "Tokens",
    "text": "Tokens\nWe’ve used the phrase “tokens” in a few places now, and it’s time to dive into what we mean by it. Tokens are the individual component pieces of the text, and tokenizing is the process of breaking up the text into those component pieces. Consider the following tweet from President Trump:\nUnless Republicans have a death wish, and it is also the right thing to do, they must approve the $2,000 payments ASAP. $600 IS NOT ENOUGH! Also, get rid of Section 230 - Don't let Big Tech steal our Country, and don't let the Democrats steal the Presidential Election. Get tough!\nWe can start to break that into constituent words (“Unless”, “Republicans”, etc.) but notice that we pretty quickly have decisions to make. Should we include the punctuation marks (“,”, “.”, “!”) with a word? As unique tokens themselves? What about numbers like 2,000? Likewise, we have to decide whether to include the “$” with 2,000; you can see why that’d be important when you get a bit further and run into “Section 230”. Finally, what should we do with contractions like “Don’t”? Is that one word or two? And should we treat “Don’t” and “don’t” as the same token, or two different tokens?\nAll of these are choices that get nested into the tokenization process. The most basic versions of a tokenizer will split on white space; others are trained to split on a host of other characteristics. The big thing to know is to always look at the data you are creating.\n\n# the default breaks on white space\nphilosophers_stone_tokens &lt;- tokens(philosophers_stone_corpus)\nprint(philosophers_stone_tokens)\n\nTokens consisting of 17 documents and 6 docvars.\ntext1 :\n [1] \"THE\"     \"BOY\"     \"WHO\"     \"LIVED\"   \"Mr\"      \".\"       \"and\"    \n [8] \"Mrs\"     \".\"       \"Dursley\" \",\"       \"of\"     \n[ ... and 5,681 more ]\n\ntext2 :\n [1] \"THE\"       \"VANISHING\" \"GLASS\"     \"Nearly\"    \"ten\"       \"years\"    \n [7] \"had\"       \"passed\"    \"since\"     \"the\"       \"Dursleys\"  \"had\"      \n[ ... and 4,142 more ]\n\ntext3 :\n [1] \"THE\"         \"LETTERS\"     \"FROM\"        \"NO\"          \"ONE\"        \n [6] \"The\"         \"escape\"      \"of\"          \"the\"         \"Brazilian\"  \n[11] \"boa\"         \"constrictor\"\n[ ... and 4,644 more ]\n\ntext4 :\n [1] \"THE\"     \"KEEPER\"  \"OF\"      \"THE\"     \"KEYS\"    \"BOOM\"    \".\"      \n [8] \"They\"    \"knocked\" \"again\"   \".\"       \"Dudley\" \n[ ... and 4,820 more ]\n\ntext5 :\n [1] \"DIAGON\"   \"ALLEY\"    \"Harry\"    \"woke\"     \"early\"    \"the\"     \n [7] \"next\"     \"morning\"  \".\"        \"Although\" \"he\"       \"could\"   \n[ ... and 8,434 more ]\n\ntext6 :\n [1] \"THE\"            \"JOURNEY\"        \"FROM\"           \"PLATFORM\"      \n [5] \"NINE\"           \"AND\"            \"THREE-QUARTERS\" \"Harry's\"       \n [9] \"last\"           \"month\"          \"with\"           \"the\"           \n[ ... and 8,004 more ]\n\n[ reached max_ndoc ... 11 more documents ]\n\n\n\n# you can also drop punctuation\nphilosophers_stone_tokens &lt;- tokens(philosophers_stone_corpus,\n       remove_punct = T)\nprint(philosophers_stone_tokens)\n\nTokens consisting of 17 documents and 6 docvars.\ntext1 :\n [1] \"THE\"     \"BOY\"     \"WHO\"     \"LIVED\"   \"Mr\"      \"and\"     \"Mrs\"    \n [8] \"Dursley\" \"of\"      \"number\"  \"four\"    \"Privet\" \n[ ... and 4,784 more ]\n\ntext2 :\n [1] \"THE\"       \"VANISHING\" \"GLASS\"     \"Nearly\"    \"ten\"       \"years\"    \n [7] \"had\"       \"passed\"    \"since\"     \"the\"       \"Dursleys\"  \"had\"      \n[ ... and 3,556 more ]\n\ntext3 :\n [1] \"THE\"         \"LETTERS\"     \"FROM\"        \"NO\"          \"ONE\"        \n [6] \"The\"         \"escape\"      \"of\"          \"the\"         \"Brazilian\"  \n[11] \"boa\"         \"constrictor\"\n[ ... and 3,984 more ]\n\ntext4 :\n [1] \"THE\"     \"KEEPER\"  \"OF\"      \"THE\"     \"KEYS\"    \"BOOM\"    \"They\"   \n [8] \"knocked\" \"again\"   \"Dudley\"  \"jerked\"  \"awake\"  \n[ ... and 3,910 more ]\n\ntext5 :\n [1] \"DIAGON\"   \"ALLEY\"    \"Harry\"    \"woke\"     \"early\"    \"the\"     \n [7] \"next\"     \"morning\"  \"Although\" \"he\"       \"could\"    \"tell\"    \n[ ... and 6,998 more ]\n\ntext6 :\n [1] \"THE\"            \"JOURNEY\"        \"FROM\"           \"PLATFORM\"      \n [5] \"NINE\"           \"AND\"            \"THREE-QUARTERS\" \"Harry's\"       \n [9] \"last\"           \"month\"          \"with\"           \"the\"           \n[ ... and 6,750 more ]\n\n[ reached max_ndoc ... 11 more documents ]\n\n\n\n# as well as numbers\nphilosophers_stone_tokens &lt;- tokens(philosophers_stone_corpus,\n       remove_punct = T,      \n       remove_numbers = T)\nprint(philosophers_stone_tokens)\n\nTokens consisting of 17 documents and 6 docvars.\ntext1 :\n [1] \"THE\"     \"BOY\"     \"WHO\"     \"LIVED\"   \"Mr\"      \"and\"     \"Mrs\"    \n [8] \"Dursley\" \"of\"      \"number\"  \"four\"    \"Privet\" \n[ ... and 4,784 more ]\n\ntext2 :\n [1] \"THE\"       \"VANISHING\" \"GLASS\"     \"Nearly\"    \"ten\"       \"years\"    \n [7] \"had\"       \"passed\"    \"since\"     \"the\"       \"Dursleys\"  \"had\"      \n[ ... and 3,556 more ]\n\ntext3 :\n [1] \"THE\"         \"LETTERS\"     \"FROM\"        \"NO\"          \"ONE\"        \n [6] \"The\"         \"escape\"      \"of\"          \"the\"         \"Brazilian\"  \n[11] \"boa\"         \"constrictor\"\n[ ... and 3,981 more ]\n\ntext4 :\n [1] \"THE\"     \"KEEPER\"  \"OF\"      \"THE\"     \"KEYS\"    \"BOOM\"    \"They\"   \n [8] \"knocked\" \"again\"   \"Dudley\"  \"jerked\"  \"awake\"  \n[ ... and 3,907 more ]\n\ntext5 :\n [1] \"DIAGON\"   \"ALLEY\"    \"Harry\"    \"woke\"     \"early\"    \"the\"     \n [7] \"next\"     \"morning\"  \"Although\" \"he\"       \"could\"    \"tell\"    \n[ ... and 6,989 more ]\n\ntext6 :\n [1] \"THE\"            \"JOURNEY\"        \"FROM\"           \"PLATFORM\"      \n [5] \"NINE\"           \"AND\"            \"THREE-QUARTERS\" \"Harry's\"       \n [9] \"last\"           \"month\"          \"with\"           \"the\"           \n[ ... and 6,749 more ]\n\n[ reached max_ndoc ... 11 more documents ]\n\n# as well as changing letters to lower cases\ntokens_tolower(philosophers_stone_tokens)\n\nTokens consisting of 17 documents and 6 docvars.\ntext1 :\n [1] \"the\"     \"boy\"     \"who\"     \"lived\"   \"mr\"      \"and\"     \"mrs\"    \n [8] \"dursley\" \"of\"      \"number\"  \"four\"    \"privet\" \n[ ... and 4,784 more ]\n\ntext2 :\n [1] \"the\"       \"vanishing\" \"glass\"     \"nearly\"    \"ten\"       \"years\"    \n [7] \"had\"       \"passed\"    \"since\"     \"the\"       \"dursleys\"  \"had\"      \n[ ... and 3,556 more ]\n\ntext3 :\n [1] \"the\"         \"letters\"     \"from\"        \"no\"          \"one\"        \n [6] \"the\"         \"escape\"      \"of\"          \"the\"         \"brazilian\"  \n[11] \"boa\"         \"constrictor\"\n[ ... and 3,981 more ]\n\ntext4 :\n [1] \"the\"     \"keeper\"  \"of\"      \"the\"     \"keys\"    \"boom\"    \"they\"   \n [8] \"knocked\" \"again\"   \"dudley\"  \"jerked\"  \"awake\"  \n[ ... and 3,907 more ]\n\ntext5 :\n [1] \"diagon\"   \"alley\"    \"harry\"    \"woke\"     \"early\"    \"the\"     \n [7] \"next\"     \"morning\"  \"although\" \"he\"       \"could\"    \"tell\"    \n[ ... and 6,989 more ]\n\ntext6 :\n [1] \"the\"            \"journey\"        \"from\"           \"platform\"      \n [5] \"nine\"           \"and\"            \"three-quarters\" \"harry's\"       \n [9] \"last\"           \"month\"          \"with\"           \"the\"           \n[ ... and 6,749 more ]\n\n[ reached max_ndoc ... 11 more documents ]\n\n\nWhen the data are tokenized, we can start to look at a more granular level at the usage of particular terms. For instance, maybe we want to know about the usage of particular terms within the corpus. We can look at that using keyword-in-context (kwic).\n\n# check the use of \"dumbledore\"\nkwic_dumbledore &lt;- kwic(philosophers_stone_tokens,\n     pattern = c(\"dumbledore\"))\n# window = 5 \n\n# look at the first few uses\nhead(kwic_dumbledore)\n\nKeyword-in-context with 6 matches.                                                             \n [text1, 2292]       This man's name was Albus | Dumbledore |\n [text1, 2294] name was Albus Dumbledore Albus | Dumbledore |\n [text1, 2489]  happening down on the pavement | Dumbledore |\n [text1, 2759]         can't blame them ` said | Dumbledore |\n [text1, 2818]      a sharp sideways glance at | Dumbledore |\n [text1, 2869]      suppose he really has gone | Dumbledore |\n                                  \n Albus Dumbledore didn't seem to  \n didn't seem to realize that      \n slipped the Put-Outer back inside\n gently ` We've had precious      \n here as though hoping he         \n ` ` It certainly seems           \n\n# now look at a broader window of terms around \"dumbledore\"\nkwic_dumbledore &lt;- kwic(philosophers_stone_tokens,\n     pattern = c(\"dumbledore\"),\n     window = 10)\n\n# look at the first few uses\nhead(kwic_dumbledore)\n\nKeyword-in-context with 6 matches.                                                                          \n [text1, 2292]      been broken at least twice This man's name was Albus |\n [text1, 2294] at least twice This man's name was Albus Dumbledore Albus |\n [text1, 2489]   to see anything that was happening down on the pavement |\n [text1, 2759]                much sense ` ` You can't blame them ` said |\n [text1, 2818]    swapping rumors ` She threw a sharp sideways glance at |\n [text1, 2869]             out about us all I suppose he really has gone |\n                                                                        \n Dumbledore | Albus Dumbledore didn't seem to realize that he had just  \n Dumbledore | didn't seem to realize that he had just arrived in        \n Dumbledore | slipped the Put-Outer back inside his cloak and set off   \n Dumbledore | gently ` We've had precious little to celebrate for eleven\n Dumbledore | here as though hoping he was going to tell her            \n Dumbledore | ` ` It certainly seems so ` said Dumbledore `             \n\n# if you are more interested in phrases, then you can do that too using phrase()\nkwic_phrase &lt;- kwic(philosophers_stone_tokens,\n                    pattern = phrase(\"daily prophet\"))\n\n\nhead(kwic_phrase)\n\nKeyword-in-context with 3 matches.                                                                   \n   [text5, 901:902] Hagrid read his newspaper the | Daily Prophet |\n [text6, 5196:5197]        It's been all over the | Daily Prophet |\n [text8, 2909:2910]        was a cutting from the | Daily Prophet |\n                                                  \n Harry had learned from Uncle                     \n but I don't suppose you                          \n GRINGOTTS BREAK-IN LATEST Investigations continue\n\n\nThe Daily Prophet offers a great example of a problem we might encounter with tokenizers; the standard approach is going to treat this as two different words when really it is the phrase itself that is likely of interest. Therefore, we can compound the tokens into a phrase using tokens_compound(). This creates a bigram; you can similarly create three-token phrases (trigram), four-token phrases (four-gram), and so on. Note that the newly created token will be the phrase separated by “_” (i.e., Daily_Prophet).\n\nphilosophers_stone_compound &lt;- tokens_compound(philosophers_stone_tokens,\n                pattern = phrase(\"Daily Prophet\"))\n\nhead(kwic(philosophers_stone_compound,\n          pattern = \"Daily_Prophet\"))\n\nKeyword-in-context with 3 matches.                                                              \n  [text5, 901] Hagrid read his newspaper the | Daily_Prophet |\n [text6, 5196]        It's been all over the | Daily_Prophet |\n [text8, 2909]        was a cutting from the | Daily_Prophet |\n                                                  \n Harry had learned from Uncle                     \n but I don't suppose you                          \n GRINGOTTS BREAK-IN LATEST Investigations continue\n\n\nOf course, you may also believe that there are lots and lots of potentially meaningful n-grams (i.e., uni-, bi-, tri-, four-, etc.) that you do not want to individually specify. In those cases, you can specify that tokenization specifically include every possible n-gram.\n\n# create a tokens object with unigrams and bigrams\nphilosophers_stone_ngrams &lt;- tokens_ngrams(philosophers_stone_tokens, n=1:2)\n\n# look at the first few observations. Note the indexing here to look at only the first few words *within the first chapter*\nhead(philosophers_stone_ngrams[[1]], 50)\n\n [1] \"THE\"        \"BOY\"        \"WHO\"        \"LIVED\"      \"Mr\"        \n [6] \"and\"        \"Mrs\"        \"Dursley\"    \"of\"         \"number\"    \n[11] \"four\"       \"Privet\"     \"Drive\"      \"were\"       \"proud\"     \n[16] \"to\"         \"say\"        \"that\"       \"they\"       \"were\"      \n[21] \"perfectly\"  \"normal\"     \"thank\"      \"you\"        \"very\"      \n[26] \"much\"       \"They\"       \"were\"       \"the\"        \"last\"      \n[31] \"people\"     \"you'd\"      \"expect\"     \"to\"         \"be\"        \n[36] \"involved\"   \"in\"         \"anything\"   \"strange\"    \"or\"        \n[41] \"mysterious\" \"because\"    \"they\"       \"just\"       \"didn't\"    \n[46] \"hold\"       \"with\"       \"such\"       \"nonsense\"   \"Mr\"        \n\ntail(philosophers_stone_ngrams[[1]], 50)\n\n [1] \"nor_that\"       \"that_he\"        \"he_would\"       \"would_spend\"   \n [5] \"spend_the\"      \"the_next\"       \"next_few\"       \"few_weeks\"     \n [9] \"weeks_being\"    \"being_prodded\"  \"prodded_and\"    \"and_pinched\"   \n[13] \"pinched_by\"     \"by_his\"         \"his_cousin\"     \"cousin_Dudley\" \n[17] \"Dudley_He\"      \"He_couldn't\"    \"couldn't_know\"  \"know_that\"     \n[21] \"that_at\"        \"at_this\"        \"this_very\"      \"very_moment\"   \n[25] \"moment_people\"  \"people_meeting\" \"meeting_in\"     \"in_secret\"     \n[29] \"secret_all\"     \"all_over\"       \"over_the\"       \"the_country\"   \n[33] \"country_were\"   \"were_holding\"   \"holding_up\"     \"up_their\"      \n[37] \"their_glasses\"  \"glasses_and\"    \"and_saying\"     \"saying_in\"     \n[41] \"in_hushed\"      \"hushed_voices\"  \"voices_`\"       \"`_To\"          \n[45] \"To_Harry\"       \"Harry_Potter\"   \"Potter_the\"     \"the_boy\"       \n[49] \"boy_who\"        \"who_lived\"     \n\n\nAs you can see, there is a pretty severe curse of dimensionality problem as you look to expand into greater and greater ngrams. Nevertheless, computational time and space is cheap, and the added information from the phrases could be useful in different research settings."
  },
  {
    "objectID": "Tutorial2.html#combining-corpora",
    "href": "Tutorial2.html#combining-corpora",
    "title": "Tutorial 2 Text as Data",
    "section": "Combining Corpora",
    "text": "Combining Corpora\nIn the above, we’ve been working with just one text, broken into chapters. But occasionally we have two corpora that we need to combine. Here, for instance, there are 6 more Harry Potter books that we have not, to this point, added to any of our analysis.\nDoing so with quanteda is easy, but getting there is hard because we have to repeat a lot of steps for seven corpora. Instead, let’s do this with loops.\n\n# list out the object (book) names that we need\nmyBooks &lt;- c(\"philosophers_stone\",\n             \"chamber_of_secrets\",\n             \"prisoner_of_azkaban\",\n             \"goblet_of_fire\",\n             \"order_of_the_phoenix\",\n             \"half_blood_prince\",\n             \"deathly_hallows\")\n\n# create loop.\nfor (i in 1:length(myBooks)){\n  \n  # create corpora\n  corpusCall &lt;- paste(myBooks[i],\"_corpus &lt;- corpus(\",myBooks[i],\")\", sep = \"\")\n  eval(parse(text=corpusCall))\n\n  # change document names for each chapter to include the book title. If you don't do this, the document names will be duplicated and you'll get an error.\n  namesCall &lt;- paste(\"tmpNames &lt;- docnames(\",myBooks[i],\"_corpus)\", sep = \"\")\n  eval(parse(text=namesCall))\n  bindCall &lt;- paste(\"docnames(\",myBooks[i],\"_corpus) &lt;- paste(\\\"\",myBooks[i],\"\\\", tmpNames, sep = \\\"-\\\")\", sep = \"\")\n  eval(parse(text=bindCall))\n\n  # create summary data\n  summaryCall &lt;- paste(myBooks[i],\"_summary &lt;- summary(\",myBooks[i],\"_corpus)\", sep = \"\")\n  eval(parse(text=summaryCall))\n\n  # add indicator\n  bookCall &lt;- paste(myBooks[i],\"_summary$book &lt;- \\\"\",myBooks[i],\"\\\"\", sep = \"\")\n  eval(parse(text=bookCall))\n\n  # add chapter indicator\n  chapterCall &lt;- paste(myBooks[i],\"_summary$chapter &lt;- as.numeric(str_extract(\",myBooks[i],\"_summary$Text, \\\"[0-9]+\\\"))\", sep = \"\")\n  eval(parse(text=chapterCall))\n\n  # add meta data to each corpus\n  metaCall &lt;- paste(\"docvars(\",myBooks[i],\"_corpus) &lt;- \",myBooks[i],\"_summary\", sep = \"\")\n  eval(parse(text=metaCall))\n\n}\n\n# once the loop finishes up, check to make sure you've created what you want\ndocvars(deathly_hallows_corpus)\n\n                     Text Types Tokens Sentences            book chapter\n1   deathly_hallows-text1  1108   3876       223 deathly_hallows       1\n2   deathly_hallows-text2  1466   4616       163 deathly_hallows       2\n3   deathly_hallows-text3  1110   4171       256 deathly_hallows       3\n4   deathly_hallows-text4  1549   6586       307 deathly_hallows       4\n5   deathly_hallows-text5  1577   7604       489 deathly_hallows       5\n6   deathly_hallows-text6  1772   8389       481 deathly_hallows       6\n7   deathly_hallows-text7  1810   8457       534 deathly_hallows       7\n8   deathly_hallows-text8  1860   7762       438 deathly_hallows       8\n9   deathly_hallows-text9  1288   5198       279 deathly_hallows       9\n10 deathly_hallows-text10  1724   8071       461 deathly_hallows      10\n11 deathly_hallows-text11  1677   6931       384 deathly_hallows      11\n12 deathly_hallows-text12  1848   7542       405 deathly_hallows      12\n13 deathly_hallows-text13  1725   7094       357 deathly_hallows      13\n14 deathly_hallows-text14  1295   5175       267 deathly_hallows      14\n15 deathly_hallows-text15  1908   8860       537 deathly_hallows      15\n16 deathly_hallows-text16  1527   6150       289 deathly_hallows      16\n17 deathly_hallows-text17  1510   6741       391 deathly_hallows      17\n18 deathly_hallows-text18  1183   4125       206 deathly_hallows      18\n19 deathly_hallows-text19  1708   8228       459 deathly_hallows      19\n20 deathly_hallows-text20  1451   5561       319 deathly_hallows      20\n21 deathly_hallows-text21  1426   6156       395 deathly_hallows      21\n22 deathly_hallows-text22  1620   7430       453 deathly_hallows      22\n23 deathly_hallows-text23  1881  10152       700 deathly_hallows      23\n24 deathly_hallows-text24  1655   8449       554 deathly_hallows      24\n25 deathly_hallows-text25  1344   5543       326 deathly_hallows      25\n26 deathly_hallows-text26  1878   8206       378 deathly_hallows      26\n27 deathly_hallows-text27   921   3207       148 deathly_hallows      27\n28 deathly_hallows-text28  1328   5622       338 deathly_hallows      28\n29 deathly_hallows-text29  1359   5806       373 deathly_hallows      29\n30 deathly_hallows-text30  1475   6266       372 deathly_hallows      30\n31 deathly_hallows-text31  2069   9913       560 deathly_hallows      31\n32 deathly_hallows-text32  1535   6801       326 deathly_hallows      32\n33 deathly_hallows-text33  2072  10456       721 deathly_hallows      33\n34 deathly_hallows-text34  1170   4599       241 deathly_hallows      34\n35 deathly_hallows-text35  1329   6321       426 deathly_hallows      35\n36 deathly_hallows-text36  1893   8635       424 deathly_hallows      36\n37 deathly_hallows-text37   710   2072       141 deathly_hallows      37\n\n# You can change the book name to any of the seven Harry Potter books\n\nNow that we have all of the corpora in order, we can combine then using c().\n\n# create combined corpora of the first 7 harry potter books.\nharry_potter_corpus &lt;-\n  c(philosophers_stone_corpus,                  chamber_of_secrets_corpus,                  prisoner_of_azkaban_corpus,\n    goblet_of_fire_corpus,                      order_of_the_phoenix_corpus,\n    half_blood_prince_corpus,\n    deathly_hallows_corpus)\nsummary(harry_potter_corpus)\n\nCorpus consisting of 200 documents, showing 100 documents:\n\n                       Text Types Tokens Sentences                       Text\n   philosophers_stone-text1  1271   5693       350   philosophers_stone-text1\n   philosophers_stone-text2  1066   4154       237   philosophers_stone-text2\n   philosophers_stone-text3  1226   4656       297   philosophers_stone-text3\n   philosophers_stone-text4  1193   4832       322   philosophers_stone-text4\n   philosophers_stone-text5  1818   8446       563   philosophers_stone-text5\n   philosophers_stone-text6  1563   8016       566   philosophers_stone-text6\n   philosophers_stone-text7  1374   5487       351   philosophers_stone-text7\n   philosophers_stone-text8  1095   3608       198   philosophers_stone-text8\n   philosophers_stone-text9  1422   6195       411   philosophers_stone-text9\n  philosophers_stone-text10  1293   5237       334  philosophers_stone-text10\n  philosophers_stone-text11  1110   4215       277  philosophers_stone-text11\n  philosophers_stone-text12  1509   6790       447  philosophers_stone-text12\n  philosophers_stone-text13  1076   3953       262  philosophers_stone-text13\n  philosophers_stone-text14  1110   4394       308  philosophers_stone-text14\n  philosophers_stone-text15  1385   6486       459  philosophers_stone-text15\n  philosophers_stone-text16  1581   8357       591  philosophers_stone-text16\n  philosophers_stone-text17  1489   7172       506  philosophers_stone-text17\n   chamber_of_secrets-text1   998   3206       183   chamber_of_secrets-text1\n   chamber_of_secrets-text2  1040   3715       252   chamber_of_secrets-text2\n   chamber_of_secrets-text3  1410   5743       342   chamber_of_secrets-text3\n   chamber_of_secrets-text4  1751   7350       393   chamber_of_secrets-text4\n   chamber_of_secrets-text5  1630   6390       354   chamber_of_secrets-text5\n   chamber_of_secrets-text6  1648   6145       358   chamber_of_secrets-text6\n   chamber_of_secrets-text7  1335   5019       363   chamber_of_secrets-text7\n   chamber_of_secrets-text8  1625   5896       353   chamber_of_secrets-text8\n   chamber_of_secrets-text9  1554   6701       431   chamber_of_secrets-text9\n  chamber_of_secrets-text10  1563   6652       407  chamber_of_secrets-text10\n  chamber_of_secrets-text11  1763   7380       444  chamber_of_secrets-text11\n  chamber_of_secrets-text12  1472   5582       349  chamber_of_secrets-text12\n  chamber_of_secrets-text13  1092   4050       263  chamber_of_secrets-text13\n  chamber_of_secrets-text14   898   2959       195  chamber_of_secrets-text14\n  chamber_of_secrets-text15  1377   5328       335  chamber_of_secrets-text15\n  chamber_of_secrets-text16   986   3150       168  chamber_of_secrets-text16\n  chamber_of_secrets-text17  1485   6613       464  chamber_of_secrets-text17\n  chamber_of_secrets-text18  1063   4316       307  chamber_of_secrets-text18\n  chamber_of_secrets-text19  2063  10949       729  chamber_of_secrets-text19\n  prisoner_of_azkaban-text1  1267   4299       217  prisoner_of_azkaban-text1\n  prisoner_of_azkaban-text2  1254   4795       302  prisoner_of_azkaban-text2\n  prisoner_of_azkaban-text3  1383   5660       364  prisoner_of_azkaban-text3\n  prisoner_of_azkaban-text4  1619   6414       382  prisoner_of_azkaban-text4\n  prisoner_of_azkaban-text5  1621   7059       437  prisoner_of_azkaban-text5\n  prisoner_of_azkaban-text6  1762   7978       504  prisoner_of_azkaban-text6\n  prisoner_of_azkaban-text7  1318   5501       388  prisoner_of_azkaban-text7\n  prisoner_of_azkaban-text8  1583   6469       417  prisoner_of_azkaban-text8\n  prisoner_of_azkaban-text9  1542   6737       450  prisoner_of_azkaban-text9\n prisoner_of_azkaban-text10  1982   9021       570 prisoner_of_azkaban-text10\n prisoner_of_azkaban-text11  1575   6517       416 prisoner_of_azkaban-text11\n prisoner_of_azkaban-text12  1366   6283       433 prisoner_of_azkaban-text12\n prisoner_of_azkaban-text13  1374   5388       326 prisoner_of_azkaban-text13\n prisoner_of_azkaban-text14  1609   7043       496 prisoner_of_azkaban-text14\n prisoner_of_azkaban-text15  1664   6908       459 prisoner_of_azkaban-text15\n prisoner_of_azkaban-text16  1325   5007       344 prisoner_of_azkaban-text16\n prisoner_of_azkaban-text17  1206   5464       360 prisoner_of_azkaban-text17\n prisoner_of_azkaban-text18   784   2955       186 prisoner_of_azkaban-text18\n prisoner_of_azkaban-text19  1382   7013       486 prisoner_of_azkaban-text19\n prisoner_of_azkaban-text20   736   2571       184 prisoner_of_azkaban-text20\n prisoner_of_azkaban-text21  1622   9752       748 prisoner_of_azkaban-text21\n prisoner_of_azkaban-text22  1436   6165       475 prisoner_of_azkaban-text22\n       goblet_of_fire-text1  1225   5150       278       goblet_of_fire-text1\n       goblet_of_fire-text2   975   3307       151       goblet_of_fire-text2\n       goblet_of_fire-text3  1064   3777       214       goblet_of_fire-text3\n       goblet_of_fire-text4  1053   3753       236       goblet_of_fire-text4\n       goblet_of_fire-text5  1244   4773       274       goblet_of_fire-text5\n       goblet_of_fire-text6   878   3080       199       goblet_of_fire-text6\n       goblet_of_fire-text7  1647   6731       416       goblet_of_fire-text7\n       goblet_of_fire-text8  1655   7219       421       goblet_of_fire-text8\n       goblet_of_fire-text9  1734   9396       647       goblet_of_fire-text9\n      goblet_of_fire-text10  1108   4194       267      goblet_of_fire-text10\n      goblet_of_fire-text11  1162   4246       258      goblet_of_fire-text11\n      goblet_of_fire-text12  1700   6770       394      goblet_of_fire-text12\n      goblet_of_fire-text13  1408   5021       312      goblet_of_fire-text13\n      goblet_of_fire-text14  1453   6107       393      goblet_of_fire-text14\n      goblet_of_fire-text15  1733   6429       343      goblet_of_fire-text15\n      goblet_of_fire-text16  1635   7579       458      goblet_of_fire-text16\n      goblet_of_fire-text17  1255   5269       345      goblet_of_fire-text17\n      goblet_of_fire-text18  1758   8330       467      goblet_of_fire-text18\n      goblet_of_fire-text19  1695   7876       441      goblet_of_fire-text19\n      goblet_of_fire-text20  1747   9024       516      goblet_of_fire-text20\n      goblet_of_fire-text21  1616   7134       443      goblet_of_fire-text21\n      goblet_of_fire-text22  1310   5735       379      goblet_of_fire-text22\n      goblet_of_fire-text23  2131  10268       619      goblet_of_fire-text23\n      goblet_of_fire-text24  1742   8010       501      goblet_of_fire-text24\n      goblet_of_fire-text25  1505   7175       409      goblet_of_fire-text25\n      goblet_of_fire-text26  2021   9944       529      goblet_of_fire-text26\n      goblet_of_fire-text27  1815   8780       534      goblet_of_fire-text27\n      goblet_of_fire-text28  1898   9578       670      goblet_of_fire-text28\n      goblet_of_fire-text29  1278   5674       406      goblet_of_fire-text29\n      goblet_of_fire-text30  1492   8054       515      goblet_of_fire-text30\n      goblet_of_fire-text31  1932   9930       661      goblet_of_fire-text31\n      goblet_of_fire-text32   689   2410       131      goblet_of_fire-text32\n      goblet_of_fire-text33  1134   5217       275      goblet_of_fire-text33\n      goblet_of_fire-text34   818   3713       145      goblet_of_fire-text34\n      goblet_of_fire-text35  1442   7531       607      goblet_of_fire-text35\n      goblet_of_fire-text36  1518   7797       535      goblet_of_fire-text36\n      goblet_of_fire-text37  1408   6273       443      goblet_of_fire-text37\n order_of_the_phoenix-text1  1761   7026       405 order_of_the_phoenix-text1\n order_of_the_phoenix-text2  1654   7724       545 order_of_the_phoenix-text2\n order_of_the_phoenix-text3  1622   6435       398 order_of_the_phoenix-text3\n order_of_the_phoenix-text4  1641   7158       447 order_of_the_phoenix-text4\n order_of_the_phoenix-text5  1544   6877       431 order_of_the_phoenix-text5\n Types Tokens Sentences                 book chapter\n  1271   5693       350   philosophers_stone       1\n  1066   4154       237   philosophers_stone       2\n  1226   4656       297   philosophers_stone       3\n  1193   4832       322   philosophers_stone       4\n  1818   8446       563   philosophers_stone       5\n  1563   8016       566   philosophers_stone       6\n  1374   5487       351   philosophers_stone       7\n  1095   3608       198   philosophers_stone       8\n  1422   6195       411   philosophers_stone       9\n  1293   5237       334   philosophers_stone      10\n  1110   4215       277   philosophers_stone      11\n  1509   6790       447   philosophers_stone      12\n  1076   3953       262   philosophers_stone      13\n  1110   4394       308   philosophers_stone      14\n  1385   6486       459   philosophers_stone      15\n  1581   8357       591   philosophers_stone      16\n  1489   7172       506   philosophers_stone      17\n   998   3206       183   chamber_of_secrets       1\n  1040   3715       252   chamber_of_secrets       2\n  1410   5743       342   chamber_of_secrets       3\n  1751   7350       393   chamber_of_secrets       4\n  1630   6390       354   chamber_of_secrets       5\n  1648   6145       358   chamber_of_secrets       6\n  1335   5019       363   chamber_of_secrets       7\n  1625   5896       353   chamber_of_secrets       8\n  1554   6701       431   chamber_of_secrets       9\n  1563   6652       407   chamber_of_secrets      10\n  1763   7380       444   chamber_of_secrets      11\n  1472   5582       349   chamber_of_secrets      12\n  1092   4050       263   chamber_of_secrets      13\n   898   2959       195   chamber_of_secrets      14\n  1377   5328       335   chamber_of_secrets      15\n   986   3150       168   chamber_of_secrets      16\n  1485   6613       464   chamber_of_secrets      17\n  1063   4316       307   chamber_of_secrets      18\n  2063  10949       729   chamber_of_secrets      19\n  1267   4299       217  prisoner_of_azkaban       1\n  1254   4795       302  prisoner_of_azkaban       2\n  1383   5660       364  prisoner_of_azkaban       3\n  1619   6414       382  prisoner_of_azkaban       4\n  1621   7059       437  prisoner_of_azkaban       5\n  1762   7978       504  prisoner_of_azkaban       6\n  1318   5501       388  prisoner_of_azkaban       7\n  1583   6469       417  prisoner_of_azkaban       8\n  1542   6737       450  prisoner_of_azkaban       9\n  1982   9021       570  prisoner_of_azkaban      10\n  1575   6517       416  prisoner_of_azkaban      11\n  1366   6283       433  prisoner_of_azkaban      12\n  1374   5388       326  prisoner_of_azkaban      13\n  1609   7043       496  prisoner_of_azkaban      14\n  1664   6908       459  prisoner_of_azkaban      15\n  1325   5007       344  prisoner_of_azkaban      16\n  1206   5464       360  prisoner_of_azkaban      17\n   784   2955       186  prisoner_of_azkaban      18\n  1382   7013       486  prisoner_of_azkaban      19\n   736   2571       184  prisoner_of_azkaban      20\n  1622   9752       748  prisoner_of_azkaban      21\n  1436   6165       475  prisoner_of_azkaban      22\n  1225   5150       278       goblet_of_fire       1\n   975   3307       151       goblet_of_fire       2\n  1064   3777       214       goblet_of_fire       3\n  1053   3753       236       goblet_of_fire       4\n  1244   4773       274       goblet_of_fire       5\n   878   3080       199       goblet_of_fire       6\n  1647   6731       416       goblet_of_fire       7\n  1655   7219       421       goblet_of_fire       8\n  1734   9396       647       goblet_of_fire       9\n  1108   4194       267       goblet_of_fire      10\n  1162   4246       258       goblet_of_fire      11\n  1700   6770       394       goblet_of_fire      12\n  1408   5021       312       goblet_of_fire      13\n  1453   6107       393       goblet_of_fire      14\n  1733   6429       343       goblet_of_fire      15\n  1635   7579       458       goblet_of_fire      16\n  1255   5269       345       goblet_of_fire      17\n  1758   8330       467       goblet_of_fire      18\n  1695   7876       441       goblet_of_fire      19\n  1747   9024       516       goblet_of_fire      20\n  1616   7134       443       goblet_of_fire      21\n  1310   5735       379       goblet_of_fire      22\n  2131  10268       619       goblet_of_fire      23\n  1742   8010       501       goblet_of_fire      24\n  1505   7175       409       goblet_of_fire      25\n  2021   9944       529       goblet_of_fire      26\n  1815   8780       534       goblet_of_fire      27\n  1898   9578       670       goblet_of_fire      28\n  1278   5674       406       goblet_of_fire      29\n  1492   8054       515       goblet_of_fire      30\n  1932   9930       661       goblet_of_fire      31\n   689   2410       131       goblet_of_fire      32\n  1134   5217       275       goblet_of_fire      33\n   818   3713       145       goblet_of_fire      34\n  1442   7531       607       goblet_of_fire      35\n  1518   7797       535       goblet_of_fire      36\n  1408   6273       443       goblet_of_fire      37\n  1761   7026       405 order_of_the_phoenix       1\n  1654   7724       545 order_of_the_phoenix       2\n  1622   6435       398 order_of_the_phoenix       3\n  1641   7158       447 order_of_the_phoenix       4\n  1544   6877       431 order_of_the_phoenix       5\n\n\nNow we’re cooking. Here are some handy functions that can help us get a handle of the size and scope of our corpus now that we’re not going to be able to quickly see everything even in a summary page.\n\n# check the number of documents (here, total chapters in the 7 books)\nndoc(harry_potter_corpus)\n\n[1] 200\n\n# check the total length of the text (i.e., the total word count)\nsum(ntoken(harry_potter_corpus))\n\n[1] 1365970\n\n\nWe’ll learn other ways to characterize and explore the texts later this semester when we turn to the different manners in which we present them. For now, you should have all of the tools you need to get your own corpus set up in R, and to be able to identify a number of important characteristics (the size of the corpus in terms of documents and vocabulary, for instance)."
  },
  {
    "objectID": "Tutorial2.html#need-more-practice",
    "href": "Tutorial2.html#need-more-practice",
    "title": "Tutorial 2 Text as Data",
    "section": "Need more practice?",
    "text": "Need more practice?\n\n#install.packages(\"janeaustenr\")\nlibrary(janeaustenr)\n\n# Jane Austen's novel \"sense and sensibility\"\nsensesensibility[1:30]\n\n [1] \"SENSE AND SENSIBILITY\"                                                  \n [2] \"\"                                                                       \n [3] \"by Jane Austen\"                                                         \n [4] \"\"                                                                       \n [5] \"(1811)\"                                                                 \n [6] \"\"                                                                       \n [7] \"\"                                                                       \n [8] \"\"                                                                       \n [9] \"\"                                                                       \n[10] \"CHAPTER 1\"                                                              \n[11] \"\"                                                                       \n[12] \"\"                                                                       \n[13] \"The family of Dashwood had long been settled in Sussex.  Their estate\"  \n[14] \"was large, and their residence was at Norland Park, in the centre of\"   \n[15] \"their property, where, for many generations, they had lived in so\"      \n[16] \"respectable a manner as to engage the general good opinion of their\"    \n[17] \"surrounding acquaintance.  The late owner of this estate was a single\"  \n[18] \"man, who lived to a very advanced age, and who for many years of his\"   \n[19] \"life, had a constant companion and housekeeper in his sister.  But her\" \n[20] \"death, which happened ten years before his own, produced a great\"       \n[21] \"alteration in his home; for to supply her loss, he invited and received\"\n[22] \"into his house the family of his nephew Mr. Henry Dashwood, the legal\"  \n[23] \"inheritor of the Norland estate, and the person to whom he intended to\" \n[24] \"bequeath it.  In the society of his nephew and niece, and their\"        \n[25] \"children, the old Gentleman's days were comfortably spent.  His\"        \n[26] \"attachment to them all increased.  The constant attention of Mr. and\"   \n[27] \"Mrs. Henry Dashwood to his wishes, which proceeded not merely from\"     \n[28] \"interest, but from goodness of heart, gave him every degree of solid\"   \n[29] \"comfort which his age could receive; and the cheerfulness of the\"       \n[30] \"children added a relish to his existence.\"                              \n\nclass(sensesensibility)\n\n[1] \"character\"\n\nsensesensibility[110]\n\n[1] \"father's decease; but the indelicacy of her conduct was so much the\"\n\n\nWe see that this is a character vector that contains multiple strings. We want to combine these strings into one.\n\nsensesensibility_test &lt;- paste(sensesensibility, collapse = \" \")\nclass(sensesensibility_test)\n\n[1] \"character\"\n\n#print(sensesensibility_test)\n#Output is too long, so I won't render it\n#We'll show this in class\n\nWOW, this is really long. We notice that this character vector now contains only one element, which is the whole book. And we want to separate it by chapters.\n\nsensesensibility2 &lt;- unlist(strsplit(sensesensibility_test, \"CHAPTER [0-9]+   \"))\n\nprint(sensesensibility2[1])\n\n[1] \"SENSE AND SENSIBILITY  by Jane Austen  (1811)     \"\n\n\nNow we convert the character vectors to a corpus\n\nsnse2corpus &lt;- corpus(sensesensibility2)\nsensesummary &lt;- summary(snse2corpus)\nprint(sensesummary)\n\nCorpus consisting of 51 documents, showing 51 documents:\n\n   Text Types Tokens Sentences\n  text1     9      9         1\n  text2   560   1810        49\n  text3   611   2339        99\n  text4   606   1785        79\n  text5   682   2255        88\n  text6   463   1171        33\n  text7   557   1499        44\n  text8   523   1424        41\n  text9   511   1452        62\n text10   729   2191        79\n text11   774   2412        82\n text12   590   1636        58\n text13   628   1965        68\n text14   653   2637       130\n text15   577   1752        70\n text16   794   3003       138\n text17   745   2386       103\n text18   612   2063       114\n text19   611   1823        77\n text20   898   3502       139\n text21   650   3032       154\n text22   914   3455       107\n text23   819   3437       120\n text24   814   2710        69\n text25   719   2481        82\n text26   660   2248        65\n text27   817   2929        95\n text28   843   2973       115\n text29   581   1689        59\n text30  1071   4636       172\n text31   900   3729       166\n text32  1114   4556       156\n text33   853   3011        99\n text34   916   3601       138\n text35   879   3117        69\n text36   731   2850        95\n text37   965   3556       100\n text38  1126   5484       167\n text39   838   3755       119\n text40   707   2396        54\n text41   759   3217       114\n text42   818   3246        84\n text43   709   2081        34\n text44  1013   4030        99\n text45  1363   6959       228\n text46   745   2556        67\n text47  1016   3419        87\n text48   785   2869        90\n text49   523   1601        60\n text50  1155   4955       110\n text51   871   2884        57\n\n\nWhy are there NINE tokens in text1 (The book title)? (hint: find the answer using tokens function)"
  },
  {
    "objectID": "readingPDF.html",
    "href": "readingPDF.html",
    "title": "Reading all PDF files in a directory",
    "section": "",
    "text": "# list.files(): to list all files in a directory\n# returns a character vector containing the names of the files in the specified directory\n# syntax: list.files(path, pattern, all.files, full.names)\n\nlist.files(path=\"~/Documents/R Practice/Quarto/TAD23/HarryPotter\", pattern = NULL,\n           all.files = FALSE, full.names = FALSE)\n\n[1] \"chapter1.pdf\" \"chapter2.pdf\" \"chapter3.pdf\" \"chapter4.pdf\" \"chapter5.pdf\"\n\nlist.files(path = \"~/Documents/R Practice/Quarto/TAD23/HarryPotter\", pattern = \".pdf\", all.files = TRUE, full.names=TRUE)\n\n[1] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter1.pdf\"\n[2] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter2.pdf\"\n[3] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter3.pdf\"\n[4] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter4.pdf\"\n[5] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter5.pdf\"\n\n\n\nrequire(pdftools)\n\nLoading required package: pdftools\n\n\nUsing poppler version 22.02.0\n\nlibrary(quanteda)\n\nPackage version: 3.3.1\nUnicode version: 14.0\nICU version: 70.1\n\n\nParallel computing: 4 of 4 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\n# create a list of the files in the designated directory\nfile_list &lt;- list.files(path = \"~/Documents/R Practice/Quarto/TAD23/HarryPotter\", pattern = \".pdf\", full.names = TRUE)\n\nfile_list\n\n[1] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter1.pdf\"\n[2] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter2.pdf\"\n[3] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter3.pdf\"\n[4] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter4.pdf\"\n[5] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter5.pdf\"\n\ntest &lt;- lapply(file_list, pdf_text) # pdf_text: extracting text\n\ntest[1]\n\n[[1]]\n [1] \"Harry Potter and the\\nPhilosopher’s Stone\\n\\n      J. K. Rowling\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n [2] \"     All rights reserved; no part of this publication may be reproduced or\\n       transmitted by any means, electronic, mechanical, photocopying\\n          or otherwise, without the prior permission of the publisher\\n\\n                  First published in Great Britain in 1997\\n       Bloomsbury Publishing Plc, 36 Soho Square, London, W1D 3QY\\n\\n                     This edition first published in 2004\\n\\n                       Copyright © 1997 J. K. Rowling\\n\\n            Harry Potter, names, characters and related indicia are\\n               copyright and trademark Warner Bros., 2000™\\n\\n               The moral right of the author has been asserted\\n   A CIP catalogue record of this book is available from the British Library\\n\\n                           ISBN 978 0 7475 7360 9\\n\\nThe paper this book is printed on is certified by the © 1996 Forest Stewardship\\n      Council A.C. (FSC). It is ancient-forest friendly. The printer holds\\n                    FSC chain of custody SGS-COC-2061.\\n                                                       ©\\n\\n\\n\\n                                           F SC\\n                                     Mixed Sources\\n                                 Product group from well-managed\\n                               forests and other controlled sources\\n\\n                                    Cert no. SGS-COC-2061\\n                                         www.fsc.org\\n                               ©1996 Forest Stewardship Council\\n\\n\\n\\n               Printed in Great Britain by Clays Ltd, St Ives plc\\n                     Typeset by Dorchester Typesetting\\n\\n                                  5 7 9 10 8 6 4\\n\\n                      www.bloomsbury.com/harrypotter\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n [3] \"   for Jessica, who loves stories,\\n  for Anne, who loved them too,\\nand for Di, who heard this one first.\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n [4] \"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n [5] \"                     — CHAPTER ONE —\\n\\n\\n\\n         The Boy Who Lived\\nMr and Mrs Dursley, of number four, Privet Drive, were proud to\\nsay that they were perfectly normal, thank you very much. They\\nwere the last people you’d expect to be involved in anything\\nstrange or mysterious, because they just didn’t hold with such\\nnonsense.\\n    Mr Dursley was the director of a firm called Grunnings, which\\nmade drills. He was a big, beefy man with hardly any neck,\\nalthough he did have a very large moustache. Mrs Dursley was\\nthin and blonde and had nearly twice the usual amount of neck,\\nwhich came in very useful as she spent so much of her time craning\\nover garden fences, spying on the neighbours. The Dursleys had a\\nsmall son called Dudley and in their opinion there was no finer\\nboy anywhere.\\n    The Dursleys had everything they wanted, but they also had a\\nsecret, and their greatest fear was that somebody would discover\\nit. They didn’t think they could bear it if anyone found out about\\nthe Potters. Mrs Potter was Mrs Dursley’s sister, but they hadn’t\\nmet for several years; in fact, Mrs Dursley pretended she didn’t\\nhave a sister, because her sister and her good-for-nothing husband\\nwere as unDursleyish as it was possible to be. The Dursleys\\nshuddered to think what the neighbours would say if the Potters\\narrived in the street. The Dursleys knew that the Potters had a\\nsmall son, too, but they had never even seen him. This boy was\\nanother good reason for keeping the Potters away; they didn’t\\nwant Dudley mixing with a child like that.\\n    When Mr and Mrs Dursley woke up on the dull, grey Tuesday\\nour story starts, there was nothing about the cloudy sky outside to\\nsuggest that strange and mysterious things would soon be hap-\\npening all over the country. Mr Dursley hummed as he picked out\\nhis most boring tie for work and Mrs Dursley gossiped away\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n [6] \"8                          HARRY POTTER\\n\\nhappily as she wrestled a screaming Dudley into his high chair.\\n    None of them noticed a large tawny owl flutter past the window.\\n    At half past eight, Mr Dursley picked up his briefcase, pecked\\nMrs Dursley on the cheek and tried to kiss Dudley goodbye but\\nmissed, because Dudley was now having a tantrum and throwing\\nhis cereal at the walls. ‘Little tyke,’ chortled Mr Dursley as he left\\nthe house. He got into his car and backed out of number four’s\\ndrive.\\n    It was on the corner of the street that he noticed the first sign\\nof something peculiar – a cat reading a map. For a second, Mr\\nDursley didn’t realise what he had seen – then he jerked his head\\naround to look again. There was a tabby cat standing on the corner\\nof Privet Drive, but there wasn’t a map in sight. What could\\nhe have been thinking of? It must have been a trick of the light.\\nMr Dursley blinked and stared at the cat. It stared back. As Mr\\nDursley drove around the corner and up the road, he watched the\\ncat in his mirror. It was now reading the sign that said Privet Drive\\n– no, looking at the sign; cats couldn’t read maps or signs. Mr\\nDursley gave himself a little shake and put the cat out of his\\nmind. As he drove towards town he thought of nothing except a\\nlarge order of drills he was hoping to get that day.\\n    But on the edge of town, drills were driven out of his mind by\\nsomething else. As he sat in the usual morning traffic jam, he\\ncouldn’t help noticing that there seemed to be a lot of strangely\\ndressed people about. People in cloaks. Mr Dursley couldn’t bear\\npeople who dressed in funny clothes – the get-ups you saw on\\nyoung people! He supposed this was some stupid new fashion. He\\ndrummed his fingers on the steering wheel and his eyes fell on a\\nhuddle of these weirdos standing quite close by. They were whis-\\npering excitedly together. Mr Dursley was enraged to see that a\\ncouple of them weren’t young at all; why, that man had to be older\\nthan he was, and wearing an emerald-green cloak! The nerve of\\nhim! But then it struck Mr Dursley that this was probably some\\nsilly stunt – these people were obviously collecting for something\\n... yes, that would be it. The traffic moved on, and a few minutes\\nlater, Mr Dursley arrived in the Grunnings car park, his mind\\nback on drills.\\n    Mr Dursley always sat with his back to the window in his office\\non the ninth floor. If he hadn’t, he might have found it harder to\\nconcentrate on drills that morning. He didn’t see the owls\\n\"\n [7] \"                       THE BOY WHO LIVED                            9\\n\\nswooping past in broad daylight, though people down in the\\nstreet did; they pointed and gazed open-mouthed as owl after owl\\nsped overhead. Most of them had never seen an owl even at night-\\ntime. Mr Dursley, however, had a perfectly normal, owl-free morn-\\ning. He yelled at five different people. He made several important\\ntelephone calls and shouted a bit more. He was in a very good\\nmood until lunch-time, when he thought he’d stretch his legs\\nand walk across the road to buy himself a bun from the baker’s\\nopposite.\\n    He’d forgotten all about the people in cloaks until he passed a\\ngroup of them next to the baker’s. He eyed them angrily as he\\npassed. He didn’t know why, but they made him uneasy. This lot\\nwere whispering excitedly, too, and he couldn’t see a single\\ncollecting tin. It was on his way back past them, clutching a large\\ndoughnut in a bag, that he caught a few words of what they were\\nsaying.\\n    ‘The Potters, that’s right, that’s what I heard –’\\n    ‘– yes, their son, Harry –’\\n    Mr Dursley stopped dead. Fear flooded him. He looked back at\\nthe whisperers as if he wanted to say something to them, but\\nthought better of it.\\n    He dashed back across the road, hurried up to his office,\\nsnapped at his secretary not to disturb him, seized his telephone\\nand had almost finished dialling his home number when he\\nchanged his mind. He put the receiver back down and stroked his\\nmoustache, thinking ... no, he was being stupid. Potter wasn’t\\nsuch an unusual name. He was sure there were lots of people\\ncalled Potter who had a son called Harry. Come to think of it, he\\nwasn’t even sure his nephew was called Harry. He’d never even\\nseen the boy. It might have been Harvey. Or Harold. There was no\\npoint in worrying Mrs Dursley, she always got so upset at any\\nmention of her sister. He didn’t blame her – if he’d had a sister like\\nthat ... but all the same, those people in cloaks ...\\n    He found it a lot harder to concentrate on drills that afternoon,\\nand when he left the building at five o’clock, he was still so\\nworried that he walked straight into someone just outside the door.\\n    ‘Sorry,’ he grunted, as the tiny old man stumbled and almost\\nfell. It was a few seconds before Mr Dursley realised that the man\\nwas wearing a violet cloak. He didn’t seem at all upset at being\\nalmost knocked to the ground. On the contrary, his face split into\\n\"                                                                              \n [8] \"10                         HARRY POTTER\\n\\na wide smile and he said in a squeaky voice that made passers-by\\nstare: ‘Don’t be sorry, my dear sir, for nothing could upset me\\ntoday! Rejoice, for You-Know-Who has gone at last! Even\\nMuggles like yourself should be celebrating, this happy happy\\nday!’\\n     And the old man hugged Mr Dursley around the middle and\\nwalked off.\\n     Mr Dursley stood rooted to the spot. He had been hugged by a\\ncomplete stranger. He also thought he had been called a Muggle,\\nwhatever that was. He was rattled. He hurried to his car and set\\noff home, hoping he was imagining things, which he had never\\nhoped before, because he didn’t approve of imagination.\\n     As he pulled into the driveway of number four, the first thing he\\nsaw – and it didn’t improve his mood – was the tabby cat he’d\\nspotted that morning. It was now sitting on his garden wall. He was\\nsure it was the same one; it had the same markings around its eyes.\\n     ‘Shoo!’ said Mr Dursley loudly.\\n     The cat didn’t move. It just gave him a stern look. Was this nor-\\nmal cat behaviour, Mr Dursley wondered. Trying to pull himself\\ntogether, he let himself into the house. He was still determined\\nnot to mention anything to his wife.\\n     Mrs Dursley had had a nice, normal day. She told him over din-\\nner all about Mrs Next Door’s problems with her daughter and\\nhow Dudley had learnt a new word (‘Shan’t!’). Mr Dursley tried to\\nact normally. When Dudley had been put to bed, he went into the\\nliving-room in time to catch the last report on the evening news:\\n     And finally, bird-watchers everywhere have reported that the\\nnation’s owls have been behaving very unusually today. Although\\nowls normally hunt at night and are hardly ever seen in daylight,\\nthere have been hundreds of sightings of these birds flying in\\nevery direction since sunrise. Experts are unable to explain why\\nthe owls have suddenly changed their sleeping pattern.’ The news\\nreader allowed himself a grin. ‘Most mysterious. And now, over to\\nJim McGuffin with the weather. Going to be any more showers of\\nowls tonight, Jim?’\\n     ‘Well, Ted,’ said the weatherman, ‘I don’t know about that, but\\nit’s not only the owls that have been acting oddly today. Viewers as\\nfar apart as Kent, Yorkshire and Dundee have been phoning in\\nto tell me that instead of the rain I promised yesterday, they’ve\\nhad a downpour of shooting stars! Perhaps people have been\\n\"                                                                                                         \n [9] \"                        THE BOY WHO LIVED                             11\\n\\ncelebrating Bonfire Night early – it’s not until next week, folks!\\nBut I can promise a wet night tonight.’\\n    Mr Dursley sat frozen in his armchair. Shooting stars all over\\nBritain? Owls flying by daylight? Mysterious people in cloaks all\\nover the place? And a whisper, a whisper about the Potters ...\\n    Mrs Dursley came into the living-room carrying two cups of\\ntea. It was no good. He’d have to say something to her. He cleared\\nhis throat nervously. ‘Er – Petunia, dear – you haven’t heard from\\nyour sister lately, have you?’\\n    As he had expected, Mrs Dursley looked shocked and angry.\\nAfter all, they normally pretended she didn’t have a sister.\\n    ‘No,’ she said sharply. ‘Why?’\\n    ‘Funny stuff on the news,’ Mr Dursley mumbled. ‘Owls ...\\nshooting stars ... and there were a lot of funny-looking people in\\ntown today ...’\\n    ‘So?’ snapped Mrs Dursley.\\n    ‘Well, I just thought ... maybe ... it was something to do with ...\\nyou know ... her lot.’\\n    Mrs Dursley sipped her tea through pursed lips. Mr Dursley\\nwondered whether he dared tell her he’d heard the name ‘Potter’.\\nHe decided he didn’t dare. Instead he said, as casually as he could,\\n‘Their son – he’d be about Dudley’s age now, wouldn’t he?’\\n    ‘I suppose so,’ said Mrs Dursley stiffly.\\n    ‘What’s his name again? Howard, isn’t it?’\\n    ‘Harry. Nasty, common name, if you ask me.’\\n    ‘Oh, yes,’ said Mr Dursley, his heart sinking horribly. ‘Yes, I\\nquite agree.’\\n    He didn’t say another word on the subject as they went upstairs\\nto bed. While Mrs Dursley was in the bathroom, Mr Dursley crept\\nto the bedroom window and peered down into the front garden.\\nThe cat was still there. It was staring down Privet Drive as though\\nit was waiting for something.\\n    Was he imagining things? Could all this have anything to do\\nwith the Potters? If it did ... if it got out that they were related to a\\npair of – well, he didn’t think he could bear it.\\n    The Dursleys got into bed. Mrs Dursley fell asleep quickly but\\nMr Dursley lay awake, turning it all over in his mind. His last,\\ncomforting thought before he fell asleep was that even if the\\nPotters were involved, there was no reason for them to come near\\nhim and Mrs Dursley. The Potters knew very well what he and\\n\"                                                                                                                                                                                                           \n[10] \"12                         HARRY POTTER\\n\\nPetunia thought about them and their kind ... He couldn’t see how\\nhe and Petunia could get mixed up in anything that might be\\ngoing on. He yawned and turned over. It couldn’t affect them ...\\n    How very wrong he was.\\n    Mr Dursley might have been drifting into an uneasy sleep, but\\nthe cat on the wall outside was showing no sign of sleepiness. It\\nwas sitting as still as a statue, its eyes fixed unblinkingly on the\\nfar corner of Privet Drive. It didn’t so much as quiver when a car\\ndoor slammed in the next street, nor when two owls swooped\\noverhead. In fact, it was nearly midnight before the cat moved at all.\\n    A man appeared on the corner the cat had been watching,\\nappeared so suddenly and silently you’d have thought he’d just\\npopped out of the ground. The cat’s tail twitched and its eyes\\nnarrowed.\\n    Nothing like this man had ever been seen in Privet Drive. He\\nwas tall, thin and very old, judging by the silver of his hair and\\nbeard, which were both long enough to tuck into his belt. He was\\nwearing long robes, a purple cloak which swept the ground and\\nhigh-heeled, buckled boots. His blue eyes were light, bright and\\nsparkling behind half-moon spectacles and his nose was very long\\nand crooked, as though it had been broken at least twice. This\\nman’s name was Albus Dumbledore.\\n    Albus Dumbledore didn’t seem to realise that he had just\\narrived in a street where everything from his name to his boots\\nwas unwelcome. He was busy rummaging in his cloak, looking for\\nsomething. But he did seem to realise he was being watched,\\nbecause he looked up suddenly at the cat, which was still staring\\nat him from the other end of the street. For some reason, the sight\\nof the cat seemed to amuse him. He chuckled and muttered, ‘I\\nshould have known.’\\n    He had found what he was looking for in his inside pocket. It\\nseemed to be a silver cigarette lighter. He flicked it open, held it\\nup in the air and clicked it. The nearest street lamp went out with\\na little pop. He clicked it again – the next lamp flickered into\\ndarkness. Twelve times he clicked the Put-Outer, until the only\\nlights left in the whole street were two tiny pinpricks in the dis-\\ntance, which were the eyes of the cat watching him. If anyone\\nlooked out of their window now, even beady-eyed Mrs Dursley,\\nthey wouldn’t be able to see anything that was happening down\\non the pavement. Dumbledore slipped the Put-Outer back inside\\n\"                                                                 \n[11] \"                        THE BOY WHO LIVED                            13\\n\\nhis cloak and set off down the street towards number four, where\\nhe sat down on the wall next to the cat. He didn’t look at it, but\\nafter a moment he spoke to it.\\n    ‘Fancy seeing you here, Professor McGonagall.’\\n    He turned to smile at the tabby, but it had gone. Instead he was\\nsmiling at a rather severe-looking woman who was wearing square\\nglasses exactly the shape of the markings the cat had had around\\nits eyes. She, too, was wearing a cloak, an emerald one. Her black\\nhair was drawn into a tight bun. She looked distinctly ruffled.\\n    ‘How did you know it was me?’ she asked.\\n    ‘My dear Professor, I’ve never seen a cat sit so stiffly.’\\n    ‘You’d be stiff if you’d been sitting on a brick wall all day,’ said\\nProfessor McGonagall.\\n    ‘All day? When you could have been celebrating? I must have\\npassed a dozen feasts and parties on my way here.’\\n    Professor McGonagall sniffed angrily.\\n    ‘Oh yes, everyone’s celebrating, all right,’ she said impatiently.\\n‘You’d think they’d be a bit more careful, but no – even the\\nMuggles have noticed something’s going on. It was on their news.’\\nShe jerked her head back at the Dursleys’ dark living-room\\nwindow. ‘I heard it. Flocks of owls ... shooting stars ... Well,\\nthey’re not completely stupid. They were bound to notice\\nsomething. Shooting stars down in Kent – I’ll bet that was Dedalus\\nDiggle. He never had much sense.’\\n    ‘You can’t blame them,’ said Dumbledore gently. ‘We’ve had\\nprecious little to celebrate for eleven years.’\\n    ‘I know that,’ said Professor McGonagall irritably. ‘But that’s no\\nreason to lose our heads. People are being downright careless, out\\non the streets in broad daylight, not even dressed in Muggle\\nclothes, swapping rumours.’\\n    She threw a sharp, sideways glance at Dumbledore here, as\\nthough hoping he was going to tell her something, but he didn’t,\\nso she went on: ‘A fine thing it would be if, on the very day You-\\nKnow-Who seems to have disappeared at last, the Muggles found\\nout about us all. I suppose he really has gone, Dumbledore?’\\n    ‘It certainly seems so,’ said Dumbledore. ‘We have much to be\\nthankful for. Would you care for a sherbet lemon?’\\n    ‘A what?’\\n    ‘A sherbet lemon. They’re a kind of Muggle sweet I’m rather\\nfond of.’\\n\"                                                                                                                                                                                                     \n[12] \"14                          HARRY POTTER\\n\\n    ‘No, thank you,’ said Professor McGonagall coldly, as though\\nshe didn’t think this was the moment for sherbet lemons. ‘As I say,\\neven if You-Know-Who has gone –’\\n    ‘My dear Professor, surely a sensible person like yourself can\\ncall him by his name? All this “You-Know-Who” nonsense – for\\neleven years I have been trying to persuade people to call him by\\nhis proper name: Voldemort.’ Professor McGonagall flinched, but\\nDumbledore, who was unsticking two sherbet lemons, seemed\\nnot to notice. ‘It all gets so confusing if we keep saying “You-\\nKnow-Who”.’ I have never seen any reason to be frightened of\\nsaying Voldemort’s name.’\\n    ‘I know you haven’t,’ said Professor McGonagall, sounding half-\\nexasperated, half-admiring. ‘But you’re different. Everyone knows\\nyou’re the only one You-Know – oh, all right, Voldemort – was\\nfrightened of.’\\n    ‘You flatter me,’ said Dumbledore calmly. ‘Voldemort had\\npowers I will never have.’\\n    ‘Only because you’re too – well – noble to use them.’\\n    ‘It’s lucky it’s dark. I haven’t blushed so much since Madam\\nPomfrey told me she liked my new earmuffs.’\\n    Professor McGonagall shot a sharp look at Dumbledore and\\nsaid, ‘The owls are nothing to the rumours that are flying around.\\nYou know what everyone’s saying? About why he’s disappeared?\\nAbout what finally stopped him?’\\n    It seemed that Professor McGonagall had reached the point she\\nwas most anxious to discuss, the real reason she had been waiting\\non a cold hard wall all day, for neither as a cat nor as a woman\\nhad she fixed Dumbledore with such a piercing stare as she did\\nnow. It was plain that whatever ‘everyone’ was saying, she was not\\ngoing to believe it until Dumbledore told her it was true.\\nDumbledore, however, was choosing another sherbet lemon and\\ndid not answer.\\n    ‘What they’re saying,’ she pressed on, ‘is that last night Voldemort\\nturned up in Godric’s Hollow. He went to find the Potters. The\\nrumour is that Lily and James Potter are – are – that they’re –\\ndead.’\\n    Dumbledore bowed his head. Professor McGonagall gasped.\\n    ‘Lily and James ... I can’t believe it ... I didn’t want to believe it\\n... Oh, Albus ...’\\n    Dumbledore reached out and patted her on the shoulder. ‘I\\n\"                                                                                                                                                                                                                                                                       \n[13] \"                         THE BOY WHO LIVED                              15\\n\\nknow ... I know ...’ he said heavily.\\n    Professor McGonagall’s voice trembled as she went on. ‘That’s\\nnot all. They’re saying he tried to kill the Potters’ son, Harry. But –\\nhe couldn’t. He couldn’t kill that little boy. No one knows why, or\\nhow, but they’re saying that when he couldn’t kill Harry Potter,\\nVoldemort’s power somehow broke – and that’s why he’s gone.’\\n    Dumbledore nodded glumly.\\n    ‘It’s – it’s true?’ faltered Professor McGonagall. ‘After all he’s\\ndone ... all the people he’s killed ... he couldn’t kill a little boy? It’s\\njust astounding ... of all the things to stop him ... but how in the\\nname of heaven did Harry survive?’\\n    ‘We can only guess,’ said Dumbledore. ‘We may never know.’\\n    Professor McGonagall pulled out a lace handkerchief and\\ndabbed at her eyes beneath her spectacles. Dumbledore gave a\\ngreat sniff as he took a golden watch from his pocket and examined\\nit. It was a very odd watch. It had twelve hands but no numbers;\\ninstead, little planets were moving around the edge. It must have\\nmade sense to Dumbledore, though, because he put it back in his\\npocket and said, ‘Hagrid’s late. I suppose it was he who told you\\nI’d be here, by the way?’\\n    ‘Yes,’ said Professor McGonagall. ‘And I don’t suppose you’re\\ngoing to tell me why you’re here, of all places?’\\n    ‘I’ve come to bring Harry to his aunt and uncle. They’re the\\nonly family he has left now.’\\n    ‘You don’t mean – you can’t mean the people who live here?’\\ncried Professor McGonagall, jumping to her feet and pointing at\\nnumber four. ‘Dumbledore – you can’t. I’ve been watching them\\nall day. You couldn’t find two people who are less like us. And\\nthey’ve got this son – I saw him kicking his mother all the way up\\nthe street, screaming for sweets. Harry Potter come and live here!’\\n    ‘It’s the best place for him,’ said Dumbledore firmly. ‘His aunt\\nand uncle will be able to explain everything to him when he’s\\nolder. I’ve written them a letter.’\\n    ‘A letter?’ repeated Professor McGonagall faintly, sitting back\\ndown on the wall. ‘Really, Dumbledore, you think you can explain\\nall this in a letter? These people will never understand him! He’ll\\nbe famous – a legend – I wouldn’t be surprised if today was\\nknown as Harry Potter Day in future – there will be books written\\nabout Harry – every child in our world will know his name!’\\n    ‘Exactly,’ said Dumbledore, looking very seriously over the top\\n\"                          \n[14] \"16                         HARRY POTTER\\n\\nof his half-moon glasses. ‘It would be enough to turn any boy’s\\nhead. Famous before he can walk and talk! Famous for something\\nhe won’t even remember! Can’t you see how much better off he’ll\\nbe, growing up away from all that until he’s ready to take it?’\\n    Professor McGonagall opened her mouth, changed her mind,\\nswallowed and then said, ‘Yes – yes, you’re right, of course. But\\nhow is the boy getting here, Dumbledore?’ She eyed his cloak\\nsuddenly as though she thought he might be hiding Harry\\nunderneath it.\\n    ‘Hagrid’s bringing him.’\\n    ‘You think it – wise – to trust Hagrid with something as impor-\\ntant as this?’\\n    ‘I would trust Hagrid with my life,’ said Dumbledore.\\n    ‘I’m not saying his heart isn’t in the right place,’ said Professor\\nMcGonagall grudgingly, ‘but you can’t pretend he’s not careless.\\nHe does tend to – what was that?’\\n    A low rumbling sound had broken the silence around them. It\\ngrew steadily louder as they looked up and down the street for\\nsome sign of a headlight; it swelled to a roar as they both looked\\nup at the sky – and a huge motorbike fell out of the air and landed\\non the road in front of them.\\n    If the motorbike was huge, it was nothing to the man sitting\\nastride it. He was almost twice as tall as a normal man and at least\\nfive times as wide. He looked simply too big to be allowed, and so\\nwild – long tangles of bushy black hair and beard hid most of his\\nface, he had hands the size of dustbin lids and his feet in their\\nleather boots were like baby dolphins. In his vast, muscular arms\\nhe was holding a bundle of blankets.\\n    ‘Hagrid,’ said Dumbledore, sounding relieved. ‘At last. And\\nwhere did you get that motorbike?’\\n    ‘Borrowed it, Professor Dumbledore, sir,’ said the giant, climbing\\ncarefully off the motorbike as he spoke. ‘Young Sirius Black lent it\\nme. I’ve got him, sir.’\\n    ‘No problems, were there?’\\n    ‘No, sir – house was almost destroyed but I got him out all\\nright before the Muggles started swarmin’ around. He fell asleep\\nas we was flyin’ over Bristol.’\\n    Dumbledore and Professor McGonagall bent forward over the\\nbundle of blankets. Inside, just visible, was a baby boy, fast asleep.\\nUnder a tuft of jet-black hair over his forehead they could see a\\n\"                                                                                                                                                                                                                                           \n[15] \"                        THE BOY WHO LIVED                            17\\n\\ncuriously shaped cut, like a bolt of lightning.\\n    ‘Is that where –?’ whispered Professor McGonagall.\\n    ‘Yes,’ said Dumbledore. ‘He’ll have that scar for ever.’\\n    ‘Couldn’t you do something about it, Dumbledore?’\\n    ‘Even if I could, I wouldn’t. Scars can come in useful. I have\\none myself above my left knee which is a perfect map of the\\nLondon Underground. Well – give him here, Hagrid – we’d better\\nget this over with.’\\n    Dumbledore took Harry in his arms and turned towards the\\nDursleys’ house.\\n    ‘Could I – could I say goodbye to him, sir?’ asked Hagrid.\\n    He bent his great, shaggy head over Harry and gave him what\\nmust have been a very scratchy, whiskery kiss. Then, suddenly,\\nHagrid let out a howl like a wounded dog.\\n    ‘Shhh!’ hissed Professor McGonagall. ‘You’ll wake the Muggles!’\\n    ‘S-s-sorry,’ sobbed Hagrid, taking out a large spotted handker-\\nchief and burying his face in it. ‘But I c-c-can’t stand it – Lily an’\\nJames dead – an’ poor little Harry off ter live with Muggles –’\\n    ‘Yes, yes, it’s all very sad, but get a grip on yourself, Hagrid, or\\nwe’ll be found,’ Professor McGonagall whispered, patting Hagrid\\ngingerly on the arm as Dumbledore stepped over the low garden\\nwall and walked to the front door. He laid Harry gently on the\\ndoorstep, took a letter out of his cloak, tucked it inside Harry’s\\nblankets and then came back to the other two. For a full minute\\nthe three of them stood and looked at the little bundle; Hagrid’s\\nshoulders shook, Professor McGonagall blinked furiously and the\\ntwinkling light that usually shone from Dumbledore’s eyes seemed\\nto have gone out.\\n    ‘Well,’ said Dumbledore finally, ‘that’s that. We’ve no business\\nstaying here. We may as well go and join the celebrations.’\\n    ‘Yeah,’ said Hagrid in a very muffled voice. ‘I’d best get\\nthis bike away. G’night, Professor McGonagall – Professor\\nDumbledore, sir.’\\n    Wiping his streaming eyes on his jacket sleeve, Hagrid swung\\nhimself on to the motorbike and kicked the engine into life; with\\na roar it rose into the air and off into the night.\\n    ‘I shall see you soon, I expect, Professor McGonagall,’ said\\nDumbledore, nodding to her. Professor McGonagall blew her nose\\nin reply.\\n    Dumbledore turned and walked back down the street. On the\\n\"                                                                                                                                                                             \n[16] \"18                       HARRY POTTER\\n\\ncorner he stopped and took out the silver Put-Outer. He clicked it\\nonce and twelve balls of light sped back to their street lamps so\\nthat Privet Drive glowed suddenly orange and he could make out\\na tabby cat slinking around the corner at the other end of the\\nstreet. He could just see the bundle of blankets on the step of\\nnumber four.\\n    ‘Good luck, Harry,’ he murmured. He turned on his heel and\\nwith a swish of his cloak he was gone.\\n    A breeze ruffled the neat hedges of Privet Drive, which lay\\nsilent and tidy under the inky sky, the very last place you would\\nexpect astonishing things to happen. Harry Potter rolled over\\ninside his blankets without waking up. One small hand closed on\\nthe letter beside him and he slept on, not knowing he was special,\\nnot knowing he was famous, not knowing he would be woken in\\na few hours’ time by Mrs Dursley’s scream as she opened the front\\ndoor to put out the milk bottles, nor that he would spend the next\\nfew weeks being prodded and pinched by his cousin Dudley ... He\\ncouldn’t know that at this very moment, people meeting in secret\\nall over the country were holding up their glasses and saying in\\nhushed voices: ‘To Harry Potter – the boy who lived!’\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n\nlength(test)\n\n[1] 5\n\nlibrary(purrr)\nlapply(test, length)\n\n[[1]]\n[1] 16\n\n[[2]]\n[1] 9\n\n[[3]]\n[1] 11\n\n[[4]]\n[1] 10\n\n[[5]]\n[1] 18\n\nmap(test, length)\n\n[[1]]\n[1] 16\n\n[[2]]\n[1] 9\n\n[[3]]\n[1] 11\n\n[[4]]\n[1] 10\n\n[[5]]\n[1] 18\n\n#shows the pages of each file\n\nlibrary(tm)\n\nLoading required package: NLP\n\n\n\nAttaching package: 'NLP'\n\n\nThe following objects are masked from 'package:quanteda':\n\n    meta, meta&lt;-\n\n\n\nAttaching package: 'tm'\n\n\nThe following object is masked from 'package:quanteda':\n\n    stopwords\n\ntest.corpus &lt;- Corpus(URISource(file_list),\n                      readerControl = list(reader = readPDF))\n\ntest.corpus # tm corpus\n\n&lt;&lt;VCorpus&gt;&gt;\nMetadata:  corpus specific: 0, document level (indexed): 0\nContent:  documents: 5\n\ntest.corpus.quanteda &lt;- corpus(test.corpus)\nsummary(test.corpus.quanteda)\n\nCorpus consisting of 5 documents, showing 5 documents:\n\n  Text Types Tokens Sentences       author       datetimestamp\n text1  1403   5865       361 J.K. Rowling 2023-09-13 14:04:05\n text2  1083   4132       239 J.K. Rowling 2023-09-13 14:05:02\n text3  1237   4649       299 J.K. Rowling 2023-09-13 14:06:18\n text4  1207   4809       326 J.K. Rowling 2023-09-13 14:06:43\n text5  1867   8405       565 J.K. Rowling 2023-09-13 14:07:14\n                                                                    description\n Reference Quality Electronic Book Version of the British Bloomsbury Hard Cover\n Reference Quality Electronic Book Version of the British Bloomsbury Hard Cover\n Reference Quality Electronic Book Version of the British Bloomsbury Hard Cover\n Reference Quality Electronic Book Version of the British Bloomsbury Hard Cover\n Reference Quality Electronic Book Version of the British Bloomsbury Hard Cover\n                                  heading           id language\n Harry Potter and the Philosopher's Stone chapter1.pdf       en\n Harry Potter and the Philosopher's Stone chapter2.pdf       en\n Harry Potter and the Philosopher's Stone chapter3.pdf       en\n Harry Potter and the Philosopher's Stone chapter4.pdf       en\n Harry Potter and the Philosopher's Stone chapter5.pdf       en\n                   origin\n PScript5.dll Version 5.2\n PScript5.dll Version 5.2\n PScript5.dll Version 5.2\n PScript5.dll Version 5.2\n PScript5.dll Version 5.2"
  },
  {
    "objectID": "readingPDF.html#reading-all-files-in-a-directory",
    "href": "readingPDF.html#reading-all-files-in-a-directory",
    "title": "Reading all PDF files in a directory",
    "section": "",
    "text": "# list.files(): to list all files in a directory\n# returns a character vector containing the names of the files in the specified directory\n# syntax: list.files(path, pattern, all.files, full.names)\n\nlist.files(path=\"~/Documents/R Practice/Quarto/TAD23/HarryPotter\", pattern = NULL,\n           all.files = FALSE, full.names = FALSE)\n\n[1] \"chapter1.pdf\" \"chapter2.pdf\" \"chapter3.pdf\" \"chapter4.pdf\" \"chapter5.pdf\"\n\nlist.files(path = \"~/Documents/R Practice/Quarto/TAD23/HarryPotter\", pattern = \".pdf\", all.files = TRUE, full.names=TRUE)\n\n[1] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter1.pdf\"\n[2] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter2.pdf\"\n[3] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter3.pdf\"\n[4] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter4.pdf\"\n[5] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter5.pdf\"\n\n\n\nrequire(pdftools)\n\nLoading required package: pdftools\n\n\nUsing poppler version 22.02.0\n\nlibrary(quanteda)\n\nPackage version: 3.3.1\nUnicode version: 14.0\nICU version: 70.1\n\n\nParallel computing: 4 of 4 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\n# create a list of the files in the designated directory\nfile_list &lt;- list.files(path = \"~/Documents/R Practice/Quarto/TAD23/HarryPotter\", pattern = \".pdf\", full.names = TRUE)\n\nfile_list\n\n[1] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter1.pdf\"\n[2] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter2.pdf\"\n[3] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter3.pdf\"\n[4] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter4.pdf\"\n[5] \"/Users/mpang/Documents/R Practice/Quarto/TAD23/HarryPotter/chapter5.pdf\"\n\ntest &lt;- lapply(file_list, pdf_text) # pdf_text: extracting text\n\ntest[1]\n\n[[1]]\n [1] \"Harry Potter and the\\nPhilosopher’s Stone\\n\\n      J. K. Rowling\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n [2] \"     All rights reserved; no part of this publication may be reproduced or\\n       transmitted by any means, electronic, mechanical, photocopying\\n          or otherwise, without the prior permission of the publisher\\n\\n                  First published in Great Britain in 1997\\n       Bloomsbury Publishing Plc, 36 Soho Square, London, W1D 3QY\\n\\n                     This edition first published in 2004\\n\\n                       Copyright © 1997 J. K. Rowling\\n\\n            Harry Potter, names, characters and related indicia are\\n               copyright and trademark Warner Bros., 2000™\\n\\n               The moral right of the author has been asserted\\n   A CIP catalogue record of this book is available from the British Library\\n\\n                           ISBN 978 0 7475 7360 9\\n\\nThe paper this book is printed on is certified by the © 1996 Forest Stewardship\\n      Council A.C. (FSC). It is ancient-forest friendly. The printer holds\\n                    FSC chain of custody SGS-COC-2061.\\n                                                       ©\\n\\n\\n\\n                                           F SC\\n                                     Mixed Sources\\n                                 Product group from well-managed\\n                               forests and other controlled sources\\n\\n                                    Cert no. SGS-COC-2061\\n                                         www.fsc.org\\n                               ©1996 Forest Stewardship Council\\n\\n\\n\\n               Printed in Great Britain by Clays Ltd, St Ives plc\\n                     Typeset by Dorchester Typesetting\\n\\n                                  5 7 9 10 8 6 4\\n\\n                      www.bloomsbury.com/harrypotter\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n [3] \"   for Jessica, who loves stories,\\n  for Anne, who loved them too,\\nand for Di, who heard this one first.\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n [4] \"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n [5] \"                     — CHAPTER ONE —\\n\\n\\n\\n         The Boy Who Lived\\nMr and Mrs Dursley, of number four, Privet Drive, were proud to\\nsay that they were perfectly normal, thank you very much. They\\nwere the last people you’d expect to be involved in anything\\nstrange or mysterious, because they just didn’t hold with such\\nnonsense.\\n    Mr Dursley was the director of a firm called Grunnings, which\\nmade drills. He was a big, beefy man with hardly any neck,\\nalthough he did have a very large moustache. Mrs Dursley was\\nthin and blonde and had nearly twice the usual amount of neck,\\nwhich came in very useful as she spent so much of her time craning\\nover garden fences, spying on the neighbours. The Dursleys had a\\nsmall son called Dudley and in their opinion there was no finer\\nboy anywhere.\\n    The Dursleys had everything they wanted, but they also had a\\nsecret, and their greatest fear was that somebody would discover\\nit. They didn’t think they could bear it if anyone found out about\\nthe Potters. Mrs Potter was Mrs Dursley’s sister, but they hadn’t\\nmet for several years; in fact, Mrs Dursley pretended she didn’t\\nhave a sister, because her sister and her good-for-nothing husband\\nwere as unDursleyish as it was possible to be. The Dursleys\\nshuddered to think what the neighbours would say if the Potters\\narrived in the street. The Dursleys knew that the Potters had a\\nsmall son, too, but they had never even seen him. This boy was\\nanother good reason for keeping the Potters away; they didn’t\\nwant Dudley mixing with a child like that.\\n    When Mr and Mrs Dursley woke up on the dull, grey Tuesday\\nour story starts, there was nothing about the cloudy sky outside to\\nsuggest that strange and mysterious things would soon be hap-\\npening all over the country. Mr Dursley hummed as he picked out\\nhis most boring tie for work and Mrs Dursley gossiped away\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n [6] \"8                          HARRY POTTER\\n\\nhappily as she wrestled a screaming Dudley into his high chair.\\n    None of them noticed a large tawny owl flutter past the window.\\n    At half past eight, Mr Dursley picked up his briefcase, pecked\\nMrs Dursley on the cheek and tried to kiss Dudley goodbye but\\nmissed, because Dudley was now having a tantrum and throwing\\nhis cereal at the walls. ‘Little tyke,’ chortled Mr Dursley as he left\\nthe house. He got into his car and backed out of number four’s\\ndrive.\\n    It was on the corner of the street that he noticed the first sign\\nof something peculiar – a cat reading a map. For a second, Mr\\nDursley didn’t realise what he had seen – then he jerked his head\\naround to look again. There was a tabby cat standing on the corner\\nof Privet Drive, but there wasn’t a map in sight. What could\\nhe have been thinking of? It must have been a trick of the light.\\nMr Dursley blinked and stared at the cat. It stared back. As Mr\\nDursley drove around the corner and up the road, he watched the\\ncat in his mirror. It was now reading the sign that said Privet Drive\\n– no, looking at the sign; cats couldn’t read maps or signs. Mr\\nDursley gave himself a little shake and put the cat out of his\\nmind. As he drove towards town he thought of nothing except a\\nlarge order of drills he was hoping to get that day.\\n    But on the edge of town, drills were driven out of his mind by\\nsomething else. As he sat in the usual morning traffic jam, he\\ncouldn’t help noticing that there seemed to be a lot of strangely\\ndressed people about. People in cloaks. Mr Dursley couldn’t bear\\npeople who dressed in funny clothes – the get-ups you saw on\\nyoung people! He supposed this was some stupid new fashion. He\\ndrummed his fingers on the steering wheel and his eyes fell on a\\nhuddle of these weirdos standing quite close by. They were whis-\\npering excitedly together. Mr Dursley was enraged to see that a\\ncouple of them weren’t young at all; why, that man had to be older\\nthan he was, and wearing an emerald-green cloak! The nerve of\\nhim! But then it struck Mr Dursley that this was probably some\\nsilly stunt – these people were obviously collecting for something\\n... yes, that would be it. The traffic moved on, and a few minutes\\nlater, Mr Dursley arrived in the Grunnings car park, his mind\\nback on drills.\\n    Mr Dursley always sat with his back to the window in his office\\non the ninth floor. If he hadn’t, he might have found it harder to\\nconcentrate on drills that morning. He didn’t see the owls\\n\"\n [7] \"                       THE BOY WHO LIVED                            9\\n\\nswooping past in broad daylight, though people down in the\\nstreet did; they pointed and gazed open-mouthed as owl after owl\\nsped overhead. Most of them had never seen an owl even at night-\\ntime. Mr Dursley, however, had a perfectly normal, owl-free morn-\\ning. He yelled at five different people. He made several important\\ntelephone calls and shouted a bit more. He was in a very good\\nmood until lunch-time, when he thought he’d stretch his legs\\nand walk across the road to buy himself a bun from the baker’s\\nopposite.\\n    He’d forgotten all about the people in cloaks until he passed a\\ngroup of them next to the baker’s. He eyed them angrily as he\\npassed. He didn’t know why, but they made him uneasy. This lot\\nwere whispering excitedly, too, and he couldn’t see a single\\ncollecting tin. It was on his way back past them, clutching a large\\ndoughnut in a bag, that he caught a few words of what they were\\nsaying.\\n    ‘The Potters, that’s right, that’s what I heard –’\\n    ‘– yes, their son, Harry –’\\n    Mr Dursley stopped dead. Fear flooded him. He looked back at\\nthe whisperers as if he wanted to say something to them, but\\nthought better of it.\\n    He dashed back across the road, hurried up to his office,\\nsnapped at his secretary not to disturb him, seized his telephone\\nand had almost finished dialling his home number when he\\nchanged his mind. He put the receiver back down and stroked his\\nmoustache, thinking ... no, he was being stupid. Potter wasn’t\\nsuch an unusual name. He was sure there were lots of people\\ncalled Potter who had a son called Harry. Come to think of it, he\\nwasn’t even sure his nephew was called Harry. He’d never even\\nseen the boy. It might have been Harvey. Or Harold. There was no\\npoint in worrying Mrs Dursley, she always got so upset at any\\nmention of her sister. He didn’t blame her – if he’d had a sister like\\nthat ... but all the same, those people in cloaks ...\\n    He found it a lot harder to concentrate on drills that afternoon,\\nand when he left the building at five o’clock, he was still so\\nworried that he walked straight into someone just outside the door.\\n    ‘Sorry,’ he grunted, as the tiny old man stumbled and almost\\nfell. It was a few seconds before Mr Dursley realised that the man\\nwas wearing a violet cloak. He didn’t seem at all upset at being\\nalmost knocked to the ground. On the contrary, his face split into\\n\"                                                                              \n [8] \"10                         HARRY POTTER\\n\\na wide smile and he said in a squeaky voice that made passers-by\\nstare: ‘Don’t be sorry, my dear sir, for nothing could upset me\\ntoday! Rejoice, for You-Know-Who has gone at last! Even\\nMuggles like yourself should be celebrating, this happy happy\\nday!’\\n     And the old man hugged Mr Dursley around the middle and\\nwalked off.\\n     Mr Dursley stood rooted to the spot. He had been hugged by a\\ncomplete stranger. He also thought he had been called a Muggle,\\nwhatever that was. He was rattled. He hurried to his car and set\\noff home, hoping he was imagining things, which he had never\\nhoped before, because he didn’t approve of imagination.\\n     As he pulled into the driveway of number four, the first thing he\\nsaw – and it didn’t improve his mood – was the tabby cat he’d\\nspotted that morning. It was now sitting on his garden wall. He was\\nsure it was the same one; it had the same markings around its eyes.\\n     ‘Shoo!’ said Mr Dursley loudly.\\n     The cat didn’t move. It just gave him a stern look. Was this nor-\\nmal cat behaviour, Mr Dursley wondered. Trying to pull himself\\ntogether, he let himself into the house. He was still determined\\nnot to mention anything to his wife.\\n     Mrs Dursley had had a nice, normal day. She told him over din-\\nner all about Mrs Next Door’s problems with her daughter and\\nhow Dudley had learnt a new word (‘Shan’t!’). Mr Dursley tried to\\nact normally. When Dudley had been put to bed, he went into the\\nliving-room in time to catch the last report on the evening news:\\n     And finally, bird-watchers everywhere have reported that the\\nnation’s owls have been behaving very unusually today. Although\\nowls normally hunt at night and are hardly ever seen in daylight,\\nthere have been hundreds of sightings of these birds flying in\\nevery direction since sunrise. Experts are unable to explain why\\nthe owls have suddenly changed their sleeping pattern.’ The news\\nreader allowed himself a grin. ‘Most mysterious. And now, over to\\nJim McGuffin with the weather. Going to be any more showers of\\nowls tonight, Jim?’\\n     ‘Well, Ted,’ said the weatherman, ‘I don’t know about that, but\\nit’s not only the owls that have been acting oddly today. Viewers as\\nfar apart as Kent, Yorkshire and Dundee have been phoning in\\nto tell me that instead of the rain I promised yesterday, they’ve\\nhad a downpour of shooting stars! Perhaps people have been\\n\"                                                                                                         \n [9] \"                        THE BOY WHO LIVED                             11\\n\\ncelebrating Bonfire Night early – it’s not until next week, folks!\\nBut I can promise a wet night tonight.’\\n    Mr Dursley sat frozen in his armchair. Shooting stars all over\\nBritain? Owls flying by daylight? Mysterious people in cloaks all\\nover the place? And a whisper, a whisper about the Potters ...\\n    Mrs Dursley came into the living-room carrying two cups of\\ntea. It was no good. He’d have to say something to her. He cleared\\nhis throat nervously. ‘Er – Petunia, dear – you haven’t heard from\\nyour sister lately, have you?’\\n    As he had expected, Mrs Dursley looked shocked and angry.\\nAfter all, they normally pretended she didn’t have a sister.\\n    ‘No,’ she said sharply. ‘Why?’\\n    ‘Funny stuff on the news,’ Mr Dursley mumbled. ‘Owls ...\\nshooting stars ... and there were a lot of funny-looking people in\\ntown today ...’\\n    ‘So?’ snapped Mrs Dursley.\\n    ‘Well, I just thought ... maybe ... it was something to do with ...\\nyou know ... her lot.’\\n    Mrs Dursley sipped her tea through pursed lips. Mr Dursley\\nwondered whether he dared tell her he’d heard the name ‘Potter’.\\nHe decided he didn’t dare. Instead he said, as casually as he could,\\n‘Their son – he’d be about Dudley’s age now, wouldn’t he?’\\n    ‘I suppose so,’ said Mrs Dursley stiffly.\\n    ‘What’s his name again? Howard, isn’t it?’\\n    ‘Harry. Nasty, common name, if you ask me.’\\n    ‘Oh, yes,’ said Mr Dursley, his heart sinking horribly. ‘Yes, I\\nquite agree.’\\n    He didn’t say another word on the subject as they went upstairs\\nto bed. While Mrs Dursley was in the bathroom, Mr Dursley crept\\nto the bedroom window and peered down into the front garden.\\nThe cat was still there. It was staring down Privet Drive as though\\nit was waiting for something.\\n    Was he imagining things? Could all this have anything to do\\nwith the Potters? If it did ... if it got out that they were related to a\\npair of – well, he didn’t think he could bear it.\\n    The Dursleys got into bed. Mrs Dursley fell asleep quickly but\\nMr Dursley lay awake, turning it all over in his mind. His last,\\ncomforting thought before he fell asleep was that even if the\\nPotters were involved, there was no reason for them to come near\\nhim and Mrs Dursley. The Potters knew very well what he and\\n\"                                                                                                                                                                                                           \n[10] \"12                         HARRY POTTER\\n\\nPetunia thought about them and their kind ... He couldn’t see how\\nhe and Petunia could get mixed up in anything that might be\\ngoing on. He yawned and turned over. It couldn’t affect them ...\\n    How very wrong he was.\\n    Mr Dursley might have been drifting into an uneasy sleep, but\\nthe cat on the wall outside was showing no sign of sleepiness. It\\nwas sitting as still as a statue, its eyes fixed unblinkingly on the\\nfar corner of Privet Drive. It didn’t so much as quiver when a car\\ndoor slammed in the next street, nor when two owls swooped\\noverhead. In fact, it was nearly midnight before the cat moved at all.\\n    A man appeared on the corner the cat had been watching,\\nappeared so suddenly and silently you’d have thought he’d just\\npopped out of the ground. The cat’s tail twitched and its eyes\\nnarrowed.\\n    Nothing like this man had ever been seen in Privet Drive. He\\nwas tall, thin and very old, judging by the silver of his hair and\\nbeard, which were both long enough to tuck into his belt. He was\\nwearing long robes, a purple cloak which swept the ground and\\nhigh-heeled, buckled boots. His blue eyes were light, bright and\\nsparkling behind half-moon spectacles and his nose was very long\\nand crooked, as though it had been broken at least twice. This\\nman’s name was Albus Dumbledore.\\n    Albus Dumbledore didn’t seem to realise that he had just\\narrived in a street where everything from his name to his boots\\nwas unwelcome. He was busy rummaging in his cloak, looking for\\nsomething. But he did seem to realise he was being watched,\\nbecause he looked up suddenly at the cat, which was still staring\\nat him from the other end of the street. For some reason, the sight\\nof the cat seemed to amuse him. He chuckled and muttered, ‘I\\nshould have known.’\\n    He had found what he was looking for in his inside pocket. It\\nseemed to be a silver cigarette lighter. He flicked it open, held it\\nup in the air and clicked it. The nearest street lamp went out with\\na little pop. He clicked it again – the next lamp flickered into\\ndarkness. Twelve times he clicked the Put-Outer, until the only\\nlights left in the whole street were two tiny pinpricks in the dis-\\ntance, which were the eyes of the cat watching him. If anyone\\nlooked out of their window now, even beady-eyed Mrs Dursley,\\nthey wouldn’t be able to see anything that was happening down\\non the pavement. Dumbledore slipped the Put-Outer back inside\\n\"                                                                 \n[11] \"                        THE BOY WHO LIVED                            13\\n\\nhis cloak and set off down the street towards number four, where\\nhe sat down on the wall next to the cat. He didn’t look at it, but\\nafter a moment he spoke to it.\\n    ‘Fancy seeing you here, Professor McGonagall.’\\n    He turned to smile at the tabby, but it had gone. Instead he was\\nsmiling at a rather severe-looking woman who was wearing square\\nglasses exactly the shape of the markings the cat had had around\\nits eyes. She, too, was wearing a cloak, an emerald one. Her black\\nhair was drawn into a tight bun. She looked distinctly ruffled.\\n    ‘How did you know it was me?’ she asked.\\n    ‘My dear Professor, I’ve never seen a cat sit so stiffly.’\\n    ‘You’d be stiff if you’d been sitting on a brick wall all day,’ said\\nProfessor McGonagall.\\n    ‘All day? When you could have been celebrating? I must have\\npassed a dozen feasts and parties on my way here.’\\n    Professor McGonagall sniffed angrily.\\n    ‘Oh yes, everyone’s celebrating, all right,’ she said impatiently.\\n‘You’d think they’d be a bit more careful, but no – even the\\nMuggles have noticed something’s going on. It was on their news.’\\nShe jerked her head back at the Dursleys’ dark living-room\\nwindow. ‘I heard it. Flocks of owls ... shooting stars ... Well,\\nthey’re not completely stupid. They were bound to notice\\nsomething. Shooting stars down in Kent – I’ll bet that was Dedalus\\nDiggle. He never had much sense.’\\n    ‘You can’t blame them,’ said Dumbledore gently. ‘We’ve had\\nprecious little to celebrate for eleven years.’\\n    ‘I know that,’ said Professor McGonagall irritably. ‘But that’s no\\nreason to lose our heads. People are being downright careless, out\\non the streets in broad daylight, not even dressed in Muggle\\nclothes, swapping rumours.’\\n    She threw a sharp, sideways glance at Dumbledore here, as\\nthough hoping he was going to tell her something, but he didn’t,\\nso she went on: ‘A fine thing it would be if, on the very day You-\\nKnow-Who seems to have disappeared at last, the Muggles found\\nout about us all. I suppose he really has gone, Dumbledore?’\\n    ‘It certainly seems so,’ said Dumbledore. ‘We have much to be\\nthankful for. Would you care for a sherbet lemon?’\\n    ‘A what?’\\n    ‘A sherbet lemon. They’re a kind of Muggle sweet I’m rather\\nfond of.’\\n\"                                                                                                                                                                                                     \n[12] \"14                          HARRY POTTER\\n\\n    ‘No, thank you,’ said Professor McGonagall coldly, as though\\nshe didn’t think this was the moment for sherbet lemons. ‘As I say,\\neven if You-Know-Who has gone –’\\n    ‘My dear Professor, surely a sensible person like yourself can\\ncall him by his name? All this “You-Know-Who” nonsense – for\\neleven years I have been trying to persuade people to call him by\\nhis proper name: Voldemort.’ Professor McGonagall flinched, but\\nDumbledore, who was unsticking two sherbet lemons, seemed\\nnot to notice. ‘It all gets so confusing if we keep saying “You-\\nKnow-Who”.’ I have never seen any reason to be frightened of\\nsaying Voldemort’s name.’\\n    ‘I know you haven’t,’ said Professor McGonagall, sounding half-\\nexasperated, half-admiring. ‘But you’re different. Everyone knows\\nyou’re the only one You-Know – oh, all right, Voldemort – was\\nfrightened of.’\\n    ‘You flatter me,’ said Dumbledore calmly. ‘Voldemort had\\npowers I will never have.’\\n    ‘Only because you’re too – well – noble to use them.’\\n    ‘It’s lucky it’s dark. I haven’t blushed so much since Madam\\nPomfrey told me she liked my new earmuffs.’\\n    Professor McGonagall shot a sharp look at Dumbledore and\\nsaid, ‘The owls are nothing to the rumours that are flying around.\\nYou know what everyone’s saying? About why he’s disappeared?\\nAbout what finally stopped him?’\\n    It seemed that Professor McGonagall had reached the point she\\nwas most anxious to discuss, the real reason she had been waiting\\non a cold hard wall all day, for neither as a cat nor as a woman\\nhad she fixed Dumbledore with such a piercing stare as she did\\nnow. It was plain that whatever ‘everyone’ was saying, she was not\\ngoing to believe it until Dumbledore told her it was true.\\nDumbledore, however, was choosing another sherbet lemon and\\ndid not answer.\\n    ‘What they’re saying,’ she pressed on, ‘is that last night Voldemort\\nturned up in Godric’s Hollow. He went to find the Potters. The\\nrumour is that Lily and James Potter are – are – that they’re –\\ndead.’\\n    Dumbledore bowed his head. Professor McGonagall gasped.\\n    ‘Lily and James ... I can’t believe it ... I didn’t want to believe it\\n... Oh, Albus ...’\\n    Dumbledore reached out and patted her on the shoulder. ‘I\\n\"                                                                                                                                                                                                                                                                       \n[13] \"                         THE BOY WHO LIVED                              15\\n\\nknow ... I know ...’ he said heavily.\\n    Professor McGonagall’s voice trembled as she went on. ‘That’s\\nnot all. They’re saying he tried to kill the Potters’ son, Harry. But –\\nhe couldn’t. He couldn’t kill that little boy. No one knows why, or\\nhow, but they’re saying that when he couldn’t kill Harry Potter,\\nVoldemort’s power somehow broke – and that’s why he’s gone.’\\n    Dumbledore nodded glumly.\\n    ‘It’s – it’s true?’ faltered Professor McGonagall. ‘After all he’s\\ndone ... all the people he’s killed ... he couldn’t kill a little boy? It’s\\njust astounding ... of all the things to stop him ... but how in the\\nname of heaven did Harry survive?’\\n    ‘We can only guess,’ said Dumbledore. ‘We may never know.’\\n    Professor McGonagall pulled out a lace handkerchief and\\ndabbed at her eyes beneath her spectacles. Dumbledore gave a\\ngreat sniff as he took a golden watch from his pocket and examined\\nit. It was a very odd watch. It had twelve hands but no numbers;\\ninstead, little planets were moving around the edge. It must have\\nmade sense to Dumbledore, though, because he put it back in his\\npocket and said, ‘Hagrid’s late. I suppose it was he who told you\\nI’d be here, by the way?’\\n    ‘Yes,’ said Professor McGonagall. ‘And I don’t suppose you’re\\ngoing to tell me why you’re here, of all places?’\\n    ‘I’ve come to bring Harry to his aunt and uncle. They’re the\\nonly family he has left now.’\\n    ‘You don’t mean – you can’t mean the people who live here?’\\ncried Professor McGonagall, jumping to her feet and pointing at\\nnumber four. ‘Dumbledore – you can’t. I’ve been watching them\\nall day. You couldn’t find two people who are less like us. And\\nthey’ve got this son – I saw him kicking his mother all the way up\\nthe street, screaming for sweets. Harry Potter come and live here!’\\n    ‘It’s the best place for him,’ said Dumbledore firmly. ‘His aunt\\nand uncle will be able to explain everything to him when he’s\\nolder. I’ve written them a letter.’\\n    ‘A letter?’ repeated Professor McGonagall faintly, sitting back\\ndown on the wall. ‘Really, Dumbledore, you think you can explain\\nall this in a letter? These people will never understand him! He’ll\\nbe famous – a legend – I wouldn’t be surprised if today was\\nknown as Harry Potter Day in future – there will be books written\\nabout Harry – every child in our world will know his name!’\\n    ‘Exactly,’ said Dumbledore, looking very seriously over the top\\n\"                          \n[14] \"16                         HARRY POTTER\\n\\nof his half-moon glasses. ‘It would be enough to turn any boy’s\\nhead. Famous before he can walk and talk! Famous for something\\nhe won’t even remember! Can’t you see how much better off he’ll\\nbe, growing up away from all that until he’s ready to take it?’\\n    Professor McGonagall opened her mouth, changed her mind,\\nswallowed and then said, ‘Yes – yes, you’re right, of course. But\\nhow is the boy getting here, Dumbledore?’ She eyed his cloak\\nsuddenly as though she thought he might be hiding Harry\\nunderneath it.\\n    ‘Hagrid’s bringing him.’\\n    ‘You think it – wise – to trust Hagrid with something as impor-\\ntant as this?’\\n    ‘I would trust Hagrid with my life,’ said Dumbledore.\\n    ‘I’m not saying his heart isn’t in the right place,’ said Professor\\nMcGonagall grudgingly, ‘but you can’t pretend he’s not careless.\\nHe does tend to – what was that?’\\n    A low rumbling sound had broken the silence around them. It\\ngrew steadily louder as they looked up and down the street for\\nsome sign of a headlight; it swelled to a roar as they both looked\\nup at the sky – and a huge motorbike fell out of the air and landed\\non the road in front of them.\\n    If the motorbike was huge, it was nothing to the man sitting\\nastride it. He was almost twice as tall as a normal man and at least\\nfive times as wide. He looked simply too big to be allowed, and so\\nwild – long tangles of bushy black hair and beard hid most of his\\nface, he had hands the size of dustbin lids and his feet in their\\nleather boots were like baby dolphins. In his vast, muscular arms\\nhe was holding a bundle of blankets.\\n    ‘Hagrid,’ said Dumbledore, sounding relieved. ‘At last. And\\nwhere did you get that motorbike?’\\n    ‘Borrowed it, Professor Dumbledore, sir,’ said the giant, climbing\\ncarefully off the motorbike as he spoke. ‘Young Sirius Black lent it\\nme. I’ve got him, sir.’\\n    ‘No problems, were there?’\\n    ‘No, sir – house was almost destroyed but I got him out all\\nright before the Muggles started swarmin’ around. He fell asleep\\nas we was flyin’ over Bristol.’\\n    Dumbledore and Professor McGonagall bent forward over the\\nbundle of blankets. Inside, just visible, was a baby boy, fast asleep.\\nUnder a tuft of jet-black hair over his forehead they could see a\\n\"                                                                                                                                                                                                                                           \n[15] \"                        THE BOY WHO LIVED                            17\\n\\ncuriously shaped cut, like a bolt of lightning.\\n    ‘Is that where –?’ whispered Professor McGonagall.\\n    ‘Yes,’ said Dumbledore. ‘He’ll have that scar for ever.’\\n    ‘Couldn’t you do something about it, Dumbledore?’\\n    ‘Even if I could, I wouldn’t. Scars can come in useful. I have\\none myself above my left knee which is a perfect map of the\\nLondon Underground. Well – give him here, Hagrid – we’d better\\nget this over with.’\\n    Dumbledore took Harry in his arms and turned towards the\\nDursleys’ house.\\n    ‘Could I – could I say goodbye to him, sir?’ asked Hagrid.\\n    He bent his great, shaggy head over Harry and gave him what\\nmust have been a very scratchy, whiskery kiss. Then, suddenly,\\nHagrid let out a howl like a wounded dog.\\n    ‘Shhh!’ hissed Professor McGonagall. ‘You’ll wake the Muggles!’\\n    ‘S-s-sorry,’ sobbed Hagrid, taking out a large spotted handker-\\nchief and burying his face in it. ‘But I c-c-can’t stand it – Lily an’\\nJames dead – an’ poor little Harry off ter live with Muggles –’\\n    ‘Yes, yes, it’s all very sad, but get a grip on yourself, Hagrid, or\\nwe’ll be found,’ Professor McGonagall whispered, patting Hagrid\\ngingerly on the arm as Dumbledore stepped over the low garden\\nwall and walked to the front door. He laid Harry gently on the\\ndoorstep, took a letter out of his cloak, tucked it inside Harry’s\\nblankets and then came back to the other two. For a full minute\\nthe three of them stood and looked at the little bundle; Hagrid’s\\nshoulders shook, Professor McGonagall blinked furiously and the\\ntwinkling light that usually shone from Dumbledore’s eyes seemed\\nto have gone out.\\n    ‘Well,’ said Dumbledore finally, ‘that’s that. We’ve no business\\nstaying here. We may as well go and join the celebrations.’\\n    ‘Yeah,’ said Hagrid in a very muffled voice. ‘I’d best get\\nthis bike away. G’night, Professor McGonagall – Professor\\nDumbledore, sir.’\\n    Wiping his streaming eyes on his jacket sleeve, Hagrid swung\\nhimself on to the motorbike and kicked the engine into life; with\\na roar it rose into the air and off into the night.\\n    ‘I shall see you soon, I expect, Professor McGonagall,’ said\\nDumbledore, nodding to her. Professor McGonagall blew her nose\\nin reply.\\n    Dumbledore turned and walked back down the street. On the\\n\"                                                                                                                                                                             \n[16] \"18                       HARRY POTTER\\n\\ncorner he stopped and took out the silver Put-Outer. He clicked it\\nonce and twelve balls of light sped back to their street lamps so\\nthat Privet Drive glowed suddenly orange and he could make out\\na tabby cat slinking around the corner at the other end of the\\nstreet. He could just see the bundle of blankets on the step of\\nnumber four.\\n    ‘Good luck, Harry,’ he murmured. He turned on his heel and\\nwith a swish of his cloak he was gone.\\n    A breeze ruffled the neat hedges of Privet Drive, which lay\\nsilent and tidy under the inky sky, the very last place you would\\nexpect astonishing things to happen. Harry Potter rolled over\\ninside his blankets without waking up. One small hand closed on\\nthe letter beside him and he slept on, not knowing he was special,\\nnot knowing he was famous, not knowing he would be woken in\\na few hours’ time by Mrs Dursley’s scream as she opened the front\\ndoor to put out the milk bottles, nor that he would spend the next\\nfew weeks being prodded and pinched by his cousin Dudley ... He\\ncouldn’t know that at this very moment, people meeting in secret\\nall over the country were holding up their glasses and saying in\\nhushed voices: ‘To Harry Potter – the boy who lived!’\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n\nlength(test)\n\n[1] 5\n\nlibrary(purrr)\nlapply(test, length)\n\n[[1]]\n[1] 16\n\n[[2]]\n[1] 9\n\n[[3]]\n[1] 11\n\n[[4]]\n[1] 10\n\n[[5]]\n[1] 18\n\nmap(test, length)\n\n[[1]]\n[1] 16\n\n[[2]]\n[1] 9\n\n[[3]]\n[1] 11\n\n[[4]]\n[1] 10\n\n[[5]]\n[1] 18\n\n#shows the pages of each file\n\nlibrary(tm)\n\nLoading required package: NLP\n\n\n\nAttaching package: 'NLP'\n\n\nThe following objects are masked from 'package:quanteda':\n\n    meta, meta&lt;-\n\n\n\nAttaching package: 'tm'\n\n\nThe following object is masked from 'package:quanteda':\n\n    stopwords\n\ntest.corpus &lt;- Corpus(URISource(file_list),\n                      readerControl = list(reader = readPDF))\n\ntest.corpus # tm corpus\n\n&lt;&lt;VCorpus&gt;&gt;\nMetadata:  corpus specific: 0, document level (indexed): 0\nContent:  documents: 5\n\ntest.corpus.quanteda &lt;- corpus(test.corpus)\nsummary(test.corpus.quanteda)\n\nCorpus consisting of 5 documents, showing 5 documents:\n\n  Text Types Tokens Sentences       author       datetimestamp\n text1  1403   5865       361 J.K. Rowling 2023-09-13 14:04:05\n text2  1083   4132       239 J.K. Rowling 2023-09-13 14:05:02\n text3  1237   4649       299 J.K. Rowling 2023-09-13 14:06:18\n text4  1207   4809       326 J.K. Rowling 2023-09-13 14:06:43\n text5  1867   8405       565 J.K. Rowling 2023-09-13 14:07:14\n                                                                    description\n Reference Quality Electronic Book Version of the British Bloomsbury Hard Cover\n Reference Quality Electronic Book Version of the British Bloomsbury Hard Cover\n Reference Quality Electronic Book Version of the British Bloomsbury Hard Cover\n Reference Quality Electronic Book Version of the British Bloomsbury Hard Cover\n Reference Quality Electronic Book Version of the British Bloomsbury Hard Cover\n                                  heading           id language\n Harry Potter and the Philosopher's Stone chapter1.pdf       en\n Harry Potter and the Philosopher's Stone chapter2.pdf       en\n Harry Potter and the Philosopher's Stone chapter3.pdf       en\n Harry Potter and the Philosopher's Stone chapter4.pdf       en\n Harry Potter and the Philosopher's Stone chapter5.pdf       en\n                   origin\n PScript5.dll Version 5.2\n PScript5.dll Version 5.2\n PScript5.dll Version 5.2\n PScript5.dll Version 5.2\n PScript5.dll Version 5.2"
  },
  {
    "objectID": "Tutorial2.html#reading-all-files-in-a-directory",
    "href": "Tutorial2.html#reading-all-files-in-a-directory",
    "title": "Tutorial 2 Text as Data",
    "section": "Reading all files in a directory",
    "text": "Reading all files in a directory\nUnfortunately, the harrypotter package, which contains the book contents of all seven Harry Potter books, we used in previous years no longer works now (It seems R packages can disappear faster than a disarming spell!). So before we move on to text analysis, let’s read in our data first. Please visit my Github HarryPotter repository. Download all the .rda files, and save them under the same folder.\nMake sure to replace the folder path with the correct path on your own device.\n\n# Define the folder containing the .rda files. \nfolder &lt;- \"/Users/mpang/Dropbox/Teaching Resources/DACSS_TAD/HarryPotter\"\n\n# Get the list of all .rda files in the folder\nrda_files &lt;- list.files(folder, pattern = \"\\\\.rda$\", full.names = TRUE)\n\n# Load all .rda files into the environment\nlapply(rda_files, load, .GlobalEnv)\n\n[[1]]\n[1] \"chamber_of_secrets\"\n\n[[2]]\n[1] \"deathly_hallows\"\n\n[[3]]\n[1] \"goblet_of_fire\"\n\n[[4]]\n[1] \"half_blood_prince\"\n\n[[5]]\n[1] \"order_of_the_phoenix\"\n\n[[6]]\n[1] \"philosophers_stone\"\n\n[[7]]\n[1] \"prisoner_of_azkaban\""
  },
  {
    "objectID": "Tutorial3.html",
    "href": "Tutorial3.html",
    "title": "Tutorial 3 Web Scraping in R",
    "section": "",
    "text": "This tutorial walks you through the process of using the package rvest to scrape websites using R. rvest (sounds like “harvest”, get it?) is the workhorse package for text scraping in R and contains all of the functionality for basic scraping applications. We’ll start by loading the package and tidyverse more generally so that we can use many of the tidyverse functions later on.\n\n#install.packages(\"rvest\")\nlibrary(rvest)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()         masks stats::filter()\n✖ readr::guess_encoding() masks rvest::guess_encoding()\n✖ dplyr::lag()            masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nNicely Formatted Data\nLet’s start with a simple example; we’ll use the work from the Stanford Data Challenge Lab prepared by Sara Altman and Bill Behrman. Specifically, let’s say you are interested in the study of famines and come across this website. Wonderful! But how can we get this into a format for analysis? Time to waste a lot of hours on manually entering the data or maybe we have some spare funds to hire an undergrad…\nFortunately, that’s not true. With rvest this is arbitrarily easy. First, we can tell R the webpage we are looking at.\n\n# identify the url\nurl &lt;- \"https://ourworldindata.org/famines#the-our-world-in-data-dataset-of-famines\"\n\nOf course, we can’t just read that webpage in because webpages are filled with all variety of — from our perspective — junk.\n\n# what happens when we just read that in; it's a full webpage\nread_html(url)\n\n{html_document}\n&lt;html class=\"js-disabled\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\n&lt;header class=\"site-header\"&gt;&lt;div class=\"site-navigation-root\"&gt;&lt;di ...\n\n\nYeah, that’s not helpful. Instead, we need to identify just the table. This is the hardest part, but it’s not really too difficult. If you are familiar with HTML / CSS, then you might be able to just inspect the HTML to identify the correct selector. If you are in a Chrome browser, for instance, you can right click then select “Inspect”.\nIf you are less familiar, you can use SelectorGadget, a Chrome add-on that lets you — for whatever webpage you are visiting — “select” the portion of a page you are interested in scraping. SelectorGadget will then highlight whatever the Selector (in the bar at the bottom) would return. Once you’ve identified the Selector that works for the element of interest to you, you can add that to our code and you are off and running.\nNote that, especially if you aren’t familiar, this is going to take some time to get used to and require some patience and practice. Take a moment now to install the SelectorGadget Chrome extension, then play around with SelectorGadget on the website until you can correctly identify “table”, the CSS selector that will let us pull the table.\n\n# use SelectorGadget to find the info\ncss_selector &lt;- \"table\"\n\nOnce we have the correct selector, we still need to (1) read the full html, (2) select out the portion that we are interested in, and (3) start formatting. That third step is really particularly easy when what we are pulling is already in tabular format. So, for example, here’s how we could read our table of data on famines into R.\n\n# pull the table\nurl %&gt;%\n  read_html() %&gt;%\n  html_node(css = css_selector) %&gt;%\n  html_table()\n\n# A tibble: 77 × 6\n   Year      Country           Excess Mortality midpoin…¹ Excess Mortality low…²\n   &lt;chr&gt;     &lt;chr&gt;             &lt;chr&gt;                      &lt;chr&gt;                 \n 1 1846–52   Ireland           1,000,000                  1,000,000             \n 2 1860-1    India             2,000,000                  2,000,000             \n 3 1863-67   Cape Verde        30,000                     30,000                \n 4 1866-7    India             961,043                    961,043               \n 5 1868      Finland           100,000                    100,000               \n 6 1868-70   India             1,500,000                  1,500,000             \n 7 1870–1871 Persia (now Iran) 1,000,000                  500,000               \n 8 1876–79   Brazil            750,000                    500,000               \n 9 1876–79   India             7,176,346                  6,135,000             \n10 1877–79   China             11,000,000                 9,000,000             \n# ℹ 67 more rows\n# ℹ abbreviated names: ¹​`Excess Mortality midpoint`, ²​`Excess Mortality lower`\n# ℹ 2 more variables: `Excess Mortality upper` &lt;chr&gt;, Source &lt;chr&gt;\n\n\nAnd if we wanted to save that, we’d just assign the prior chunk of code to an list item object. Then, we’d be ready for analysis.\n\n\nLess Nicely Formatted Data\nOf course, more likely than not we will be encountering data that aren’t so nice and tidy. This is particularly true for web scraping when the item of interest is text data, which rarely appear in tables like the one above.\nLet’s try out an example. We’ll look at yelp reviews for everyone’s favorite brutalist building: Hotel UMass.\n\n# start by defining the url we want\nurl &lt;- \"https://www.yelp.com/biz/hotel-umass-amherst?osq=hotel+umass\"\n\n# now define the field we want\ncss_selector &lt;- \"li.y-css-1jp2syp\"\n\nreviews &lt;- url %&gt;%\n  read_html() %&gt;%\n  html_nodes(css = css_selector) %&gt;%\n  html_text()\n\nreviews\n\n [1] \"Tara H.Newport, RI7096Nov 25, 2018I booked this place after I decided we should stay overnight after Thanksgiving dinner with family in the area.  I am UMass alumni and was feeling nostalgic so when I started looking for rooms and came across the hotel (which I had forgotten was even there) I got excited about the prospect of being on the campus again.The good:  it really is a decent hotel.  Nice and modern and updated and comfortable.  They have water bottles in the room that can be used to get spring water downstairs for the duration of your stay.  I thought that this was a very nice touch since when I need a drink at night I usually end up using a plastic cup to get water out of the tap.  They also have a limited continental breakfast that does the job for some.  Would have been fine for me but left something to be desired for my kids.  The gentleman checking us in at the front desk was friendly and helpful.  We were only staying for one night during Thanksgiving so the campus was pretty much deserted but I would definitely come back and stay for longer.  The location in the center of campus in the student center with access to tons of food options and amenities is awesome.  Perfect spot from which to access and explore the entire campus.The not so good:  Hardly anything.  You only get one parking space in the garage.  Not usually an issue but because my mother was staying with us she had to pay the $20 parking fee.  Not a huge deal but I like open access to parking.  The bathroom was so teeny you can barely even turn around in there!  It does have a nice large stand-up shower which again was fine for me but my 8-year-old son lamented that he couldn't take a bath which he really wanted to do.   I would definitely recommend staying here if your main focus is seeing the campus.  You cannot beat the location and it's a pretty decent place.  Don't let the fact that it's a college hotel scare you!Helpful 1Helpful 2Thanks 0Thanks 1Love this 1Love this 2Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                              \n [2] \"Joe M.Dartmouth, MA0245Sep 18, 20211 photoWhat a disaster!! What kind of hotel has only one clerk on during a rush period? After a long day and Football game we went to check in an there was a line of at least 20 people waiting to check in. Ya think they might actually have some staff on to handle the anticipated crowd trying to check inA long line of people being forced to wait for over an hour to simply check in!Helpful 1Helpful 2Thanks 0Thanks 1Love this 1Love this 2Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n [3] \"Greg H.Wethersfield, CT04Jul 10, 2022They claim to have no rooms available during the summer but I have a hard time believing that.Helpful 1Helpful 2Thanks 0Thanks 1Love this 0Love this 1Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n [4] \"Mike D.Elite 24Ridgefield, CT34248363May 9, 2019The great: location, location, location,they've got it and you want it. So they make you pay for it. Also great covered garage parking, restaurants, pub, and nice managers reception at night and all food is great.The good, decent room, comfortable beds, in room keurig The bad, noisy, especially if near elevator, small bathroom. Pillows aren't very good, we bring ours.not very many outlets and no usb chargingThe ugly, no mini fridge for your stuff like a white wine, weird.  No safe. 3 night minimum at peak times like graduation so $900 for a stay 3x300=900SummaryWe keep coming back, so we think it's worth itHelpful 1Helpful 2Thanks 0Thanks 1Love this 0Love this 1Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n [5] \"Kim S.Coos, NH617Sep 18, 2021We book our stay over a month in advance. We were traveling to see our son and take in a show. Check in time was 3:00pm. We arrived at 3:15. There were several people in the lobby who all looked unhappy. The was a sign on the front desk that said due to staffing shortages the desk was closed and would open at 6. We left to take our son to dinner and a show. We call all the numbers for the hotel to no avail. We then called Umass police. They said many people had called. They did not know why it was not ope either. Now we are driving to Framingham to the closest hotel with availability. Do not stay there. Ruined a nice weekend.Helpful 3Helpful 4Thanks 0Thanks 1Love this 1Love this 2Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n [6] \"Howie B.Elite 24Needham, MA45135568Jun 23, 2016It was wonderful to be back at my Alma Mater! Stayed two nights in connection with my son's New Student Orientation. It was a terrific stay and it is wonderfully convenient to be situated right on campus. Having attended in the '80s, the hotel always felt aesthetically cold - both inside and out (lots of poured concrete will do that. Housed in the university's Campus Center, one of the New Student Program tour guides referred to the hotel's exterior as looking like a \\\"waffle iron.\\\" I had never heard that before, yet it is indeed true.) However, as new buildings continue to rise up around it, the Campus Center building seems to have become more charming over the years.Inside the hotel they have professionalized their operation and \\\"warmed-up\\\" the public spaces, redecorated their rooms and offer amenities like a substantial continental breakfast and free wifi (there was no wifi back in the '80s :-) There are amazing food options in the new Blue Wall on the Campus Center concourse level. Previously an aged bar on campus, this has been totally redesigned as a state-of-the-art \\\"food court\\\" with eye-popping selections (a bakery, gelateria, Mexican and Asian stations, sushi, deli, pizza, salad bar, and much more (UMASS's food service is among the most-awarded for colleges and universities in the US!)We stayed in a \\\"pond view\\\" room which offered a stunning (at least it was to me) view of the center of campus. The room was very clean, updated and well-appointed. The bed was among the most comfortable I've ever slept in. The bathroom, which was on the small side making it difficult to close the door once inside, was updated and was serviceable. One small gripe is that they don't provide bar soap in the shower, instead providing a tiny tube of body wash. I actually ran into the Hotel Manager, Rachelle Allen, during my stay (she could not have been more personable and lovely) and noted this to her. She indicated that this is done in an effort to economize and avoid waste. However, she said she would be happy to send up bar soap if requested.This was a wonderful and very pleasant stay and it was great to be back on campus. I look forward to many more stays over the next four years!Helpful 2Helpful 3Thanks 0Thanks 1Love this 0Love this 1Oh no 0Oh no 1\"\n [7] \"Kat Y.Elite 24Newport, RI37206277Mar 9, 2019Clean, easy, convenient. Bring your own conditioner, they do not produce any. This was a major problem for my textured hair. This is the best place to say if you have business at the Flagship UMass campus.Helpful 3Helpful 4Thanks 0Thanks 1Love this 1Love this 2Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n [8] \"Chris K.Elite 24Bloomfield Township, MI118851149Feb 17, 2019I admit I wasn't sure what to expect on arrival, viewing the exterior - which I later learned was a brutalist architectural style. Staff were very friendly on check-in and quickly got me settled in my quiet room with good views of the surrounding campus. The two-queen room was spacious and very clean. My only cons would be the wireless internet (good speed, reliable) seemed to kick you off a few times per day and force you to re-enter the password each time, and it was very quiet - nice to have at least a fan on the AC unit that can create a little white noise for sleeping at night. Breakfast buffet in the morning, cookies on the check-in counter, lots of nice touches. Was overall very happy here, and would happily stay again!Helpful 1Helpful 2Thanks 0Thanks 1Love this 0Love this 1Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n [9] \"Roy A.Elite 24Alexandria, VA3574525829Apr 3, 2018The five stars is based on basic services and the desire to be located in the center of UMass Amherst while doing a tour. Being on campus is the bonus, the room was small but very clean, room service was prompt and no issues. The price is very reasonable and I highly recommend the hotel. It has no high end feature so if your looking for luxuries you may need to look elsewhere. Unlimited access to parking in the connected garage and multiple dining options also a plus.Helpful 1Helpful 2Thanks 0Thanks 1Love this 1Love this 2Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n[10] \"Kevin J.Philadelphia, PA195642126Mar 24, 2011I much prefer staying here than the newer-built hotels up in the town of Amherst.  The central campus location of UMass Hotel Campus Center is not only the most convenient, it's actually picturesque. They offer a view of the little pond and the trees in late autumn are particularly breathtaking - the entrance plaza stairs could even be called artistic.  UMass offers a reduced rate specific for students and their guests. They offer the most convenient location for walking to campus events at the Mullins Center and other facilities. They offer indoor swimming pool access. Would stay here again in a heartbeat.  Although simple accommodations, it still makes for a very comfortable setting.  Qualifies for my Best of the A10 list... no other hotel in Amherst or Springfield comes close to providing the same convenience or at-home feeling.Helpful 2Helpful 3Thanks 0Thanks 1Love this 0Love this 1Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n[11] \"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n\n\nThere are 10 reviews on each page, but we find an extra empty review [11]. So we want to filter out empty reviews.\n\nreviews &lt;- reviews[reviews != \"\"]\n\nreviews\n\n [1] \"Tara H.Newport, RI7096Nov 25, 2018I booked this place after I decided we should stay overnight after Thanksgiving dinner with family in the area.  I am UMass alumni and was feeling nostalgic so when I started looking for rooms and came across the hotel (which I had forgotten was even there) I got excited about the prospect of being on the campus again.The good:  it really is a decent hotel.  Nice and modern and updated and comfortable.  They have water bottles in the room that can be used to get spring water downstairs for the duration of your stay.  I thought that this was a very nice touch since when I need a drink at night I usually end up using a plastic cup to get water out of the tap.  They also have a limited continental breakfast that does the job for some.  Would have been fine for me but left something to be desired for my kids.  The gentleman checking us in at the front desk was friendly and helpful.  We were only staying for one night during Thanksgiving so the campus was pretty much deserted but I would definitely come back and stay for longer.  The location in the center of campus in the student center with access to tons of food options and amenities is awesome.  Perfect spot from which to access and explore the entire campus.The not so good:  Hardly anything.  You only get one parking space in the garage.  Not usually an issue but because my mother was staying with us she had to pay the $20 parking fee.  Not a huge deal but I like open access to parking.  The bathroom was so teeny you can barely even turn around in there!  It does have a nice large stand-up shower which again was fine for me but my 8-year-old son lamented that he couldn't take a bath which he really wanted to do.   I would definitely recommend staying here if your main focus is seeing the campus.  You cannot beat the location and it's a pretty decent place.  Don't let the fact that it's a college hotel scare you!Helpful 1Helpful 2Thanks 0Thanks 1Love this 1Love this 2Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                              \n [2] \"Joe M.Dartmouth, MA0245Sep 18, 20211 photoWhat a disaster!! What kind of hotel has only one clerk on during a rush period? After a long day and Football game we went to check in an there was a line of at least 20 people waiting to check in. Ya think they might actually have some staff on to handle the anticipated crowd trying to check inA long line of people being forced to wait for over an hour to simply check in!Helpful 1Helpful 2Thanks 0Thanks 1Love this 1Love this 2Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n [3] \"Greg H.Wethersfield, CT04Jul 10, 2022They claim to have no rooms available during the summer but I have a hard time believing that.Helpful 1Helpful 2Thanks 0Thanks 1Love this 0Love this 1Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n [4] \"Mike D.Elite 24Ridgefield, CT34248363May 9, 2019The great: location, location, location,they've got it and you want it. So they make you pay for it. Also great covered garage parking, restaurants, pub, and nice managers reception at night and all food is great.The good, decent room, comfortable beds, in room keurig The bad, noisy, especially if near elevator, small bathroom. Pillows aren't very good, we bring ours.not very many outlets and no usb chargingThe ugly, no mini fridge for your stuff like a white wine, weird.  No safe. 3 night minimum at peak times like graduation so $900 for a stay 3x300=900SummaryWe keep coming back, so we think it's worth itHelpful 1Helpful 2Thanks 0Thanks 1Love this 0Love this 1Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n [5] \"Kim S.Coos, NH617Sep 18, 2021We book our stay over a month in advance. We were traveling to see our son and take in a show. Check in time was 3:00pm. We arrived at 3:15. There were several people in the lobby who all looked unhappy. The was a sign on the front desk that said due to staffing shortages the desk was closed and would open at 6. We left to take our son to dinner and a show. We call all the numbers for the hotel to no avail. We then called Umass police. They said many people had called. They did not know why it was not ope either. Now we are driving to Framingham to the closest hotel with availability. Do not stay there. Ruined a nice weekend.Helpful 3Helpful 4Thanks 0Thanks 1Love this 1Love this 2Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n [6] \"Howie B.Elite 24Needham, MA45135568Jun 23, 2016It was wonderful to be back at my Alma Mater! Stayed two nights in connection with my son's New Student Orientation. It was a terrific stay and it is wonderfully convenient to be situated right on campus. Having attended in the '80s, the hotel always felt aesthetically cold - both inside and out (lots of poured concrete will do that. Housed in the university's Campus Center, one of the New Student Program tour guides referred to the hotel's exterior as looking like a \\\"waffle iron.\\\" I had never heard that before, yet it is indeed true.) However, as new buildings continue to rise up around it, the Campus Center building seems to have become more charming over the years.Inside the hotel they have professionalized their operation and \\\"warmed-up\\\" the public spaces, redecorated their rooms and offer amenities like a substantial continental breakfast and free wifi (there was no wifi back in the '80s :-) There are amazing food options in the new Blue Wall on the Campus Center concourse level. Previously an aged bar on campus, this has been totally redesigned as a state-of-the-art \\\"food court\\\" with eye-popping selections (a bakery, gelateria, Mexican and Asian stations, sushi, deli, pizza, salad bar, and much more (UMASS's food service is among the most-awarded for colleges and universities in the US!)We stayed in a \\\"pond view\\\" room which offered a stunning (at least it was to me) view of the center of campus. The room was very clean, updated and well-appointed. The bed was among the most comfortable I've ever slept in. The bathroom, which was on the small side making it difficult to close the door once inside, was updated and was serviceable. One small gripe is that they don't provide bar soap in the shower, instead providing a tiny tube of body wash. I actually ran into the Hotel Manager, Rachelle Allen, during my stay (she could not have been more personable and lovely) and noted this to her. She indicated that this is done in an effort to economize and avoid waste. However, she said she would be happy to send up bar soap if requested.This was a wonderful and very pleasant stay and it was great to be back on campus. I look forward to many more stays over the next four years!Helpful 2Helpful 3Thanks 0Thanks 1Love this 0Love this 1Oh no 0Oh no 1\"\n [7] \"Kat Y.Elite 24Newport, RI37206277Mar 9, 2019Clean, easy, convenient. Bring your own conditioner, they do not produce any. This was a major problem for my textured hair. This is the best place to say if you have business at the Flagship UMass campus.Helpful 3Helpful 4Thanks 0Thanks 1Love this 1Love this 2Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n [8] \"Chris K.Elite 24Bloomfield Township, MI118851149Feb 17, 2019I admit I wasn't sure what to expect on arrival, viewing the exterior - which I later learned was a brutalist architectural style. Staff were very friendly on check-in and quickly got me settled in my quiet room with good views of the surrounding campus. The two-queen room was spacious and very clean. My only cons would be the wireless internet (good speed, reliable) seemed to kick you off a few times per day and force you to re-enter the password each time, and it was very quiet - nice to have at least a fan on the AC unit that can create a little white noise for sleeping at night. Breakfast buffet in the morning, cookies on the check-in counter, lots of nice touches. Was overall very happy here, and would happily stay again!Helpful 1Helpful 2Thanks 0Thanks 1Love this 0Love this 1Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n [9] \"Roy A.Elite 24Alexandria, VA3574525829Apr 3, 2018The five stars is based on basic services and the desire to be located in the center of UMass Amherst while doing a tour. Being on campus is the bonus, the room was small but very clean, room service was prompt and no issues. The price is very reasonable and I highly recommend the hotel. It has no high end feature so if your looking for luxuries you may need to look elsewhere. Unlimited access to parking in the connected garage and multiple dining options also a plus.Helpful 1Helpful 2Thanks 0Thanks 1Love this 1Love this 2Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n[10] \"Kevin J.Philadelphia, PA195642126Mar 24, 2011I much prefer staying here than the newer-built hotels up in the town of Amherst.  The central campus location of UMass Hotel Campus Center is not only the most convenient, it's actually picturesque. They offer a view of the little pond and the trees in late autumn are particularly breathtaking - the entrance plaza stairs could even be called artistic.  UMass offers a reduced rate specific for students and their guests. They offer the most convenient location for walking to campus events at the Mullins Center and other facilities. They offer indoor swimming pool access. Would stay here again in a heartbeat.  Although simple accommodations, it still makes for a very comfortable setting.  Qualifies for my Best of the A10 list... no other hotel in Amherst or Springfield comes close to providing the same convenience or at-home feeling.Helpful 2Helpful 3Thanks 0Thanks 1Love this 0Love this 1Oh no 0Oh no 1\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n\n\nThat’s great, but as you can see it only pulls the first 10 reviews. We’d ideally like to have them all. To do so, we need to iterate through the pages. This takes some understanding of how Yelp sets up their website. Here’s the URL we were using\n\nurl\n\n[1] \"https://www.yelp.com/biz/hotel-umass-amherst?osq=hotel+umass\"\n\n\nIf you head to that webpage, you’ll notice that only the first set of reviews is actually present. How might we get the second set? The easiest way would be if we could identify some standard language that Yelp is using that we can then leverage to “loop” through each page (that is, run the same operation as above for each page). To see how Yelp lays everything out, click through to the second page of reviews. Here’s the URL for that page, and a comparison of the two URLs.\n\nnew_url &lt;- \"https://www.yelp.com/biz/hotel-umass-amherst?osq=hotel+umass&start=10\"\n\n# print these next to each other\nurl\n\n[1] \"https://www.yelp.com/biz/hotel-umass-amherst?osq=hotel+umass\"\n\nnew_url\n\n[1] \"https://www.yelp.com/biz/hotel-umass-amherst?osq=hotel+umass&start=10\"\n\n\nAha! That shows us how they are indexing the pages, with a little statement :“&start=10”. The indexing here is kind of weird but if you click on the next set, you’ll see that 10 jumps up to 20. So they are counting by 10 and creating a new page each time. We can use that to set up a loop. Ostensibly, you could identify the total number of pages is 4 pages. The first page is — as we just saw — unnumbered, and then each thereafter is indexed by 10. Because the first is unnumbered, we’ll subtract one from our number of pages to loop through.\n\n# create the indices\npageNumber &lt;- 10 * c(1:3)\n# get an idea of what we just created\nhead(pageNumber)\n\n[1] 10 20 30\n\nlength(pageNumber)\n\n[1] 3\n\n# set up a new vector to store the urls\nurls &lt;- url\n\n# loop through the page numbers and create the new urls\nfor (i in 1:length(pageNumber)){\n  urls &lt;- c(urls, paste(\"https://www.yelp.com/biz/hotel-umass-amherst?osq=hotel+umass&start=\",pageNumber[i], sep = \"\"))\n}\n\n# look at the first few\nhead(urls)\n\n[1] \"https://www.yelp.com/biz/hotel-umass-amherst?osq=hotel+umass\"         \n[2] \"https://www.yelp.com/biz/hotel-umass-amherst?osq=hotel+umass&start=10\"\n[3] \"https://www.yelp.com/biz/hotel-umass-amherst?osq=hotel+umass&start=20\"\n[4] \"https://www.yelp.com/biz/hotel-umass-amherst?osq=hotel+umass&start=30\"\n\n\nNow that we have all of the urls, we can loop through each pulling the reviews from each page. To do so, we need to create a new vector that will store all of the reviews; otherwise, we’ll just be overwriting our review object in each loop.\nNote that this will take a couple of minutes; we’re iterating through 4 pages after all.\n\n# set up an empty vector to store reviews\nreviews &lt;- c()\n\n# loop through urls\nfor (i in 1:length(urls)){\n  # extract reviews for this url\n  tmpReviews &lt;- urls[i] %&gt;%\n  read_html() %&gt;%\n  html_nodes(css = css_selector) %&gt;%\n  html_text()\n\n  # add them to the set of reviews\n  reviews &lt;- c(reviews,tmpReviews)\n}\n\n#filter out empty reviews\nreviews &lt;- reviews[reviews != \"\"]\n\n# look at the 12th reviews\nreviews[12]\n\n[1] \"Song R.Chevy Chase Village, MD0326Feb 1, 2019What happened? Used to be a great hotel. Now it feels old and in serious need of a renovation. I'm sitting in my room right now, smelling some kind of old cooking oil smell that also seems so be permeating throughout the campus center. Yuck. Everything looks and feels rundown.Helpful 0Helpful 1Thanks 0Thanks 1Love this 0Love this 1Oh no 0Oh no 1\"\n\n\nThere’s a few things we’d want to do from here. We could expand the extraction to identify all sorts of other aspects of each review — the name, location, and prior reviews by a reviewer; the overall rating as well as the rating across different categories; and so on — and add those extractions to each stage of our loop. We’ll leave those steps to future analyses.\n\n\nA Quick Analysis\nWith the text scraped from the site, we can use a bit of what we’ve done in past tutorials (and will continue to do going forward) to take a look at what folks are saying about Hotel UMass.\n\nlibrary(quanteda)\n\nPackage version: 4.0.2\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textplots)\n\n# convert to corpus\nhotel_corpus &lt;- corpus(reviews)\n\n# create a word cloud\nhotel_dfm &lt;- tokens(hotel_corpus, remove_punct=TRUE) %&gt;%\n          tokens_select(pattern=stopwords(\"en\"),\n          selection=\"remove\") %&gt;%\n          dfm()\n\ntextplot_wordcloud(hotel_dfm)\n\n\n\n\n\n\n\n\nIt’s hard to find a pattern. Since we included the buttons “helpful”, “thanks”, etc. under the reviews, the most common words are “1love”, “0oh”. Also, since we are interested in hotel reviews, we also see common words such as “hotel”, “rooms”, “campus”, etc. Let’s see if we can remove these words.\n\nhotel_dfm2 &lt;- tokens(hotel_corpus,\n                     remove_punct=TRUE,\n                     remove_numbers = TRUE) %&gt;%\n  tokens_select(pattern=c(stopwords(\"en\"),\n          \"hotel\",\"rooms\",\"room\",\"umass\",\"campus\"),selection=\"remove\") %&gt;%\n  ## Regular expression that matches words that start with numbers followed by letters\n    tokens_select(pattern = \"^\\\\d+\\\\w+$\", selection = \"remove\", valuetype = \"regex\") %&gt;%\n             dfm()\n\ntextplot_wordcloud(hotel_dfm2)\n\n\n\n\n\n\n\n\nNow the reviews are pretty obvious! Good job Hotel UMass!\n\n\nConclusion\nEverything here worked well. But what if we have a website that doesn’t follow the same nice URL nomenclature / pattern that we found above? In that case, we’ll need to resort to RSelenium. We’ll leave that for another tutorial."
  },
  {
    "objectID": "Tutorial4.html",
    "href": "Tutorial4.html",
    "title": "Tutorial 4 Natural Language Processing",
    "section": "",
    "text": "In this tutorial, we’ll learn about doing standard natural language processing (NLP) tasks in R, and will be introduced to regular expressions. After completing this notebook, you should be familar with:\n# load libraries\nlibrary(cleanNLP)\nlibrary(tidytext)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(quanteda)\n\nPackage version: 4.0.2\nUnicode version: 14.0\nICU version: 71.1\nParallel computing: disabled\nSee https://quanteda.io for tutorials and examples."
  },
  {
    "objectID": "Tutorial4.html#whats-in-a-string",
    "href": "Tutorial4.html#whats-in-a-string",
    "title": "Tutorial 4 Natural Language Processing",
    "section": "What’s in a string?",
    "text": "What’s in a string?\nLet’s look at some simpler data first.\n\nlength(sentences)\n\n[1] 720\n\nhead(sentences)\n\n[1] \"The birch canoe slid on the smooth planks.\" \n[2] \"Glue the sheet to the dark blue background.\"\n[3] \"It's easy to tell the depth of a well.\"     \n[4] \"These days a chicken leg is a rare dish.\"   \n[5] \"Rice is often served in round bowls.\"       \n[6] \"The juice of lemons makes fine punch.\"      \n\nstring &lt;- 'It\\'s easy to tell the depth of a well.'\n\nAs you can see, sentences contains 720 short and simple sentences. We’ll use these to illustrate regular expressions. A first thing to note is the string. See that \\? That’s an escape, and tells R to ignore the single quote. Why is this important? Well, notice what demarcates each sentence that’s being printed. That’s right, single quotes! So the \\ let’s R know that the element is not yet complete. That doesn’t mean the \\ is always there though. If you want to see the “printed” version of the sentence, you can use writeLines()\n\nwriteLines(head(sentences))\n\nThe birch canoe slid on the smooth planks.\nGlue the sheet to the dark blue background.\nIt's easy to tell the depth of a well.\nThese days a chicken leg is a rare dish.\nRice is often served in round bowls.\nThe juice of lemons makes fine punch.\n\n\nThere are lots of other special characters that may require escapes in R if you are doing regular expression matching. That can be particularly challenging because of the special meanings of those special characters — like the single quote — leads to particular operations. As an example, the single period . in a regular expression is a wild card for character matching, and will match any character but a newline. Therefore, if you include the . in a regular expression without escaping it, you’ll end up matching just about everything. The cheat sheet posted to the course website gives more details on these special characters."
  },
  {
    "objectID": "Tutorial4.html#combining-strings",
    "href": "Tutorial4.html#combining-strings",
    "title": "Tutorial 4 Natural Language Processing",
    "section": "Combining strings",
    "text": "Combining strings\nNow that we have our strings, let’s do some basic operations. We can combine two strings using the str_c command in stringr. For instance, if we wanted to combine the first two sentences from sentences, we could.\n\nstr_c(sentences[1], sentences[2], sep = \" \")\n\n[1] \"The birch canoe slid on the smooth planks. Glue the sheet to the dark blue background.\"\n\n\nThis also works if we have two separate string vectors that we’d like to combine. Imagine if we split sentences in half; we could combine the two halves! This is often really helpful if you have a couple of character / string variables in your dataset / metadata that you’d like to combine into a single indicator.\n\nsentencesA &lt;- sentences[1:360]\nsentencesB &lt;- sentences[361:720]\n\nhead(str_c(sentencesA, sentencesB, sep = \" \"))\n\n[1] \"The birch canoe slid on the smooth planks. Feed the white mouse some flower seeds.\"     \n[2] \"Glue the sheet to the dark blue background. The thaw came early and freed the stream.\"  \n[3] \"It's easy to tell the depth of a well. He took the lead and kept it the whole distance.\"\n[4] \"These days a chicken leg is a rare dish. The key you designed will fit the lock.\"       \n[5] \"Rice is often served in round bowls. Plead to the council to free the poor thief.\"      \n[6] \"The juice of lemons makes fine punch. Better hash is made of rare beef.\"                \n\n\nYou can also combine all of the strings in one vector into a single observation using the collapse option.\n\nlength(str_c(head(sentences), collapse = \" \"))\n\n[1] 1\n\n# Note that the string in collapse is up to you but is what will be pasted \n# between the elements in the new string. So here's a version with a new line\n# which gets specified by \\n\nstr_c(head(sentences), collapse = \"\\n\")\n\n[1] \"The birch canoe slid on the smooth planks.\\nGlue the sheet to the dark blue background.\\nIt's easy to tell the depth of a well.\\nThese days a chicken leg is a rare dish.\\nRice is often served in round bowls.\\nThe juice of lemons makes fine punch.\"\n\n# and here's what that looks like with writeLines() then\nwriteLines(str_c(head(sentences), collapse = \"\\n\"))\n\nThe birch canoe slid on the smooth planks.\nGlue the sheet to the dark blue background.\nIt's easy to tell the depth of a well.\nThese days a chicken leg is a rare dish.\nRice is often served in round bowls.\nThe juice of lemons makes fine punch.\n\n\nYou can also do the opposite, splitting a string into two by using str_split() and identifying a common splitting indicator.\n\n# create combined string\ncombined_string &lt;- str_c(head(sentences), collapse = \"\\n\")\ncombined_string\n\n[1] \"The birch canoe slid on the smooth planks.\\nGlue the sheet to the dark blue background.\\nIt's easy to tell the depth of a well.\\nThese days a chicken leg is a rare dish.\\nRice is often served in round bowls.\\nThe juice of lemons makes fine punch.\"\n\n# create split string; simplify = TRUE returns a matrix (rather than a list)\nsplit_string &lt;- str_split(combined_string, \"\\n\", simplify = TRUE)\nsplit_string\n\n     [,1]                                        \n[1,] \"The birch canoe slid on the smooth planks.\"\n     [,2]                                         \n[1,] \"Glue the sheet to the dark blue background.\"\n     [,3]                                    \n[1,] \"It's easy to tell the depth of a well.\"\n     [,4]                                      \n[1,] \"These days a chicken leg is a rare dish.\"\n     [,5]                                  \n[1,] \"Rice is often served in round bowls.\"\n     [,6]                                   \n[1,] \"The juice of lemons makes fine punch.\""
  },
  {
    "objectID": "Tutorial4.html#substrings",
    "href": "Tutorial4.html#substrings",
    "title": "Tutorial 4 Natural Language Processing",
    "section": "Substrings",
    "text": "Substrings\nOccassionally, we need to pull out parts of strings. For instance, maybe we just want the first few letters of each string. In those instances, we can use str_sub():\n\n# example string this actually makes some sense for\nmonth.name\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"     \n [7] \"July\"      \"August\"    \"September\" \"October\"   \"November\"  \"December\" \n\n# substring the first three letters\nshort_months &lt;- str_sub(month.name, 1, 3)\nshort_months\n\n [1] \"Jan\" \"Feb\" \"Mar\" \"Apr\" \"May\" \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\"\n\n\nYou can also use str_sub() to change a string through replacement of specific characters. Here we’ll replace the first few letters of every month with “Cat”.\n\nyear_of_cat &lt;- month.name\nstr_sub(year_of_cat, 1, 2) &lt;- \"Cat\"\nyear_of_cat\n\n [1] \"Catnuary\"   \"Catbruary\"  \"Catrch\"     \"Catril\"     \"Caty\"      \n [6] \"Catne\"      \"Catly\"      \"Catgust\"    \"Catptember\" \"Cattober\"  \n[11] \"Catvember\"  \"Catcember\""
  },
  {
    "objectID": "Tutorial4.html#pattern-searches",
    "href": "Tutorial4.html#pattern-searches",
    "title": "Tutorial 4 Natural Language Processing",
    "section": "Pattern searches",
    "text": "Pattern searches\nWhere regular expressions really kick in isn’t with these sorts of operations though. It’s in searching for specific patterns. Let’s start illustrating by looking at one type of pattern: a word! To illustrate these pattern searches, we’ll use another set of words, a vector of names of fruit.\n\nfruit\n\n [1] \"apple\"             \"apricot\"           \"avocado\"          \n [4] \"banana\"            \"bell pepper\"       \"bilberry\"         \n [7] \"blackberry\"        \"blackcurrant\"      \"blood orange\"     \n[10] \"blueberry\"         \"boysenberry\"       \"breadfruit\"       \n[13] \"canary melon\"      \"cantaloupe\"        \"cherimoya\"        \n[16] \"cherry\"            \"chili pepper\"      \"clementine\"       \n[19] \"cloudberry\"        \"coconut\"           \"cranberry\"        \n[22] \"cucumber\"          \"currant\"           \"damson\"           \n[25] \"date\"              \"dragonfruit\"       \"durian\"           \n[28] \"eggplant\"          \"elderberry\"        \"feijoa\"           \n[31] \"fig\"               \"goji berry\"        \"gooseberry\"       \n[34] \"grape\"             \"grapefruit\"        \"guava\"            \n[37] \"honeydew\"          \"huckleberry\"       \"jackfruit\"        \n[40] \"jambul\"            \"jujube\"            \"kiwi fruit\"       \n[43] \"kumquat\"           \"lemon\"             \"lime\"             \n[46] \"loquat\"            \"lychee\"            \"mandarine\"        \n[49] \"mango\"             \"mulberry\"          \"nectarine\"        \n[52] \"nut\"               \"olive\"             \"orange\"           \n[55] \"pamelo\"            \"papaya\"            \"passionfruit\"     \n[58] \"peach\"             \"pear\"              \"persimmon\"        \n[61] \"physalis\"          \"pineapple\"         \"plum\"             \n[64] \"pomegranate\"       \"pomelo\"            \"purple mangosteen\"\n[67] \"quince\"            \"raisin\"            \"rambutan\"         \n[70] \"raspberry\"         \"redcurrant\"        \"rock melon\"       \n[73] \"salal berry\"       \"satsuma\"           \"star fruit\"       \n[76] \"strawberry\"        \"tamarillo\"         \"tangerine\"        \n[79] \"ugli fruit\"        \"watermelon\"       \n\n\nWe can look to see which include the string berry in their name in a whole lot of different ways.\n\n# which elements in the vector have berry in the name\nlength(str_which(fruit, \"berry\"))\n\n[1] 14\n\n# does the fruit contain the string `berry`\nstr_detect(fruit, \"berry\")\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n[37] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[49] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n[73]  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n\n# name the fruits that contain the string `berry`\nstr_subset(fruit, \"berry\")\n\n [1] \"bilberry\"    \"blackberry\"  \"blueberry\"   \"boysenberry\" \"cloudberry\" \n [6] \"cranberry\"   \"elderberry\"  \"goji berry\"  \"gooseberry\"  \"huckleberry\"\n[11] \"mulberry\"    \"raspberry\"   \"salal berry\" \"strawberry\""
  },
  {
    "objectID": "Tutorial4.html#classes-and-quantifiers",
    "href": "Tutorial4.html#classes-and-quantifiers",
    "title": "Tutorial 4 Natural Language Processing",
    "section": "Classes and quantifiers",
    "text": "Classes and quantifiers\nThere are also a series of escaped characters that have special meanings. These let you match, for example, any alphanumeric character (), any space (), or any number (. These become really helpful when combined with quantifiers, which indicate the number of occurrences of the character. On this, * indicates zero or more of the character, and + indicates one more of the character.\n\n# create some strings\ntutone &lt;- c(\"Jenny Jenny, who can I turn to?\", \"867-5309\")\n\n# match any number string of more than one number\nstr_extract_all(tutone, \"\\\\d+\")\n\n[[1]]\ncharacter(0)\n\n[[2]]\n[1] \"867\"  \"5309\"\n\n# match any alphanumeric string of 0 or more\nstr_extract_all(tutone, \"\\\\w*\")\n\n[[1]]\n [1] \"Jenny\" \"\"      \"Jenny\" \"\"      \"\"      \"who\"   \"\"      \"can\"   \"\"     \n[10] \"I\"     \"\"      \"turn\"  \"\"      \"to\"    \"\"      \"\"     \n\n[[2]]\n[1] \"867\"  \"\"     \"5309\" \"\"    \n\n# match any number string of more than three numbers; note the comma\nstr_extract_all(tutone, \"\\\\d{4,}\")\n\n[[1]]\ncharacter(0)\n\n[[2]]\n[1] \"5309\""
  },
  {
    "objectID": "Tutorial4.html#extracting-data",
    "href": "Tutorial4.html#extracting-data",
    "title": "Tutorial 4 Natural Language Processing",
    "section": "Extracting data",
    "text": "Extracting data\nWhere regular expressions start to get really powerful for us is in automating the extraction of information from rich digital text. Consider an example where we want to identify mentions of courts from inaugural addresses. We can leverage regular expressions to do just that:\n\nstr_match(text, \" [C|c]ourt[s]*|[J|j]udicia[\\\\w]+ \")\n\n      [,1]        \n [1,] NA          \n [2,] NA          \n [3,] NA          \n [4,] NA          \n [5,] NA          \n [6,] NA          \n [7,] \" court\"    \n [8,] NA          \n [9,] NA          \n[10,] \"judiciary \"\n[11,] NA          \n[12,] NA          \n[13,] \"judicial \" \n[14,] \"judiciary \"\n[15,] NA          \n[16,] \"judicial \" \n[17,] NA          \n[18,] \"judicial \" \n[19,] \" Court\"    \n[20,] NA          \n[21,] NA          \n[22,] NA          \n[23,] NA          \n[24,] \" court\"    \n[25,] NA          \n[26,] \"judicial \" \n[27,] \" courts\"   \n[28,] \" courts\"   \n[29,] NA          \n[30,] NA          \n[31,] \" courts\"   \n[32,] NA          \n[33,] NA          \n[34,] \" court\"    \n[35,] \" Court\"    \n[36,] \"judicial \" \n[37,] \" court\"    \n[38,] NA          \n[39,] NA          \n[40,] NA          \n[41,] NA          \n[42,] NA          \n[43,] NA          \n[44,] NA          \n[45,] NA          \n[46,] NA          \n[47,] NA          \n[48,] NA          \n[49,] NA          \n[50,] NA          \n[51,] NA          \n[52,] NA          \n[53,] NA          \n[54,] NA          \n[55,] NA          \n[56,] NA          \n[57,] NA          \n[58,] NA          \n[59,] NA          \n\n\nWhile that isn’t necessarily the most useful, consider if you were looking instead for something like the authors of each text, where the author was featured in a consistent format at the start of each text. While you could go through by hand and code each of those, it is much more straightforward to do this with regular expressions. As you start working with your corpus — and particularly if you are in any way thinking of coding something by hand from the corpus — take some time to think and to chat with me about whether it’s something we can do with regular expressions."
  },
  {
    "objectID": "Tutorial5.html",
    "href": "Tutorial5.html",
    "title": "Tutorial 5 Preprocessing",
    "section": "",
    "text": "This is our fifth tutorial for running R. In this tutorial, we’ll learn about text pre-processing. Text data is often messy and noisy, and careful preprocessing can soften the edges for downstream analyses.\nBy the end of this notebook, you should be familiar with the following:"
  },
  {
    "objectID": "Tutorial5.html#quanteda-package",
    "href": "Tutorial5.html#quanteda-package",
    "title": "Tutorial 5 Preprocessing",
    "section": "Quanteda package",
    "text": "Quanteda package\nWith quanteda, we can remove stopwords using any of few pre-defined lists that come shipped with the package. Here, we can print that list out first, then remove the tokens:\n\nlength(print(stopwords(\"en\")))\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\n\n[1] 175\n\n# Available for other languages as well\nhead(stopwords(\"zh\", source=\"misc\"),50)\n\n [1] \"按\"     \"按照\"   \"俺\"     \"们\"     \"阿\"     \"别\"     \"别人\"   \"别处\"  \n [9] \"是\"     \"别的\"   \"别管\"   \"说\"     \"不\"     \"不仅\"   \"不但\"   \"不光\"  \n[17] \"不单\"   \"不只\"   \"不外乎\" \"不如\"   \"不妨\"   \"不尽\"   \"然\"     \"不得\"  \n[25] \"不怕\"   \"不惟\"   \"不成\"   \"不拘\"   \"料\"     \"不是\"   \"不比\"   \"不然\"  \n[33] \"特\"     \"不独\"   \"不管\"   \"不至于\" \"若\"     \"不论\"   \"不过\"   \"不问\"  \n[41] \"比\"     \"方\"     \"比如\"   \"及\"     \"本身\"   \"本着\"   \"本地\"   \"本人\"  \n[49] \"本\"     \"巴巴\"  \n\n\n\n# remove stopwords from our tokens object\nphilosophers_stone_tokens1 &lt;- tokens_select(philosophers_stone_tokens, \n                     pattern = stopwords(\"en\"),\n                     selection = \"remove\")\n\nlength(philosophers_stone_tokens1)\n\n[1] 17\n\nprint(philosophers_stone_tokens1)\n\nTokens consisting of 17 documents and 6 docvars.\ntext1 :\n [1] \"boy\"       \"lived\"     \"mr\"        \"mrs\"       \"dursley\"   \"number\"   \n [7] \"four\"      \"privet\"    \"drive\"     \"proud\"     \"say\"       \"perfectly\"\n[ ... and 2,507 more ]\n\ntext2 :\n [1] \"vanishing\" \"glass\"     \"nearly\"    \"ten\"       \"years\"     \"passed\"   \n [7] \"since\"     \"dursleys\"  \"woken\"     \"find\"      \"nephew\"    \"front\"    \n[ ... and 1,906 more ]\n\ntext3 :\n [1] \"letters\"      \"one\"          \"escape\"       \"brazilian\"    \"boa\"         \n [6] \"constrictor\"  \"earned\"       \"harry\"        \"longest-ever\" \"punishment\"  \n[11] \"time\"         \"allowed\"     \n[ ... and 2,213 more ]\n\ntext4 :\n [1] \"keeper\"   \"keys\"     \"boom\"     \"knocked\"  \"dudley\"   \"jerked\"  \n [7] \"awake\"    \"`\"        \"cannon\"   \"`\"        \"said\"     \"stupidly\"\n[ ... and 2,201 more ]\n\ntext5 :\n [1] \"diagon\"   \"alley\"    \"harry\"    \"woke\"     \"early\"    \"next\"    \n [7] \"morning\"  \"although\" \"tell\"     \"daylight\" \"kept\"     \"eyes\"    \n[ ... and 4,141 more ]\n\ntext6 :\n [1] \"journey\"        \"platform\"       \"nine\"           \"three-quarters\"\n [5] \"harry's\"        \"last\"           \"month\"          \"dursleys\"      \n [9] \"fun\"            \"true\"           \"dudley\"         \"now\"           \n[ ... and 3,759 more ]\n\n[ reached max_ndoc ... 11 more documents ]"
  },
  {
    "objectID": "Tutorial5.html#tidytext-package",
    "href": "Tutorial5.html#tidytext-package",
    "title": "Tutorial 5 Preprocessing",
    "section": "Tidytext package",
    "text": "Tidytext package\nIn addition to quanteda, there are other packages containing different lists of stopwords, for example, tidytext. Tidytext provides various lexicons for English stop words. Sources of the stopwords are: snowball, SMART, or onix.\n\ntidytextstopwords &lt;- tidytext::stop_words\nView(tidytextstopwords)\ntable(tidytextstopwords$lexicon)\n\n\n    onix    SMART snowball \n     404      571      174 \n\n#OR\ntidytext::get_stopwords(source = \"smart\")\n\n# A tibble: 571 × 2\n   word        lexicon\n   &lt;chr&gt;       &lt;chr&gt;  \n 1 a           smart  \n 2 a's         smart  \n 3 able        smart  \n 4 about       smart  \n 5 above       smart  \n 6 according   smart  \n 7 accordingly smart  \n 8 across      smart  \n 9 actually    smart  \n10 after       smart  \n# ℹ 561 more rows\n\ntidytext::get_stopwords(language = \"de\") # change language\n\n# A tibble: 231 × 2\n   word  lexicon \n   &lt;chr&gt; &lt;chr&gt;   \n 1 aber  snowball\n 2 alle  snowball\n 3 allem snowball\n 4 allen snowball\n 5 aller snowball\n 6 alles snowball\n 7 als   snowball\n 8 also  snowball\n 9 am    snowball\n10 an    snowball\n# ℹ 221 more rows\n\n\nRemove stop words using tidytext stopwords. And we see find the number of tokens after removing stopwords is different as we use different packages.\n\ntidystop &lt;- get_stopwords(source = \"smart\")\n\nphilosophers_stone_tokens2 &lt;-     tokens_select(philosophers_stone_tokens, \n                     pattern = tidystop$word,\n                     selection = \"remove\")\n\nlength(philosophers_stone_tokens2)\n\n[1] 17\n\nprint(philosophers_stone_tokens2)\n\nTokens consisting of 17 documents and 6 docvars.\ntext1 :\n [1] \"boy\"       \"lived\"     \"mr\"        \"mrs\"       \"dursley\"   \"number\"   \n [7] \"privet\"    \"drive\"     \"proud\"     \"perfectly\" \"normal\"    \"people\"   \n[ ... and 1,989 more ]\n\ntext2 :\n [1] \"vanishing\" \"glass\"     \"ten\"       \"years\"     \"passed\"    \"dursleys\" \n [7] \"woken\"     \"find\"      \"nephew\"    \"front\"     \"step\"      \"privet\"   \n[ ... and 1,533 more ]\n\ntext3 :\n [1] \"letters\"      \"escape\"       \"brazilian\"    \"boa\"          \"constrictor\" \n [6] \"earned\"       \"harry\"        \"longest-ever\" \"punishment\"   \"time\"        \n[11] \"allowed\"      \"cupboard\"    \n[ ... and 1,814 more ]\n\ntext4 :\n [1] \"keeper\"   \"keys\"     \"boom\"     \"knocked\"  \"dudley\"   \"jerked\"  \n [7] \"awake\"    \"`\"        \"cannon\"   \"`\"        \"stupidly\" \"crash\"   \n[ ... and 1,760 more ]\n\ntext5 :\n [1] \"diagon\"   \"alley\"    \"harry\"    \"woke\"     \"early\"    \"morning\" \n [7] \"daylight\" \"eyes\"     \"shut\"     \"tight\"    \"`\"        \"dream\"   \n[ ... and 3,315 more ]\n\ntext6 :\n [1] \"journey\"        \"platform\"       \"three-quarters\" \"harry's\"       \n [5] \"month\"          \"dursleys\"       \"fun\"            \"true\"          \n [9] \"dudley\"         \"scared\"         \"harry\"          \"stay\"          \n[ ... and 2,898 more ]\n\n[ reached max_ndoc ... 11 more documents ]"
  },
  {
    "objectID": "Tutorial6.html",
    "href": "Tutorial6.html",
    "title": "Tutorial 6 Text Representation I",
    "section": "",
    "text": "In this tutorial, we’ll learn about representing texts. This week, we’ll continue looking at the Harry Potter series. We’ll first install and load the packages for today’s notebook.\n\nlibrary(devtools)\n\nLoading required package: usethis\n\nlibrary(tidytext)\nlibrary(plyr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::arrange()   masks plyr::arrange()\n✖ purrr::compact()   masks plyr::compact()\n✖ dplyr::count()     masks plyr::count()\n✖ dplyr::desc()      masks plyr::desc()\n✖ dplyr::failwith()  masks plyr::failwith()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::id()        masks plyr::id()\n✖ dplyr::lag()       masks stats::lag()\n✖ dplyr::mutate()    masks plyr::mutate()\n✖ dplyr::rename()    masks plyr::rename()\n✖ dplyr::summarise() masks plyr::summarise()\n✖ dplyr::summarize() masks plyr::summarize()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(quanteda)\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\nParallel computing: disabled\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textplots)\n\nFirst load all the Harry Potter books.\n\n# Define the folder containing the .rda files (Change to your path). \nfolder &lt;- \"/Users/mpang/Dropbox/Teaching Resources/DACSS_TAD/HarryPotter\"\n\n# Get the list of all .rda files in the folder\nrda_files &lt;- list.files(folder, pattern = \"\\\\.rda$\", full.names = TRUE)\n\n# Load all .rda files into the environment\nlapply(rda_files, load, .GlobalEnv)\n\n[[1]]\n[1] \"chamber_of_secrets\"\n\n[[2]]\n[1] \"deathly_hallows\"\n\n[[3]]\n[1] \"goblet_of_fire\"\n\n[[4]]\n[1] \"half_blood_prince\"\n\n[[5]]\n[1] \"order_of_the_phoenix\"\n\n[[6]]\n[1] \"philosophers_stone\"\n\n[[7]]\n[1] \"prisoner_of_azkaban\"\n\n\nAs a reminder, we have seven books — each stored as a character vector where each chapter is an element in that vector — now available in our workspace. These are:\n\nphilosophers_stone: Harry Potter and the Philosophers Stone (1997)\nchamber_of_secrets: Harry Potter and the Chamber of Secrets (1998)\nprisoner_of_azkaban: Harry Potter and the Prisoner of Azkaban (1999)\ngoblet_of_fire: Harry Potter and the Goblet of Fire (2000)\norder_of_the_phoenix: Harry Potter and the Order of the Phoenix\nhalf_blood_price: Harry Potter and the Half-Blood Prince (2005)\ndeathly_hallows: Harry Potter and the Deathly Hallows (2007)\n\nAs you’ll recall, we want to convert these to corpus objects that are easier to work with.\n\nphilosophers_stone_corpus &lt;- corpus(philosophers_stone)\nphilosophers_stone_summary &lt;- summary(philosophers_stone_corpus) \nphilosophers_stone_summary\n\nCorpus consisting of 17 documents, showing 17 documents:\n\n   Text Types Tokens Sentences\n  text1  1271   5693       350\n  text2  1066   4154       237\n  text3  1226   4656       297\n  text4  1193   4832       322\n  text5  1818   8446       563\n  text6  1563   8016       566\n  text7  1374   5487       351\n  text8  1095   3608       198\n  text9  1422   6195       411\n text10  1293   5237       334\n text11  1110   4215       277\n text12  1509   6790       447\n text13  1076   3953       262\n text14  1110   4394       308\n text15  1385   6486       459\n text16  1581   8357       591\n text17  1489   7172       506\n\n\n\n# add an indicator for the book; this will be useful later when we add all the books together into a single corpus\nphilosophers_stone_summary$book &lt;- \"Philosopher's Stone\"\n\n# create a chapter indicator\nphilosophers_stone_summary$chapter &lt;- as.numeric(str_extract(philosophers_stone_summary$Text, \"[0-9]+\"))\n\n# add the metadata\ndocvars(philosophers_stone_corpus) &lt;- philosophers_stone_summary\n\n\nDocument-Feature Matrix\nA common first-step in text analysis is converting texts from their written format (“The dog runs down the hall.”) to a numerical representation of that language. The basic approach for representing a sentence is the document-feature matrix, sometimes also call the document-term matrix. Here, we are creating a matrix where the rows indicate documents, the columns indicate words, and the value of each cell in the matrix is the count of the word (column) for the document (row).\nWe can use quanteda’s dfm command to generate the document-feature matrix directly from the corpus object.\n\n# create the dfm\nphilosophers_stone_dfm &lt;- dfm(tokens(philosophers_stone_corpus))\n\n# find out a quick summary of the dfm\nphilosophers_stone_dfm\n\nDocument-feature matrix of: 17 documents, 6,116 features (80.08% sparse) and 6 docvars.\n       features\ndocs    the boy who lived mr   . and mrs dursley   ,\n  text1 204   9   9     2 30 417 102  21      45 290\n  text2 181   6   7     3  3 253  84   5       3 235\n  text3 222   1   6     0  4 306 112   3       0 230\n  text4 126   5  11     2  2 289  74   0       6 289\n  text5 274  14  10     0 26 569 155   0       0 526\n  text6 282  26  18     0  0 489 185   0       0 500\n[ reached max_ndoc ... 11 more documents, reached max_nfeat ... 6,106 more features ]\n\n\nThe summary of the document-feature matrix provides a few interesting notes for us. We have the number of documents (17 chapters) and the number of features (6,116). We also get a note about sparsity. This refers to the number of 0 entries in our matrix; here, 80.08% of our matrix is 0. The high sparsity of text data is a well-recognized trait and something we will regularly return to.\nBelow the summary statement, we can see the first few rows and columns of our document-feature matrix. The first entry in the matrix, for instance, indicates that “the” appears 204 times in the first chapter (text1) of “The Philosopher’s Stone”. This reminds us that we did not preprocess our corpus. Fortunately, the dfm() function explicitly includes the ability to preprocess when you are creating your matrix. Indeed, that’s why the text is lower-cased above; the function defaults to removing capitalization. We can be a bit more heavy-handed with our preprocessing as follows.\n\n# create the dfm\nphilosophers_stone_dfm &lt;- tokens(philosophers_stone_corpus,\n                                    remove_punct = TRUE,\n                                    remove_numbers = TRUE) %&gt;%\n                           dfm(tolower=TRUE) %&gt;%\n                           dfm_remove(stopwords('english'))\n# find out a quick summary of the dfm\nphilosophers_stone_dfm\n\nDocument-feature matrix of: 17 documents, 5,918 features (81.91% sparse) and 6 docvars.\n       features\ndocs    boy lived mr mrs dursley number four privet drive proud\n  text1   9     2 30  21      45      7    5      8     9     1\n  text2   6     3  3   5       3      1    2      1     1     0\n  text3   1     0  4   3       0      1    4      5     6     0\n  text4   5     2  2   0       6      0    0      0     0     2\n  text5  14     0 26   0       0      0    2      0     0     2\n  text6  26     0  0   0       0      4    4      0     1     0\n[ reached max_ndoc ... 11 more documents, reached max_nfeat ... 5,908 more features ]\n\n\n\n\nWorking with DFMs\nOnce we have our document-feature matrix, and have made some preprocessing decisions, we can turn to thinking about what we can learn with this new representation. Let’s start with some basics. It’s really easy to see the most frequent terms (features) now.\n\ntopfeatures(philosophers_stone_dfm, 20)\n\n         `      harry       said        ron     hagrid       back   hermione \n      4757       1213        794        410        336        261        257 \n       one        got       like        get       know       just        see \n       254        198        194        194        188        180        180 \n professor     looked        now      snape dumbledore     around \n       180        169        166        145        143        142 \n\n\nWe see the symbol “`” as the top 1 feature (which is annoying). Even though we removed punctuation, some special characters might not have been treated as punctuation by default. Let’s remove this symbol before moving on.\n\nphilosophers_stone_dfm &lt;- dfm_remove(philosophers_stone_dfm, pattern = \"`\", valuetype = \"fixed\")\n\ntopfeatures(philosophers_stone_dfm, 20)\n\n     harry       said        ron     hagrid       back   hermione        one \n      1213        794        410        336        261        257        254 \n       got       like        get       know       just        see  professor \n       198        194        194        188        180        180        180 \n    looked        now      snape dumbledore     around      going \n       169        166        145        143        142        135 \n\n\nPerhaps you’d also like to know something like which words were only used within a particular text. We can look, for instance, at the final chapter to see what words were uniquely used there.\n\nfinal_chapter_words &lt;- as.vector(colSums(philosophers_stone_dfm) == philosophers_stone_dfm[\"text17\",])\ncolnames(philosophers_stone_dfm)[final_chapter_words]\n\n  [1] \"treble\"          \"type\"            \"overgrown\"       \"suspect\"        \n  [5] \"p-p-poor\"        \"st-stuttering\"   \"p-professor\"     \"unpopular\"      \n  [9] \"nosy\"            \"scurrying\"       \"trolls\"          \"concentrating\"  \n [13] \"idly\"            \"frighten\"        \"presenting\"      \"binding\"        \n [17] \"loathed\"         \"spasm\"           \"master's\"        \"instructions\"   \n [21] \"traveled\"        \"served\"          \"faithfully\"      \"mistakes\"       \n [25] \"displeased\"      \"trailed\"         \"myseff\"          \"blood-red\"      \n [29] \"incredibly\"      \"rooting\"         \"muscle\"          \"slits\"          \n [33] \"mere\"            \"vapor\"           \"another's\"       \"willing\"        \n [37] \"strengthened\"    \"faithful\"        \"create\"          \"surged\"         \n [41] \"begging\"         \"liar\"            \"value\"           \"courageous\"     \n [45] \"vain\"            \"flame\"           \"seize\"           \"needle-sharp\"   \n [49] \"seared\"          \"lessened\"        \"blistering\"      \"lunged\"         \n [53] \"agony\"           \"pinning\"         \"palms\"           \"raw\"            \n [57] \"perform\"         \"deadly\"          \"instinct\"        \"aaaargh\"        \n [61] \"yells\"           \"grasp\"           \"blackness\"       \"swam\"           \n [65] \"linen\"           \"tokens\"          \"admirers\"        \"naturally\"      \n [69] \"misters\"         \"responsible\"     \"hygienic\"        \"confiscated\"    \n [73] \"distracted\"      \"prevent\"         \"blankly\"         \"stored\"         \n [77] \"affairs\"         \"well-organized\"  \"beings\"          \"precisely\"      \n [81] \"increases\"       \"truly\"           \"nevertheless\"    \"delayed\"        \n [85] \"merely\"          \"therefore\"       \"treated\"         \"beg\"            \n [89] \"alas\"            \"loved\"           \"hatred\"          \"marked\"         \n [93] \"bird\"            \"sheet\"           \"twinkled\"        \"detest\"         \n [97] \"unlike\"          \"dreamily\"        \"debt\"            \"hating\"         \n[101] \"otherwise\"       \"unfortunate\"     \"youth\"           \"vomitflavored\"  \n[105] \"toffee\"          \"golden-brown\"    \"nurse\"           \"pleaded\"        \n[109] \"resting\"         \"fling\"           \"audience\"        \"rocker\"         \n[113] \"stopping\"        \"accident\"        \"end-of-year\"     \"steamrollered\"  \n[117] \"food'll\"         \"bustled\"         \"stiffily\"        \"risky\"          \n[121] \"indoors\"         \"chucked\"         \"grief\"           \"remorse\"        \n[125] \"leaking\"         \"sandwich\"        \"chuckle\"         \"fix\"            \n[129] \"shoulda\"         \"leather-covered\" \"pomfrey's\"       \"fussing\"        \n[133] \"insisting\"       \"checkup\"         \"decked\"          \"slytherin's\"    \n[137] \"serpent\"         \"hush\"            \"fortunately\"     \"waffle\"         \n[141] \"fuller\"          \"awarding\"        \"thus\"            \"fifty-two\"      \n[145] \"twenty-six\"      \"stamping\"        \"account\"         \"ahem\"           \n[149] \"dish\"            \"radish\"          \"sunburn\"         \"best-played\"    \n[153] \"award\"           \"din\"             \"seventy-two\"     \"gradually\"      \n[157] \"kinds\"           \"takes\"           \"explosion\"       \"nudged\"         \n[161] \"downfall\"        \"jot\"             \"grades\"          \"scraped\"        \n[165] \"abysmal\"         \"wardrobes\"       \"toilets\"         \"boarding\"       \n[169] \"greener\"         \"tidier\"          \"towns\"           \"wizened\"        \n[173] \"gate\"            \"twos\"            \"threes\"          \"alarming\"       \n[177] \"gateway\"         \"squealed\"        \"purple-faced\"    \"mustached\"      \n[181] \"manner\"          \"holiday\"         \"spreading\"      \n\n\n\n\nWord clouds\nWe started out earlier this semester by making those fancy little word clouds. We haven’t done much of that since, as we’ve been busy getting our hands on data, getting it into R, and thinking about some of the more NLP-centric types of approaches one might take. Now that we’re moving to representing texts, though, we can quickly return to word clouds.\nThe general idea here is that the size of the word corresponds to the frequency of the term in the corpus. That is, we are characterizing the most frequent terms in a corpus. Importantly, that means the axes don’t really mean anything in these clouds, nor does the orientation of the term. For that reason, though these are pretty, they aren’t terribly useful.\n\n# programs often work with random initialization, yielding different outcomes.\n# we can set a standard starting point though to ensure the same output.\nset.seed(1234)\n\n# draw the wordcloud\ntextplot_wordcloud(philosophers_stone_dfm, min_count = 50, random_order = FALSE)\n\n\n\n\n\n\n\n\nOne way to get a bit more utility is to use the comparison option within the function to plot a comparison of wordclouds across two different documents. Here’s an example.\n\n# narrow to first and last chapters\nsmallDfm &lt;- philosophers_stone_dfm[c(1,17),]\n\n# draw the wordcloud\ntextplot_wordcloud(smallDfm, comparison = TRUE, min_count = 10, random_order = FALSE)\n\n\n\n\n\n\n\n\n\n\nZipf’s Law\nNow that our data are nicely formatted, we can also look at one of the statistical regularities that characterizes language, Zipf’s Law. Word frequencies are distributed according to Zipf’s law. What does that mean? Let’s take a look at the distribution of word frequencies.\n\n# first, we need to create a word frequency variable and the rankings\nword_counts &lt;- as.data.frame(sort(colSums(philosophers_stone_dfm),dec=T))\ncolnames(word_counts) &lt;- c(\"Frequency\")\nword_counts$Rank &lt;- c(1:ncol(philosophers_stone_dfm))\nword_counts$Word &lt;- rownames(word_counts)\nhead(word_counts)\n\n         Frequency Rank     Word\nharry         1213    1    harry\nsaid           794    2     said\nron            410    3      ron\nhagrid         336    4   hagrid\nback           261    5     back\nhermione       257    6 hermione\n\n\n\n# We only want to label top 10 words\nword_counts$Label &lt;- ifelse(word_counts$Rank &lt;= 10, word_counts$Word, NA)\n\n# now we can plot this\nggplot(word_counts, mapping = aes(x = Rank, y = Frequency)) + \n  geom_point() +\n  geom_text(aes(label = Label),vjust = -0.5, hjust = 0.5, size = 3) +\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \n  theme_bw()\n\nWarning: Removed 5907 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\n\n\nUpdating our DFMs\nHaving seen what we are working with here, we might start to think that our matrix still contains too many uninformative or very rare terms. We can trim our DFM in two different ways related to feature frequencies using dfm_trim().\n\n# trim based on the overall frequency (i.e., the word counts)\nsmaller_dfm &lt;- dfm_trim(philosophers_stone_dfm, min_termfreq = 10)\n\n# trim based on the proportion of documents that the feature appears in; here, the feature needs to appear in more than 10% of documents (chapters)\nsmaller_dfm &lt;- dfm_trim(smaller_dfm, min_docfreq = 0.1, docfreq_type = \"prop\")\n\nsmaller_dfm\n\nDocument-feature matrix of: 17 documents, 885 features (42.28% sparse) and 6 docvars.\n       features\ndocs    boy mr mrs dursley number four privet drive say normal\n  text1   9 30  21      45      7    5      8     9   7      5\n  text2   6  3   5       3      1    2      1     1   3      0\n  text3   1  4   3       0      1    4      5     6   0      0\n  text4   5  2   0       6      0    0      0     0   7      1\n  text5  14 26   0       0      0    2      0     0  12      0\n  text6  26  0   0       0      4    4      0     1   5      0\n[ reached max_ndoc ... 11 more documents, reached max_nfeat ... 875 more features ]\n\n\n\ntextplot_wordcloud(smaller_dfm, min_count = 50,\n                   random_order = FALSE)\n\n\n\n\n\n\n\n\nNote that our sparsity is now significantly decreased. We can also do this in the opposite direction as a way of avoiding features that appear frequently in our corpus and thus are perhaps more uninformative in the particular setting but that would not be caught by a standard stop-word list. As an example, we may want to drop the feature “harry” from the analysis of Harry Potter books, since every.single.reference. to Harry increases that count.\n\nsmaller_dfm2 &lt;- dfm_trim(philosophers_stone_dfm, max_termfreq = 250)\nsmaller_dfm2 &lt;- dfm_trim(smaller_dfm2, max_docfreq = .5, docfreq_type = \"prop\")\n\nsmaller_dfm2\n\nDocument-feature matrix of: 17 documents, 5,402 features (87.18% sparse) and 6 docvars.\n       features\ndocs    lived mrs dursley number privet drive proud perfectly normal thank\n  text1     2  21      45      7      8     9     1         2      5     2\n  text2     3   5       3      1      1     1     0         0      0     0\n  text3     0   3       0      1      5     6     0         0      0     0\n  text4     2   0       6      0      0     0     2         0      1     1\n  text5     0   0       0      0      0     0     2         1      0     0\n  text6     0   0       0      4      0     1     0         0      0     1\n[ reached max_ndoc ... 11 more documents, reached max_nfeat ... 5,392 more features ]\n\n# when you are doing the quiz, you might want to leverage this chunk of code \nas.vector(smaller_dfm2[,which(colnames(smaller_dfm2) == \"voldemort\")])\n\n [1]  4  0  0  1  0  2  0  0  0  0  0  0  0  0  4  6 14\n\n\n\ntextplot_wordcloud(smaller_dfm2, min_count = 20,\n                   random_order = FALSE)\n\n\n\n\n\n\n\n\n\n\nFeature Co-occurrence matrix\nRepresenting text-as-data as a document-feature matrix allows us to learn both about document-level characteristics and about corpus-level characteristics. However, it tells us less about how words within the corpus relate to one another. For this, we can turn to the feature co-occurrence matrix. The idea here is to construct a matrix that — instead of presenting the times a word appears within a document — presents the *number of times word{a} appears in the same document as word{b}. As before creating the feature co-occurrence matrix is straight-forward.\n\n# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters\nsmaller_dfm3 &lt;- dfm_trim(philosophers_stone_dfm, min_termfreq = 10)\nsmaller_dfm3 &lt;- dfm_trim(smaller_dfm3, min_docfreq = .3, docfreq_type = \"prop\")\n\n# create fcm from dfm\nsmaller_fcm &lt;- fcm(smaller_dfm3)\n\n# check the dimensions (i.e., the number of rows and the number of columnns) of the matrix we created\ndim(smaller_fcm)\n\n[1] 775 775\n\n\nNotice that the number of rows and columns are the same; that’s because they are each the vocabulary, with the entry being the number of times the row word and column word co-occur (with the diagonal elements undefined). Later on this semester, we’ll leverage these word co-occurrence matrices to estimate word embedding models.\nFor now, let’s use what we’ve got to try to learn a bit more about what features co-occur, and how, within our book. To do, we’ll visualize a semantic network using textplot_network().\n\n# pull the top features\nmyFeatures &lt;- names(sort(colSums(smaller_fcm), decreasing = TRUE)[1:30])\n\n# retain only those top features as part of our matrix\neven_smaller_fcm &lt;- fcm_select(smaller_fcm, pattern = myFeatures, selection = \"keep\")\n\n# check dimensions\ndim(even_smaller_fcm)\n\n[1] 30 30\n\n# compute size weight for vertices in network\nsize &lt;- log(colSums(even_smaller_fcm))\n\n# create plot\ntextplot_network(even_smaller_fcm, vertex_size = size/ max(size) * 3)\n\n\n\n\n\n\n\n\nThe graph above is build on dfm, so it does not show which words are closer in a sentence. If we make fcm using the original documents, and set up a window, then we can have more information about which words are more likely to appear together.\n\nbook1_token &lt;- tokens(philosophers_stone_corpus,\n                    remove_punct = TRUE,\n                    remove_numbers = TRUE) %&gt;%\n  tokens_tolower()\n\nbook1_token &lt;- tokens_select(book1_token,\n                     pattern = c(stopwords(\"en\"),\"`\"),\n                     selection = \"remove\")\n\ntry_fcm &lt;- fcm(book1_token,context = \"window\", window=2)\n\ntry_fcm\n\nFeature co-occurrence matrix of: 5,917 by 5,917 features.\n         features\nfeatures  boy lived mr mrs dursley number four privet drive proud\n  boy       4     3  1   0       0      0    0      0     0     0\n  lived     0     0  1   1       0      0    0      0     0     0\n  mr        0     0  0   3      33      0    0      0     0     1\n  mrs       0     0  0   4      17      1    0      0     0     0\n  dursley   0     0  0   0       0      1    1      0     0     0\n  number    0     0  0   0       0      0    7      1     1     0\n  four      0     0  0   0       0      0    0      1     1     0\n  privet    0     0  0   0       0      0    0      0    16     1\n  drive     0     0  0   0       0      0    0      0     0     1\n  proud     0     0  0   0       0      0    0      0     0     0\n[ reached max_feat ... 5,907 more features, reached max_nfeat ... 5,907 more features ]\n\nbook1_Features &lt;- names(sort(colSums(try_fcm), decreasing = TRUE)[1:30])\n\nbook1_small_fcm &lt;- fcm_select(try_fcm, pattern = book1_Features, selection = \"keep\")\n\ntextplot_network(book1_small_fcm, vertex_size = 2)\n\n\n\n\n\n\n\n\nWe observe in the graph that “Uncle Vernon”, and “Professor McGonagall” often appear together. Of course, Harry, Ron, and Hermione also appear together."
  },
  {
    "objectID": "Tutorial7.html",
    "href": "Tutorial7.html",
    "title": "Tutorial7_WordEmbeddings/Text Representation II",
    "section": "",
    "text": "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. They are a tool for identifying similarities between words in a corpus by using some form of model to predict the co-occurrence of words within a small chunk of text.\nWe’ll be using the text2vec package. text2vec was one of the first implementations of word embeddings functionality in R, and is designed to run fast, relatively speaking. Still, it’s important to remember that our computational complexity is amping up here, so don’t expect immediate results."
  },
  {
    "objectID": "Tutorial7.html#poki-dataset",
    "href": "Tutorial7.html#poki-dataset",
    "title": "Week7_WordEmbeddings/Text Representation II",
    "section": "PoKi Dataset",
    "text": "PoKi Dataset\nWe’ll be using PoKi, a corpus of poems written by children and teenagers from grades 1 to 12.\nOne thing to flag right off the bat is the really interesting dynamics related to who is writing these posts. We need to keep in mind that the children writing these texts are going to use less formal writing and more imaginative stories. Given this, we’ll focus on analogies that are more appropriate for this context; here, we’ll aim to create word embeddings that can recreate these two equations:\n\ncat - meow + bark = dog\n\n\nmom - girl + boy = dad\n\nBy the end, we should hopefully be able to recreate these by creating and fitting our GloVe models. But first, let’s perform the necessary pre-processing steps before creating our embedding models.\nLet’s download and read in the data:\n\n\nCode\n# Create file\ntemp &lt;- tempfile()\n\n# Downloads and unzip file\ndownload.file(\"https://raw.githubusercontent.com/whipson/PoKi-Poems-by-Kids/master/poki.csv\", temp)\n\n\n\n\nCode\n# Reads in downloaded file\npoem &lt;- read.csv(temp)\n\n# First ten rows\nhead(poem, 10)\n\n\n       id                            title       author grade\n1  104987                   I Love The Zoo                  1\n2   67185                The scary forest.                  1\n3  103555                 A Hike At School 1st grade-wh     1\n4  112483                         Computer            a     1\n5   74516                            Angel          aab     1\n6  114693         Nature Nature and Nature       aadhya     1\n7   46453                             Jack      aaliyah     1\n8   57397         When I awoke one morning        aanna     1\n9   77201 My Blue Berries and  My Cherries      aarathi     1\n10  40520                      A snowy day          ab.     1\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text\n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                  roses are red,  violets are blue.   i love the zoo.   do you?\n2                                                                                                                                                                                                                                                                                                                                                                                                                                         the forest is really haunted.  i believe it to be so.  but then we are going camping. \n3                                                                                                                                                                                            i took a hike at school today  and this is what i saw     bouncing balls      girls chatting against the walls     kids climbing on monkey bars     i even saw some teachers' cars     the wind was blowing my hair in my face     i saw a mud puddle,  but just a trace all of these things i noticed just now on my little hike. \n4                                                                                                                                                                                                                                                                                                                                                                                                                       you  can  do  what  you  want  you  can play a  game    you can do many things,   you can read and write\n5                                                                                                                                                                                                                                                                                                                                                              angel oh angle you spin like a top angel oh angel you will never stop can't you feel the air  as it blows through your hair angel oh angel itisto bad your a mop!\n6  look at the sun, what a beautiful day.  under the trees, we can run and play.  beauty of nature, we love to see,  from tiny insect to exotic tree.  it is a place to sit and think,  nature and human share the deepest link.  nature has ocean, which is in motion.  nature has tree, nature has river.  if we destroy the nature we would never be free.  our nature keeps us alive,  we must protect it, for society to thrive.  we spoil the nature, we spoil the future.  go along with nature, for your better future. \n7                                                                                                                                                                                                                                                                                                                                                                                                                                                     dog  playful,  energetic running,  jumping,  tackling my is my friend jack\n8                                                                                                                                                                                       when i awoke one morning,  a dog was on my  head.  i asked ,  ''what are you doing there?' it looked at me and said  ''woof!'' ''wouldn't you like to be outside playing?''said the man ''i'm staying here and playing here. '' said the dog he played all night and day. he came inside his new house and played inside a wet wet day. \n9                                                                                                                                                                                                                                                                                                                                                                                  i went to my blue berry tree they were no blue berries found i went to another tree to get some more free but found none but cherries round. \n10                                                                                                                                                                                                                                                                                                                      one snowy day the children went outside to play in the snow.  they threw snowballs,  went sledding and made a snowman.  afterwards they went inside to drink warm hot chocolate.  it was a fun snowy day\n   char\n1    62\n2    87\n3   324\n4   106\n5   164\n6   491\n7    74\n8   325\n9   143\n10  199\n\n\n\n\nCode\n# Checks dimensions\ndim(poem)\n\n\n[1] 61508     6\n\n\nWe want the poems themselves, so we’ll use the column text for tokenization."
  },
  {
    "objectID": "Tutorial7.html#tokenization-and-vectorization",
    "href": "Tutorial7.html#tokenization-and-vectorization",
    "title": "Week7_WordEmbeddings/Text Representation II",
    "section": "Tokenization and Vectorization",
    "text": "Tokenization and Vectorization\nThe process for text2vec is different than the standard process we’d been following. To that end, we’ll follow the same process as we will do for LDA later, creating a tokenized iterator and vectorized vocabulary first. This time, there’s no need to lowercase our words since the downloaded dataset is already lowercased.\nLet’s tokenize the data:\n\n\nCode\n# Tokenization\ntokens &lt;- word_tokenizer(poem$text)\n\n# First five rows tokenized\nhead(tokens, 5)\n\n\n[[1]]\n [1] \"roses\"   \"are\"     \"red\"     \"violets\" \"are\"     \"blue\"    \"i\"      \n [8] \"love\"    \"the\"     \"zoo\"     \"do\"      \"you\"    \n\n[[2]]\n [1] \"the\"     \"forest\"  \"is\"      \"really\"  \"haunted\" \"i\"       \"believe\"\n [8] \"it\"      \"to\"      \"be\"      \"so\"      \"but\"     \"then\"    \"we\"     \n[15] \"are\"     \"going\"   \"camping\"\n\n[[3]]\n [1] \"i\"        \"took\"     \"a\"        \"hike\"     \"at\"       \"school\"  \n [7] \"today\"    \"and\"      \"this\"     \"is\"       \"what\"     \"i\"       \n[13] \"saw\"      \"bouncing\" \"balls\"    \"girls\"    \"chatting\" \"against\" \n[19] \"the\"      \"walls\"    \"kids\"     \"climbing\" \"on\"       \"monkey\"  \n[25] \"bars\"     \"i\"        \"even\"     \"saw\"      \"some\"     \"teachers\"\n[31] \"cars\"     \"the\"      \"wind\"     \"was\"      \"blowing\"  \"my\"      \n[37] \"hair\"     \"in\"       \"my\"       \"face\"     \"i\"        \"saw\"     \n[43] \"a\"        \"mud\"      \"puddle\"   \"but\"      \"just\"     \"a\"       \n[49] \"trace\"    \"all\"      \"of\"       \"these\"    \"things\"   \"i\"       \n[55] \"noticed\"  \"just\"     \"now\"      \"on\"       \"my\"       \"little\"  \n[61] \"hike\"    \n\n[[4]]\n [1] \"you\"    \"can\"    \"do\"     \"what\"   \"you\"    \"want\"   \"you\"    \"can\"   \n [9] \"play\"   \"a\"      \"game\"   \"you\"    \"can\"    \"do\"     \"many\"   \"things\"\n[17] \"you\"    \"can\"    \"read\"   \"and\"    \"write\" \n\n[[5]]\n [1] \"angel\"   \"oh\"      \"angle\"   \"you\"     \"spin\"    \"like\"    \"a\"      \n [8] \"top\"     \"angel\"   \"oh\"      \"angel\"   \"you\"     \"will\"    \"never\"  \n[15] \"stop\"    \"can't\"   \"you\"     \"feel\"    \"the\"     \"air\"     \"as\"     \n[22] \"it\"      \"blows\"   \"through\" \"your\"    \"hair\"    \"angel\"   \"oh\"     \n[29] \"angel\"   \"itisto\"  \"bad\"     \"your\"    \"a\"       \"mop\"    \n\n\nCreate an iterator object:\n\n\nCode\n# Create iterator object\nit &lt;- itoken(tokens, progressbar = FALSE)\n\n\nBuild the vocabulary:\n\n\nCode\n# Build vocabulary\nvocab &lt;- create_vocabulary(it)\n\n# Vocabulary\nvocab\n\n\nNumber of docs: 61508 \n0 stopwords:  ... \nngram_min = 1; ngram_max = 1 \nVocabulary: \n          term term_count doc_count\n    1:    0000          1         1\n    2: 0000000          1         1\n    3: 0000001          1         1\n    4:   00a:m          1         1\n    5:    00he          1         1\n   ---                             \n56470:      to      69175     30347\n56471:     and      80863     34798\n56472:       a      92765     37607\n56473:     the     120677     37676\n56474:       i     124832     32777\n\n\n\n\nCode\n# Check dimensions\ndim(vocab)\n\n\n[1] 56474     3\n\n\nAnd prune and vectorize it. We’ll keep the terms that occur at least 5 times.\n\n\nCode\n# Prune vocabulary\nvocab &lt;- prune_vocabulary(vocab, term_count_min = 5)\n\n# Check dimensions\ndim(vocab)\n\n\n[1] 14267     3\n\n\nCode\n# Vectorize\nvectorizer &lt;- vocab_vectorizer(vocab)\n\n\nAs we can see, pruning our vocabulary deleted over 40 thousand words. I want to reiterate that this is a very small corpus from the perspective of traditional word embedding models. When we are working with word representations trained with these smaller corpora, we should be really cautious in our approach.\nMoving on, we can create out term-co-occurence matrix (TCM). We can achieve different results by experimenting with the skip_grams_window and other parameters. The definition of whether two words occur together is arbitrary, so we definitely want to play around with the parameters to see the different results.\n\n\nCode\n# use window of 5 for context words\ntcm &lt;- create_tcm(it, vectorizer, skip_grams_window = 5L)"
  },
  {
    "objectID": "Tutorial7.html#creating-and-fitting-the-glove-model",
    "href": "Tutorial7.html#creating-and-fitting-the-glove-model",
    "title": "Week7_WordEmbeddings/Text Representation II",
    "section": "Creating and fitting the GloVe model",
    "text": "Creating and fitting the GloVe model\nNow we have a TCM matrix and can factorize it via the GloVe algorithm. We will use the method $new to GlobalVectors to create our GloVe model. Here is documentation for related functions and methods.\n\n\nCode\n# Creating new GloVe model\nglove &lt;- GlobalVectors$new(rank = 50, x_max = 10)\n\n# Checking GloVe methods\nglove\n\n\n&lt;GloVe&gt;\n  Public:\n    bias_i: NULL\n    bias_j: NULL\n    clone: function (deep = FALSE) \n    components: NULL\n    fit_transform: function (x, n_iter = 10L, convergence_tol = -1, n_threads = getOption(\"rsparse_omp_threads\", \n    get_history: function () \n    initialize: function (rank, x_max, learning_rate = 0.15, alpha = 0.75, lambda = 0, \n    shuffle: FALSE\n  Private:\n    alpha: 0.75\n    b_i: NULL\n    b_j: NULL\n    cost_history: \n    fitted: FALSE\n    glove_fitter: NULL\n    initial: NULL\n    lambda: 0\n    learning_rate: 0.15\n    rank: 50\n    w_i: NULL\n    w_j: NULL\n    x_max: 10\n\n\nYou’ll be able to access the public methods. We can fit our modelusing $fit_transform to our glove variable. This may take several minutes to fit.\n\n\nCode\n# Fitting model\nwv_main &lt;- glove$fit_transform(tcm, n_iter= 10, \n                               convergence_tol = 0.01,\n                               n_threads = 8)\n\n\nINFO  [12:32:18.189] epoch 1, loss 0.1917\nINFO  [12:32:22.061] epoch 2, loss 0.1286\nINFO  [12:32:25.766] epoch 3, loss 0.1114\nINFO  [12:32:29.481] epoch 4, loss 0.1007\nINFO  [12:32:33.198] epoch 5, loss 0.0934\nINFO  [12:32:36.844] epoch 6, loss 0.0880\nINFO  [12:32:40.536] epoch 7, loss 0.0840\nINFO  [12:32:44.214] epoch 8, loss 0.0809\nINFO  [12:32:47.863] epoch 9, loss 0.0784\nINFO  [12:32:51.544] epoch 10, loss 0.0763\n\n\n\n\nCode\n# Checking dimensions\ndim(wv_main)\n\n\n[1] 14267    50\n\n\nNote that model learns two sets of word vectors–target and context. We can think of our word of interest as the target in this environment, and all the other words as the context inside the window. For both, word vectors are learned.\n\n\nCode\nwv_context &lt;- glove$components\ndim(wv_context)\n\n\n[1]    50 14267\n\n\nWhile both of word-vectors matrices can be used as result, the creators recommends to average or take a sum of main and context vector:\n\n\nCode\nword_vectors &lt;- wv_main + t(wv_context)\n\n\nHere’s a preview of the word vector matrix:\n\n\nCode\n#word_vectors"
  },
  {
    "objectID": "Tutorial7.html#cosine-similarity",
    "href": "Tutorial7.html#cosine-similarity",
    "title": "Week7_WordEmbeddings/Text Representation II",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nnow we can begin to play. Similarly to standard correlation, we can look at comparing two vectors using cosine similarity. Let’s see what is similar with ‘school’:\n\n\nCode\n# Word vector for school\nschool &lt;- word_vectors[\"school\", , drop = FALSE]\n\n# Cosine similarity\nschool_cos_sim &lt;- sim2(x = word_vectors, y = school, \n                       method = \"cosine\", norm = \"l2\")\n\n\nObviously, school is the most similar to school. Bawed on the poems that the children wrote, we can also see words like ‘work’, ‘fun’, and ‘home’ as most similar to ‘school.’\n\nPet example\nLet’s try our pet example:\n\n\nCode\n# cat - meow + bark should equal dog\ndog &lt;- word_vectors[\"cat\", , drop = FALSE] - \n  word_vectors[\"meow\", , drop = FALSE] +\n  word_vectors[\"bark\", , drop = FALSE]\n\n# Calculates pairwise similarities between the rows of two matrices\ndog_cos_sim &lt;- sim2(x = word_vectors, y = dog,\n                    method = \"cosine\", norm = \"l2\")\n\n# Top five predictions\nhead(sort(dog_cos_sim[,1], decreasing = TRUE), 5)\n\n\n      cat       dog       fat       rat       bat \n0.8091306 0.7992906 0.7492566 0.7097701 0.6616895 \n\n\nSuccess - Our predicted result was correct! We get ‘dog’ as the highest predicted result after the one we used (cat). We can think of this scenario as cats say meow and dogs say bark.\n\n\nParent example\nLet’s move on to the parent example:\n\n\nCode\n# mom - girl + boy should equal dad\ndad &lt;- word_vectors[\"mom\", , drop = FALSE] -\n  word_vectors[\"girl\", , drop = FALSE] +\n  word_vectors[\"boy\", , drop = FALSE]\n\n# Calculates pairwise similarities between the rows of two matrices\ndad_cos_sim &lt;- sim2(x = word_vectors, y = dad,\n                    method = \"cosine\", norm = \"l2\")\n\n# Top five predictions\nhead(sort(dad_cos_sim[,1], decreasing = TRUE), 5)\n\n\n      mom       dad   brother      says      said \n0.8579713 0.7287467 0.6729384 0.6591880 0.6396901 \n\n\n‘Dad’ wasn’t a top result. Finally, let’s try the infamous king and queen example.\n\n\nKing and queen example\n\n\nCode\n# king - man + woman should equal queen\nqueen &lt;- word_vectors[\"king\", , drop = FALSE] -\n  word_vectors[\"man\", , drop = FALSE] +\n  word_vectors[\"woman\", , drop = FALSE]\n\n# Calculate pairwise similarities\nqueen_cos_sim = sim2(x = word_vectors, y = queen, method = \"cosine\", norm = \"l2\")\n\n# Top five predictions\nhead(sort(queen_cos_sim[,1], decreasing = TRUE), 5)\n\n\n     king      kong     queen    martin    luther \n0.7679007 0.6551131 0.6077763 0.5833556 0.5622679 \n\n\nUnfortunately, we did not get queen as a top result. Let’s try changing man and woman to boy and girl to account for the kid’s writing.\n\n\nCode\n# king - boy + girl should equal queen\nqueen &lt;- word_vectors[\"king\", , drop = FALSE] -\n  word_vectors[\"boy\", , drop = FALSE] +\n  word_vectors[\"girl\", , drop = FALSE]\n\n# Calculate pairwise similarities\nqueen_cos_sim = sim2(x = word_vectors, y = queen, method = \"cosine\", norm = \"l2\")\n\n# Top five predictions\nhead(sort(queen_cos_sim[,1], decreasing = TRUE), 5)\n\n\n     king     queen      kong      girl      lady \n0.8526942 0.6396244 0.6247337 0.5408133 0.5361080 \n\n\nIt worked!\nAs we can see through, outcomes are highly dependent on the data and settings you select, so bear in mind the context when trying this out."
  },
  {
    "objectID": "Tutorial7.html#school-example",
    "href": "Tutorial7.html#school-example",
    "title": "Tutorial7_WordEmbeddings/Text Representation II",
    "section": "School example",
    "text": "School example\nNow we can begin to play. Similarly to standard correlation, we can look at comparing two vectors using cosine similarity. Let’s see what is similar with ‘school’:\n\n# Word vector for school\nschool &lt;- word_vectors[\"school\", , drop = FALSE]\n\n# Cosine similarity\nschool_cos_sim &lt;- sim2(x = word_vectors, y = school, \n                       method = \"cosine\", norm = \"l2\")\n\nhead(sort(school_cos_sim[,1], decreasing = TRUE), 10)\n\n   school       fun     today      work      home      time       day     class \n1.0000000 0.7419605 0.7162850 0.6896554 0.6698857 0.6665340 0.6532286 0.6456534 \n      get        to \n0.6448730 0.6371606 \n\n\nObviously, school is the most similar to school. Based on the poems that the children wrote, we can also see words like ‘work’, ‘fun’, and ‘class’ as most similar to ‘school.’\nWe can also calculate the cosine similarity between ‘school’ and ‘study’.\n\n# Word vector for homework\nstudy &lt;- word_vectors[\"study\", , drop = FALSE]\n\n# Cosine similarity\nsim2(x = school, y = study, \n      method = \"cosine\", norm = \"l2\")\n\n           study\nschool 0.2997516\n\n\nIn practice, you can write a loop to find the similarity between the keyword and a list of words. For example, calculating the similarity between ‘school’ and ‘study’, ‘learn’, ‘homework’, ‘lunch’, ‘fun’, ‘friends’.\n\n# List of words to compare\nwords &lt;- c(\"study\", \"learn\",\"homework\", \"lunch\", \"fun\", \"friends\")\n\n# Loop through each word and calculate cosine similarity\nfor (i in words) {\n  list_vector &lt;- word_vectors[i, , drop = FALSE]\n  similarity &lt;- sim2(x = school, y = list_vector, \n                     method = \"cosine\", norm = \"l2\")\n  print(paste(\"Cosine similarity between 'school' and\", i, \":\", round(similarity, 3)))\n}\n\n[1] \"Cosine similarity between 'school' and study : 0.3\"\n[1] \"Cosine similarity between 'school' and learn : 0.576\"\n[1] \"Cosine similarity between 'school' and homework : 0.571\"\n[1] \"Cosine similarity between 'school' and lunch : 0.534\"\n[1] \"Cosine similarity between 'school' and fun : 0.742\"\n[1] \"Cosine similarity between 'school' and friends : 0.537\""
  },
  {
    "objectID": "Tutorial7.html#pet-example",
    "href": "Tutorial7.html#pet-example",
    "title": "Tutorial7_WordEmbeddings/Text Representation II",
    "section": "Pet example",
    "text": "Pet example\nLet’s try our pet example:\n\n# cat - meow + bark should equal dog\ndog &lt;- word_vectors[\"cat\", , drop = FALSE] - \n  word_vectors[\"meow\", , drop = FALSE] +\n  word_vectors[\"bark\", , drop = FALSE]\n\n# Calculates pairwise similarities between the rows of two matrices\ndog_cos_sim &lt;- sim2(x = word_vectors, y = dog,\n                    method = \"cosine\", norm = \"l2\")\n\n# Top five predictions\nhead(sort(dog_cos_sim[,1], decreasing = TRUE), 5)\n\n      dog       cat        he       fat     small \n0.7848847 0.7563225 0.6984845 0.6597557 0.6459102 \n\n\nSuccess - Our predicted result was correct! We get ‘dog’ as the highest predicted result after the one we used (cat). We can think of this scenario as cats say meow and dogs say bark."
  },
  {
    "objectID": "Tutorial7.html#parent-example",
    "href": "Tutorial7.html#parent-example",
    "title": "Tutorial7_WordEmbeddings/Text Representation II",
    "section": "Parent example",
    "text": "Parent example\nLet’s move on to the parent example:\n\n# mom - girl + boy should equal dad\ndad &lt;- word_vectors[\"mom\", , drop = FALSE] -\n  word_vectors[\"girl\", , drop = FALSE] +\n  word_vectors[\"boy\", , drop = FALSE]\n\n# Calculates pairwise similarities between the rows of two matrices\ndad_cos_sim &lt;- sim2(x = word_vectors, y = dad,\n                    method = \"cosine\", norm = \"l2\")\n\n# Top five predictions\nhead(sort(dad_cos_sim[,1], decreasing = TRUE), 5)\n\n      mom       dad   brother      said        my \n0.8582012 0.8006066 0.7208358 0.6569252 0.6191766 \n\n\n‘Dad’ wasn’t a top result. Finally, let’s try the infamous king and queen example."
  },
  {
    "objectID": "Tutorial7.html#king-and-queen-example",
    "href": "Tutorial7.html#king-and-queen-example",
    "title": "Tutorial7_WordEmbeddings/Text Representation II",
    "section": "King and queen example",
    "text": "King and queen example\n\n# king - man + woman should equal queen\nqueen &lt;- word_vectors[\"king\", , drop = FALSE] -\n  word_vectors[\"man\", , drop = FALSE] +\n  word_vectors[\"woman\", , drop = FALSE]\n\n# Calculate pairwise similarities\nqueen_cos_sim = sim2(x = word_vectors, y = queen, method = \"cosine\", norm = \"l2\")\n\n# Top five predictions\nhead(sort(queen_cos_sim[,1], decreasing = TRUE), 5)\n\n     king      kong    martin    luther        jr \n0.7792882 0.6638567 0.6130658 0.6078372 0.5452142 \n\n\nUnfortunately, we did not get queen as a top result. Let’s try changing man and woman to boy and girl to account for the kid’s writing.\n\n# king - boy + girl should equal queen\nqueen &lt;- word_vectors[\"king\", , drop = FALSE] -\n  word_vectors[\"boy\", , drop = FALSE] +\n  word_vectors[\"girl\", , drop = FALSE]\n\n# Calculate pairwise similarities\nqueen_cos_sim = sim2(x = word_vectors, y = queen, method = \"cosine\", norm = \"l2\")\n\n# Top five predictions\nhead(sort(queen_cos_sim[,1], decreasing = TRUE), 5)\n\n     king     queen    valley    luther  american \n0.8217269 0.6072339 0.5400702 0.5378605 0.5050342 \n\n\nIt worked!\nAs we can see through, outcomes are highly dependent on the data and settings you select, so bear in mind the context when trying this out."
  },
  {
    "objectID": "Tutorial8.html",
    "href": "Tutorial8.html",
    "title": "Tutorial8_Dictionary/Sentiment Analysis",
    "section": "",
    "text": "In this tutorial, we’ll learn about dictionary methods."
  },
  {
    "objectID": "Tutorial8.html#tidytext-package",
    "href": "Tutorial8.html#tidytext-package",
    "title": "Tutorial8_Dictionary/Sentiment Analysis",
    "section": "Tidytext package",
    "text": "Tidytext package\n\nafinn\nAFINN is a list of words rated for valence with an integer between -5 (negative) and 5 (positive). Each word is exclusively assigned to a value.\n\nafinn_dict &lt;- get_sentiments(\"afinn\")\n\nwords &lt;- afinn_dict %&gt;%\n group_by(value) %&gt;%\n table() \n\nhead(words,10)\n\n            value\nword         -5 -4 -3 -2 -1 0 1 2 3 4 5\n  abandon     0  0  0  1  0 0 0 0 0 0 0\n  abandoned   0  0  0  1  0 0 0 0 0 0 0\n  abandons    0  0  0  1  0 0 0 0 0 0 0\n  abducted    0  0  0  1  0 0 0 0 0 0 0\n  abduction   0  0  0  1  0 0 0 0 0 0 0\n  abductions  0  0  0  1  0 0 0 0 0 0 0\n  abhor       0  0  1  0  0 0 0 0 0 0 0\n  abhorred    0  0  1  0  0 0 0 0 0 0 0\n  abhorrent   0  0  1  0  0 0 0 0 0 0 0\n  abhors      0  0  1  0  0 0 0 0 0 0 0\n\ndim(afinn_dict)\n\n[1] 2477    2\n\nggplot(afinn_dict) +\n geom_histogram(aes(x=value), stat = \"count\") +\n theme_bw()\n\nWarning in geom_histogram(aes(x = value), stat = \"count\"): Ignoring unknown\nparameters: `binwidth`, `bins`, and `pad`\n\n\n\n\n\n\n\nloughran\nLoughran-McDonald sentiment lexicon is for use with financial documents. It labels words with six sentiments important in financial contexts: constraining, litigious, negative, positive, superfluous, and uncertainty. A word may belong to multiple sentiments.\n\nloughran_dict &lt;- get_sentiments(\"loughran\")\n\nwords &lt;- loughran_dict %&gt;%\n group_by(sentiment) %&gt;%\n table() \n\nhead(words,10)\n\n              sentiment\nword           constraining litigious negative positive superfluous uncertainty\n  abandon                 0         0        1        0           0           0\n  abandoned               0         0        1        0           0           0\n  abandoning              0         0        1        0           0           0\n  abandonment             0         0        1        0           0           0\n  abandonments            0         0        1        0           0           0\n  abandons                0         0        1        0           0           0\n  abdicated               0         0        1        0           0           0\n  abdicates               0         0        1        0           0           0\n  abdicating              0         0        1        0           0           0\n  abdication              0         0        1        0           0           0\n\ndim(loughran_dict)\n\n[1] 4150    2\n\nggplot(loughran_dict) +\n geom_histogram(aes(x=sentiment), stat = \"count\") +\n theme_bw()\n\nWarning in geom_histogram(aes(x = sentiment), stat = \"count\"): Ignoring unknown\nparameters: `binwidth`, `bins`, and `pad`\n\n\n\n\n\n\n\nbing\nEnglish sentiment lexicon that categorizes words into a binary fashion, either positive or negative.\n\nbing_dict &lt;- get_sentiments(\"bing\")\n\nwords &lt;- bing_dict %&gt;%\n group_by(sentiment) %&gt;%\n table() \n\nhead(words,10)\n\n             sentiment\nword          negative positive\n  2-faces            1        0\n  abnormal           1        0\n  abolish            1        0\n  abominable         1        0\n  abominably         1        0\n  abominate          1        0\n  abomination        1        0\n  abort              1        0\n  aborted            1        0\n  aborts             1        0\n\ndim(bing_dict)\n\n[1] 6786    2\n\nggplot(bing_dict) +\n geom_histogram(aes(x=sentiment), stat = \"count\") +\n theme_bw()\n\nWarning in geom_histogram(aes(x = sentiment), stat = \"count\"): Ignoring unknown\nparameters: `binwidth`, `bins`, and `pad`\n\n\n\n\n\n\n\nnrc\nCategorize words into eight emotions: anger, fear, anticipation, trust, surprise, sadness, joy, and disgust; and two sentiments: negative and positive. One word can belong to multiple categories.\n\nnrc_dict &lt;- get_sentiments(\"nrc\")\n\nwords &lt;- nrc_dict %&gt;%\n group_by(sentiment) %&gt;%\n table() \n\nhead(words,10)\n\n             sentiment\nword          anger anticipation disgust fear joy negative positive sadness\n  abacus          0            0       0    0   0        0        0       0\n  abandon         0            0       0    1   0        1        0       1\n  abandoned       1            0       0    1   0        1        0       1\n  abandonment     1            0       0    1   0        1        0       1\n  abba            0            0       0    0   0        0        1       0\n  abbot           0            0       0    0   0        0        0       0\n  abduction       0            0       0    1   0        1        0       1\n  aberrant        0            0       0    0   0        1        0       0\n  aberration      0            0       1    0   0        1        0       0\n  abhor           1            0       1    1   0        1        0       0\n             sentiment\nword          surprise trust\n  abacus             0     1\n  abandon            0     0\n  abandoned          0     0\n  abandonment        1     0\n  abba               0     0\n  abbot              0     1\n  abduction          1     0\n  aberrant           0     0\n  aberration         0     0\n  abhor              0     0\n\ndim(nrc_dict)\n\n[1] 13872     2\n\nggplot(nrc_dict) +\n geom_histogram(aes(x=sentiment), stat = \"count\") +\n theme_bw()\n\nWarning in geom_histogram(aes(x = sentiment), stat = \"count\"): Ignoring unknown\nparameters: `binwidth`, `bins`, and `pad`"
  },
  {
    "objectID": "Tutorial8.html#quanteda-package",
    "href": "Tutorial8.html#quanteda-package",
    "title": "Tutorial8_Dictionary/Sentiment Analysis",
    "section": "Quanteda package",
    "text": "Quanteda package\nDictionaries available under Quanteda: ANEW (Affective Norms for English Words), AFINN, LSD (Lexicoder Sentiment Dictionary), Loughran McDonald, etc.\n\nnames(data_dictionary_ANEW)\n\n[1] \"pleasure\"  \"arousal\"   \"dominance\"\n\nnames(data_dictionary_LoughranMcDonald)\n\n[1] \"NEGATIVE\"           \"POSITIVE\"           \"UNCERTAINTY\"       \n[4] \"LITIGIOUS\"          \"CONSTRAINING\"       \"SUPERFLUOUS\"       \n[7] \"INTERESTING\"        \"MODAL WORDS STRONG\" \"MODAL WORDS WEAK\"  \n\nnames(data_dictionary_LSD2015)\n\n[1] \"negative\"     \"positive\"     \"neg_positive\" \"neg_negative\""
  },
  {
    "objectID": "Tutorial9.html",
    "href": "Tutorial9.html",
    "title": "Tutorial9_Supervised Learning",
    "section": "",
    "text": "In this notebook, we’ll learn about supervised learning models.\n\nFront-end matters\nLet’s load up our packages. There’s one change from prior tutorials here: we’re adding in quanteda.textmodels. Note that this isn’t strictly necessary; many of the models we’ll think about actually work with other packages or the quanteda code is based on those other packages. But we know by using quanteda.textmodels package that the data and the models are going to place nice together.\n\n#install.packages(\"tidytext\")\n#install.packages(\"plyr\")\n#install.packages(\"tidyverse\")\n#install.packages(\"quanteda\")\n#install.packages(\"quanteda.textmodels\")\n\n# load libraries\nlibrary(tidytext)\nlibrary(plyr)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.2     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.1\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::arrange()   masks plyr::arrange()\n✖ purrr::compact()   masks plyr::compact()\n✖ dplyr::count()     masks plyr::count()\n✖ dplyr::desc()      masks plyr::desc()\n✖ dplyr::failwith()  masks plyr::failwith()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::id()        masks plyr::id()\n✖ dplyr::lag()       masks stats::lag()\n✖ dplyr::mutate()    masks plyr::mutate()\n✖ dplyr::rename()    masks plyr::rename()\n✖ dplyr::summarise() masks plyr::summarise()\n✖ dplyr::summarize() masks plyr::summarize()\n\nlibrary(quanteda)\n\nPackage version: 3.3.1\nUnicode version: 14.0\nICU version: 70.1\nParallel computing: 4 of 4 threads used.\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\n\n\n#library(devtools)\n#devtools::install_github(\"kbenoit/quanteda.dictionaries\") \nlibrary(quanteda.dictionaries)\n\n\n\ncaret\nWe are also going to add the caret library for supervised learning models. caret offers a one-stop shop for a host of machine learning models, as well as some nice functionality for creating, fitting, and debugging supervised learning models. The latter part is particularly helpful for us today as we explore these models and how well they are performing.\n\n#install.packages(\"caret\")\nlibrary(caret)\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nFinally, we’ll follow up on last week’s session by working with the Maas et al. (2011) movie review data, which includes 50,000 movie reviews. You can learn more about the dataset here.\n\n# large movie review database of 50,000 movie reviews\nload(url(\"https://www.dropbox.com/s/sjdfmx8ggwfda5o/data_corpus_LMRD.rda?dl=1\"))\n\nAs a quick reminder, we have movie reviews, a polarity rating for each (positive or negative), and a rating (1 to 10) stored as metadata. Here’s what the metadata looks like. We are going to focus on polarity.\n\nsummary(docvars(data_corpus_LMRD))\n\n   docnumber         rating          set        polarity   \n Min.   :    0   Min.   : 1.000   test :25000   neg:25000  \n 1st Qu.: 3125   1st Qu.: 2.000   train:25000   pos:25000  \n Median : 6250   Median : 5.500                            \n Mean   : 6250   Mean   : 5.495                            \n 3rd Qu.: 9374   3rd Qu.: 9.000                            \n Max.   :12499   Max.   :10.000                            \n\n\n\n\nTraining & Testing Data\nFor supervised learning models, the critical first step is splitting our data into testing and training sets. We’re going to go one step further and also create an “invisible” held-out set that we can come back to at the end to evaluate the conclusions we draw from training and testing our models.\nThe big thing to remember as you set about doing this is to ensure you set the random seed. Otherwise, you won’t be able to replicate the random splits of the data that you are about to create.\n\n# set seed\nset.seed(12345)\n\n# create id variable in corpus metadata\ndocvars(data_corpus_LMRD, \"id\") &lt;- 1:ndoc(data_corpus_LMRD)\n\n# create training set (60% of data) and initial test set\nN &lt;- ndoc(data_corpus_LMRD)\ntrainIndex &lt;- sample(1:N, .6 * N)\ntestIndex &lt;- c(1:N)[-trainIndex]\n\n# split test set in half (so 20% of data are test, 20% of data are held-out)\nN &lt;- length(testIndex)\nheldOutIndex &lt;- sample(1:N, .5 * N)\ntestIndex &lt;- testIndex[-heldOutIndex]\n\n# now apply indices to create subsets and dfms\ndfmTrain &lt;- corpus_subset(data_corpus_LMRD, id %in% trainIndex) %&gt;%\n  tokens() %&gt;%\n  dfm()\n\ndfmTest &lt;- corpus_subset(data_corpus_LMRD, id %in% testIndex) %&gt;%\n  tokens() %&gt;% dfm()\n\ndfmHeldOut &lt;- corpus_subset(data_corpus_LMRD, id %in% heldOutIndex) %&gt;% tokens() %&gt;% dfm()\n\n\n\nNaïve Bayes\nOnce nice feature of quanteda is that a host of the workhorse supervised learning (and, as we’ll see, text scaling) models come pre-packaged with the download and work directly with the document-feature matrices we are creating. Because of that, we can turn quickly into supervised learning once our data are all set. We’ll start with a Naive Bayes model.\n\npolarity_NaiveBayes &lt;- textmodel_nb(dfmTrain, docvars(dfmTrain, \"polarity\"), distribution = \"Bernoulli\")\nsummary(polarity_NaiveBayes)\n\n\nCall:\ntextmodel_nb.dfm(x = dfmTrain, y = docvars(dfmTrain, \"polarity\"), \n    distribution = \"Bernoulli\")\n\nClass Priors:\n(showing first 2 elements)\nneg pos \n0.5 0.5 \n\nEstimated Feature Scores:\n       once  again      mr      .   costner    has  dragged    out      a\nneg 0.07127 0.1210 0.03317 0.9961 0.0008675 0.3834 0.006473 0.4337 0.9653\npos 0.08869 0.1318 0.03975 0.9956 0.0007324 0.4171 0.003396 0.4036 0.9644\n     movie    for     far  longer   than necessary   aside   from    the\nneg 0.6583 0.7116 0.11205 0.01788 0.2952  0.009877 0.02022 0.4619 0.9914\npos 0.5526 0.7046 0.08729 0.01944 0.2611  0.013316 0.01658 0.4643 0.9897\n    terrific      sea   rescue sequences      ,     of  which  there    are\nneg 0.006139 0.009476 0.009142   0.02496 0.9651 0.9483 0.2974 0.4208 0.5524\npos 0.023970 0.009188 0.006725   0.02390 0.9572 0.9508 0.2951 0.3476 0.5566\n      very    few      i\nneg 0.3025 0.1397 0.7953\npos 0.3886 0.1266 0.7371\n\n\nNow we want to know how well the trained classifier performed. To do so, we need to retain only the words in our testing data that also appear in the training data. To do that, we can use the dfm_match() function from quanteda, which only retains terms that appear in both corpora.\n\ndfmTestMatched &lt;- dfm_match(dfmTest, features = featnames(dfmTrain))\n\nNow let’s apply our model to the testing data and see how well it performs.\n\n# install.packages(\"e1071\")\nlibrary(e1071)\n\n\n# create a confusion matrix\nactual &lt;- docvars(dfmTestMatched, \"polarity\")\npredicted &lt;- predict(polarity_NaiveBayes, newdata = dfmTestMatched)\nconfusion &lt;- table(predicted,actual)\n\n\n# now calculate a number of statistics related to the confusion matrix\nconfusionMatrix(confusion, mode = \"everything\")\n\nConfusion Matrix and Statistics\n\n         actual\npredicted  neg  pos\n      neg 4351  714\n      pos  658 4277\n                                          \n               Accuracy : 0.8628          \n                 95% CI : (0.8559, 0.8695)\n    No Information Rate : 0.5009          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.7256          \n                                          \n Mcnemar's Test P-Value : 0.1376          \n                                          \n            Sensitivity : 0.8686          \n            Specificity : 0.8569          \n         Pos Pred Value : 0.8590          \n         Neg Pred Value : 0.8667          \n              Precision : 0.8590          \n                 Recall : 0.8686          \n                     F1 : 0.8638          \n             Prevalence : 0.5009          \n         Detection Rate : 0.4351          \n   Detection Prevalence : 0.5065          \n      Balanced Accuracy : 0.8628          \n                                          \n       'Positive' Class : neg             \n                                          \n\n\nThis is pretty good. We’ve got pretty balanced data in our testing subset, so accuracy is a pretty strong indicator for how well we are doing. Here, we’re at 86%, with a 95% confidence interval of 85.6 to 86.9%.\nLet’s look more closely at these predictions. Above we pulled just the classification (positive or negative) but we can also look at the probability of classification.\n\npredicted_prob &lt;- predict(polarity_NaiveBayes, newdata = dfmTestMatched,\n                         type = \"probability\")\nhead(predicted_prob)\n\n                              neg          pos\ntest/neg/10002_3.txt 9.999997e-01 2.973793e-07\ntest/neg/10003_3.txt 1.223449e-07 9.999999e-01\ntest/neg/10004_2.txt 9.999903e-01 9.691954e-06\ntest/neg/10007_4.txt 9.991544e-01 8.456109e-04\ntest/neg/1000_3.txt  9.865396e-01 1.346043e-02\ntest/neg/10025_2.txt 1.000000e+00 2.369531e-22\n\nsummary(predicted_prob)\n\n      neg                 pos           \n Min.   :0.0000000   Min.   :0.0000000  \n 1st Qu.:0.0000303   1st Qu.:0.0000063  \n Median :0.5783196   Median :0.4216804  \n Mean   :0.5065766   Mean   :0.4934234  \n 3rd Qu.:0.9999937   3rd Qu.:0.9999697  \n Max.   :1.0000000   Max.   :1.0000000  \n\n\nYou might be able to notice that the classifier is really confident most of the time. In fact, it might be too confident. Let’s look at some of the classified examples.\n\n# The most positive review\nmostPos &lt;- sort.list(predicted_prob[,1], dec = F)[1]\ntexts(corpus_subset(data_corpus_LMRD, id %in% testIndex))[mostPos]\n\nWarning: 'texts.corpus' is deprecated.\nUse 'as.character' instead.\nSee help(\"Deprecated\")\n\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         train/pos/3683_9.txt \n\"18 directors had the same task: tell stories of love set in Paris. Naturally, some of them turned out better than others, but the whole mosaic is pretty charming - besides, wouldn't it be boring if all of them had the same vision of love? Here's how I rank the segments (that might change on a second viewing):\\n\\n1. \\\"Quartier Latin\\\", by Gérard Depardieu\\n\\nOne of the greatest French actors ever directed my favourite segment, featuring the always stunning Gena Rowlands and Ben Gazzara. Witty and delightful.\\n\\n2. \\\"Tour Eiffel\\\", by Sylvain Chomet\\n\\nCute, visually stunning (thanks to the director of \\\"The Triplets of Belleville\\\") story of a little boy whose parents are mimes;\\n\\n3. \\\"Tuileries\\\", by Ethan and Joel Coen\\n\\nThe Coen Brothers + Steve Buscemi = Hilarious\\n\\n4. \\\"Parc Monceau\\\", by Alfonso Cuarón (\\\"Y Tu Mamá También\\\", \\\"Children of Men\\\"), feat. Nick Nolte and Ludivine Sagnier (funny);\\n\\n5. \\\"Place des Fêtes\\\", by Oliver Schmitz, feat. Seydou Boro and Aissa Maiga (touching);\\n\\n6. \\\"14th Arrondissement\\\", Alexander Payne's (\\\"Election\\\", \\\"About Schmidt\\\") wonderful look for the pathetic side of life is present here, feat. the underrated character actress Margo Martindale (Hilary Swank's mother in \\\"Million Dollar Baby\\\") as a lonely, middle-aged American woman on vacation;\\n\\n7. \\\"Faubourg Saint-Denis\\\", Tom Tykwer's (\\\"Run Lola Run\\\") frantic style works in the story of a young actress (Natalie Portman) and a blind guy (Melchior Beslon) who fall in love;\\n\\n8. \\\"Père-Lachaise\\\", by Wes Craven, feat. Emily Mortimer and Rufus Sewell (plus a curious cameo by Alexander Payne as...Oscar Wilde!);\\n\\n9. \\\"Loin du 16ème\\\", by Walter Salles and Daniela Thomas (simple but moving story from the talented Brazilian directors, feat. Catalina Sandino Moreno);\\n\\n10. \\\"Quartier des Enfants Rouges\\\", by Olivier Assayas (\\\"Clean\\\"), a sad story feat. the always fantastic Maggie Gyllenhaal;\\n\\n11. \\\"Le Marais\\\", by Gus Van Sant, feat. Gaspard Ulliel, Elias McConnell and Marianne Faithful (simple, but funny);\\n\\n12. \\\"Quartier de la Madeleine\\\", by Vincenzo Natali, feat. Elijah Wood and Olga Kurylenko;\\n\\n13. \\\"Quais de Seine\\\", by Gurinder Chadha;\\n\\n14. \\\"Place des Victoires\\\", by Nobuhiro Suwa, feat. Juliette Binoche and Willem Dafoe;\\n\\n15. \\\"Bastille\\\", by Isabel Coixet (fabulous director of the underrated \\\"My Life Without Me\\\"), feat. Miranda Richardson, Sergio Castellitto, Javier Cámara and Leonor Watling;\\n\\n16. \\\"Pigalle\\\", by Richard LaGravenese, feat. Bob Hoskins and Fanny Ardant;\\n\\n17. \\\"Montmartre\\\", by and with Bruno Podalydès;\\n\\n18. \\\"Porte de Choisy\\\", by Christopher Doyle, with Barbet Schroeder (mostly known as the director of \\\"Barfly\\\", \\\"Reversal of Fortune\\\" and \\\"Single White Female\\\").\\n\\nI could classify some segments as brilliant and others as average (or even slightly boring), but not a single of them is plain bad. On the whole, I give \\\"Paris, Je t'Aime\\\" an 8.5/10 and recommend it for what it is: a lovely mosaic about love and other things in between.\" \n\n\nThat’s definitely positive, but the most positive? I mean, they end with a 8.5/10. Why is it getting bumped up so high? Well, notice how long it is. If you have positive words used many times, then it increases the confidence of the Naive Bayes classifier. Let’s see if that’s true in the other direction too.\n\n# the most negative review\nmostNeg &lt;- sort.list(predicted_prob[,1], dec = T)[1]\ntexts(corpus_subset(data_corpus_LMRD, id %in% testIndex))[mostNeg]\n\nWarning: 'texts.corpus' is deprecated.\nUse 'as.character' instead.\nSee help(\"Deprecated\")\n\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            test/neg/10025_2.txt \n\"Oh yeah, this one is definitely a strong contender to win the questionable award of \\\"worst 80's slasher ever made\\\". \\\"The Prey\\\" has got everything you usually want to avoid in a horror flick: a routine, derivative plot that you've seen a thousand times before (and better), insufferable characters and terrible performances, a complete lack of gore and suspense, fuzzy photography and unoriginal locations and \\u0096 most irritating of all \\u0096 the largest amount of pointless padding footage you've ever encountered in your life (and that's not an exaggeration but a guarantee!). Apart from the seemingly endless amount of National Geographic stock footage, which I'll expand upon later, this film is shameless enough to include a complete banjo interlude (!) and two occasions where characters tell dillydally jokes that aren't even remotely funny! The set-up is as rudimentary as it gets, with the intro showing images of a devastating forest fire with OTT voice-over human screams. Fast forward nearly forty years later, when an elderly couple out camping in that same area get axe-whacked by something that breathes heavily off-screen. This ought to be enough information for you to derive that someone survived the fire all these years ago and remained prowling around ever since. Enter three intolerable twenty something couples heading up to the danger zone with exclusively sex on their minds, unaware of course they are sitting ducks for the stalking and panting killer. \\\"The Prey\\\" is an irredeemable boring film. Apparently it was shot in 1978 already, but nobody wanted to distribute it up until 1984 and it isn't too hard to see why. In case you would filter out all the content that is actually relevant, this would only be a short movie with a running time of 30 minutes; possibly even less. There's an unimaginably large of nature and wildlife footage, sometimes of animals that I think don't even live in that type of area, and they seem to go on forever. The only thing missing, in fact, is the typical National Geographic narration providing educational information regarding the animals' habits. Animals in their own natural biotope are undeniably nice to look at, but not in a supposedly vile and cheesy 80's slasher movie, for crying out loud. The last fifteen minutes are finally somewhat worthwhile, with some potent killing sequences and fine make-up effects on the monster (who turns out to be Lurch from \\\"The Addams Family\\\" movies), but still silliness overrules \\u0096 the scene with the vultures is too stupid \\u0096 and the final shot is just laugh-out-loud retarded. As mentioned above, \\\"The Prey\\\" easily makes my own personal list of worst 80's slashers, alongside \\\"Appointment with Fear\\\", \\\"Berserker\\\", \\\"Deadly Games\\\", \\\"Don't Go in the Woods\\\", \\\"Hollow Gate\\\", \\\"The Stay Awake\\\" and \\\"Curfew\\\".\" \n\n\nHoly toledo, that is a bad review. It’s pretty long as well, which again makes clear that the more words we have the more evidence the classifier has to put something in one (or the other) bin. That’s good, but perhaps we should also have a lot of confidence if the review just had three words: “Terrible, Horrible, Bad.” Anyhow, as a final inspection, let’s look at where the model is confused.\n\n# mixed in tone\nmixed &lt;- sort.list(abs(predicted_prob[,1] - .5), dec = F)[1]\npredicted_prob[mixed,]\n\n      neg       pos \n0.5001356 0.4998644 \n\ntexts(corpus_subset(data_corpus_LMRD, id %in% testIndex))[mixed]\n\nWarning: 'texts.corpus' is deprecated.\nUse 'as.character' instead.\nSee help(\"Deprecated\")\n\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       train/neg/11019_4.txt \n\"Fred Olen Ray is a lousy director, even as far as B movie directors go, but 'Haunting Fear' is probably one of his better films. Yes, it does butcher the great Poe story 'Premature Burial' and yes, it is badly paced and uneven throughout, but it is also pretty entertaining. Scream Queen Brinke Stevens is better than usual as a pretty, fragile housewife whose worthless husband (Jay Richardson) is plotting to do away with her because he needs money to pay off a gangster (played by Robert Quarry). Delia Sheppard, a veteran of many early 90s soft-core movies, actually gives the best performance in the film as a slutty mistress. You will also enjoy small roles played by Karen Black as a psychic, Robert Clarke as a doctor and Michael Berryman in a nice cameo in one of the better scenes. The ending didn't make much sense!\" \n\n\nThis is a great example of how these models can struggle. A lot of the words here relate to the dark content of the film, but could as easily be used to describe a bad film. Likewise, descriptions of the director literally mix positive and negative elements (a lousy director doing good work). But while we can read this and see clearly that this relates to content, the classifier can’t make that distinction.\nFinally, let’s look at a review the classifier got very wrong.\n\n# find a review with high confidence\nveryPos &lt;- sort.list(predicted_prob[1:2500, 1], dec = F)[1]\npredicted_prob[veryPos,]\n\n         neg          pos \n3.597661e-19 1.000000e+00 \n\ntexts(corpus_subset(data_corpus_LMRD, id %in% testIndex))[veryPos]\n\nWarning: 'texts.corpus' is deprecated.\nUse 'as.character' instead.\nSee help(\"Deprecated\")\n\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              test/neg/9244_4.txt \n\"Korine's established himself, by now, as a talented and impressive image-maker. The promotional posters for Mister Lonely all include the film's most impressive compositions (though there's one in particular I've yet to see in promo material: that of a blue-clad nun teasing a dog with a stick, surrounded by green forest with torrential rain pouring down). The opening images of this film, of Michael Jackson lookalike (Diego Luna) riding a small motorbike round a track, is strangely compelling and beautiful: Roy Orbison's \\\"Mister Lonely\\\" plays on the soundtrack, and the images unfold in slow-motion. There's also a funny and terrific sequence in which the same character mimes a dance, without music (though a radio sits like a silent dog next to him), in the middle of a Paris street; Korine splices in sound effects and jump-cuts that evoke both a feeling of futility and dogged liberation in the character's dance routine.\\n\\nThe first instance of the segment dealing with the nuns is also strangely poignant; Father Umbrillo (Werner Herzog) is an autocratic priest about to fly with some nuns over, and drop food into, impoverished areas nearby. In a scene that is both light-hearted and affecting, Herzog must deal with a stubbornly enthusiastic local who wishes to make the plane trip with them in order to see his wife in San Francisco. As the exchange develops, Herzog draws out of the man a confession: he has sinned, and his frequent infidelity is the cause of his wife having left him in the first place. This scene, short and sweet, gains particular weight after one learns its improvised origins: the sinner is played by a non-actor who was on set when Korine and co. were filming - and his adulterous ways had given him, in real life, a lasting, overwhelming guilt.\\n\\nHenceforth, the film is hit-and-miss; a succession of intrinsically interesting moments that add to a frivolous, muddled narrative. Whereas Gummo and Julien Donkey-Boy maintain their aesthetic and emotional weight via coherent structural frameworks, Mister Lonely feels like a victim of editing room ruthlessness. A few scenes were cut from the film, which would have otherwise painted fuller pictures of certain characters, due to continuity errors in costume - a result, no doubt, due to the absence of a shooting script and Korine's tendency for improvisation. One deleted scene in particular - in which 'Charlie Chaplin' (Denis Lavant) and 'Madonna' (Melita Morgan) have sex - would have added much more emotional conflict to a scene later on in the film (I won't spoil it, but it's there to deflate any feeling of warmth or celebration, and, as it is, only half-succeeds).\\n\\nThe two strands of the narrative, unconnected literally, are best approached as two entirely different stories with the same allegorical meaning; one compliments the other and vice versa. (It's something to do with the conflict between one's ambitions and the reality of the current situation.) But there's not enough of the Herzog scenes to merit their place in the film, and so any connection between these two allegorically-connected threads is inevitably strained - and the inclusion is, in retrospect, tedious.\\n\\nThis is an ambitious step forward from Julien Donkey-Boy that suffers mostly, at least in the lookalike segments, from having far too many characters for the film's running length, a flaw that would have been even worse had big star names played everyone (as was originally planned).\\n\\nWith many of the imagery's self-contained beauty, and moments of real, genuine connection with the soundtrack, this feels like it'd be much more suited to an art installation or photo exhibition. As an exploration of mimesis and the nature of impersonation, it'd lose none of its power - indeed, for me, it would perhaps be more impressive. The loneliness attached to iconic performativity (such as that encountered by both the icons themselves and those who aspire to be like them) is well-captured in images such as that wherein 'Marilyn Monroe' (a gorgeous Samantha Morton) seduces the camera with a Seven Year Itch pose in the middle of a forest, or when 'Sammy Davis, Jr.' (Jason Pennycooke) settles, post-dance rehearsal, with his back to the camera overlooking an incredible, tranquil lake.\\n\\nAs it is, moments like these, and all those where the titles of randomly-chosen Michael Jackson songs crawl across the scene, are married to one another in a film narrative far less affecting than it should be.\\n\\n(For those who see it, I lost all faith during the egg-singing scene, late on. You'll know which scene I mean because it sticks out like a sore thumb, as some sort of gimmicky attempt at the new cinematic language for which Korine has previously been hailed.)\" \n\n\nAh, brutal. Long, with all of the concomitant problems, and then a mix of descriptions about things the person really liked and criticism.\n\n\nSupport Vector Machines\nLet’s try out a different approach. Support Vector Machines (or SVMs) offers a more robust approach. However, it’s also much more computationally expensive. To make it tractable, we’re going to shrink the size of our training set down to a much smaller set.\n\n# set seed\nset.seed(919919)\n\n# sample smaller set of training data\nnewTrainIndex &lt;- trainIndex[sample(1:length(trainIndex), 2000)]\n\n# create small DFM\ndfmTrainSmall &lt;- corpus_subset(data_corpus_LMRD, id %in% newTrainIndex) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_select(pattern = stopwords(\"en\"), selection = \"remove\") %&gt;%\n  dfm()\n\n# trim the dfm down to frequent terms\ndfmTrainSmall &lt;- dfm_trim(dfmTrainSmall, min_docfreq = 20, min_termfreq = 20)\n\ndim(dfmTrainSmall)\n\n[1] 2000 1786\n\n# run model\npolarity_SVM &lt;- textmodel_svm(dfmTrainSmall, docvars(dfmTrainSmall, \"polarity\"))\n\nJust as we needed to do above, we’ll shrink down our test set to make this tractable. Then, we’ll evaluate the out-of-sample prediction.\n\n# update test set\ndfmTestMatchedSmall &lt;- dfm_match(dfmTest, features = featnames(dfmTrainSmall))\n\n# create a confusion matrix\nactual &lt;- docvars(dfmTestMatchedSmall, \"polarity\")\npredicted &lt;- predict(polarity_SVM, newdata = dfmTestMatchedSmall)\nconfusion &lt;- table(predicted,actual)\n\n# now calculate a number of statistics related to the confusion matrix\nconfusionMatrix(confusion, mode = \"everything\")\n\nConfusion Matrix and Statistics\n\n         actual\npredicted  neg  pos\n      neg 3886  906\n      pos 1123 4085\n                                          \n               Accuracy : 0.7971          \n                 95% CI : (0.7891, 0.8049)\n    No Information Rate : 0.5009          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.5942          \n                                          \n Mcnemar's Test P-Value : 1.625e-06       \n                                          \n            Sensitivity : 0.7758          \n            Specificity : 0.8185          \n         Pos Pred Value : 0.8109          \n         Neg Pred Value : 0.7844          \n              Precision : 0.8109          \n                 Recall : 0.7758          \n                     F1 : 0.7930          \n             Prevalence : 0.5009          \n         Detection Rate : 0.3886          \n   Detection Prevalence : 0.4792          \n      Balanced Accuracy : 0.7971          \n                                          \n       'Positive' Class : neg             \n                                          \n\n\nA little bit worse, but recall that we are using a lot fewer features than we had previously. Note also that we’ve done no tuning of the svm classifier.\nWe can also check the most positive and negative features to see whether the results make sense.\n\nsvmCoefs &lt;- as.data.frame(t(coefficients(polarity_SVM)))\nsvmCoefs &lt;- svmCoefs %&gt;% arrange(V1)\nhead(svmCoefs, 20)\n\n                    V1\nfunniest    -0.6397358\njourney     -0.5721073\ngreatest    -0.5708527\nexcellent   -0.5700547\nviewing     -0.5285494\nwind        -0.5238649\nloved       -0.5217147\nenjoyed     -0.5206025\ntells       -0.5172011\nrole        -0.4941843\nagree       -0.4941304\ndrugs       -0.4893199\nsuperb      -0.4708421\nbest        -0.4627656\nfantastic   -0.4556445\nreferences  -0.4525804\npower       -0.4448412\nfascinating -0.4386401\nfavorite    -0.4380767\nputting     -0.4303005\n\ntail(svmCoefs, 20)\n\n                     V1\nhours         0.3957591\nbecoming      0.3974458\n$             0.3976305\neffort        0.4017995\nspeaking      0.4039423\ndull          0.4115639\nturkey        0.4135784\nturned        0.4314134\nfocus         0.4407301\nterrible      0.4648218\nawful         0.4664541\ntheater       0.4758042\nloves         0.4822824\nneither       0.4830725\ndisappointing 0.5117394\nminutes       0.5193968\nstupid        0.5298348\npeter         0.5621249\nworst         0.6325081\nfails         0.6445418\n\n\nThese generally look sensible, though there’s definitely something weird going on with both “positive” for negative reviews.\n\n\nRandom Forests\nLet’s try out a Random Forest classifier. Random Forests are even more computationally intensive than SVMs; they also aren’t available with quanteda, so we’ll need to convert our DFMs to a different format for this analysis.\n\n#install.packages(\"randomForest\")\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\n\ndfmTrainSmallRf &lt;- convert(dfmTrainSmall, to = \"matrix\")\ndfmTestMatchedSmallRf &lt;- convert(dfmTestMatchedSmall, to = \"matrix\")\n\n\nset.seed(444)\npolarity_RF &lt;- randomForest(dfmTrainSmallRf,\n                            y = as.factor(docvars(dfmTrainSmall)$polarity),\n                            xtest = dfmTestMatchedSmallRf,\n                            ytest = as.factor(docvars(dfmTestMatchedSmall)$polarity),\n                            importance = TRUE,\n                            mtry = 20,\n                            ntree = 100)\n\n\n# confusion matrix\nactual &lt;- as.factor(docvars(dfmTestMatchedSmall)$polarity)\npredicted &lt;- polarity_RF$test[['predicted']]\nconfusion &lt;- table(predicted,actual)\nconfusionMatrix(confusion, mode=\"everything\")\n\nConfusion Matrix and Statistics\n\n         actual\npredicted  neg  pos\n      neg 3997  703\n      pos 1012 4288\n                                         \n               Accuracy : 0.8285         \n                 95% CI : (0.821, 0.8358)\n    No Information Rate : 0.5009         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.657          \n                                         \n Mcnemar's Test P-Value : 1.027e-13      \n                                         \n            Sensitivity : 0.7980         \n            Specificity : 0.8591         \n         Pos Pred Value : 0.8504         \n         Neg Pred Value : 0.8091         \n              Precision : 0.8504         \n                 Recall : 0.7980         \n                     F1 : 0.8234         \n             Prevalence : 0.5009         \n         Detection Rate : 0.3997         \n   Detection Prevalence : 0.4700         \n      Balanced Accuracy : 0.8286         \n                                         \n       'Positive' Class : neg            \n                                         \n\n\nRight in the same zone as our SVM classifier, and again with a much smaller set of features. Let’s see what words are most informative: that is, which can best help us predict the label, though not which label the feature predicts.\n\nvarImpPlot(polarity_RF)\n\n\n\n\n\n\nEnsembles\nWe’ve got three classifiers so far: polarity_NB, polarity_SVM, and polarity_RF. All three of our classifiers were able to classify in test sets at greater than 80% accuracy, but digging in made it evident that each had some pretty significant room for improvement.\nOne way we might try to get at that would be to ensemble their classifications. That is, we could create an aggregate measure from the individual classifications to see if the wisdom of the crowd can get us a bit closer to the actual values. Let’s try it first with a really simple approach: if 2 or more of the classifiers identify a case as positive, we’ll use that classification.\n\n# create a vector of only \"negative\" values\npredicted_class &lt;- rep(\"neg\", length(actual))\n\n# create a vector that is equal to the sum of \"pos\" predictions for each observation\nnum_predicted &lt;- 1 * (predict(polarity_NaiveBayes, newdata = dfmTestMatched) == \"pos\") + \n          1 * (predict(polarity_SVM, newdata = dfmTestMatchedSmall) == \"pos\") + \n          1 * (polarity_RF$test[['predicted']] == \"pos\") \n\n# update the predicted class vector \npredicted_class[num_predicted &gt; 1] &lt;- \"pos\"\n\n# create the confusion matrix\nconfusion &lt;- table(predicted_class, actual)\nconfusionMatrix(confusion, mode = \"everything\")\n\nConfusion Matrix and Statistics\n\n               actual\npredicted_class  neg  pos\n            neg 4216  626\n            pos  793 4365\n                                          \n               Accuracy : 0.8581          \n                 95% CI : (0.8511, 0.8649)\n    No Information Rate : 0.5009          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7162          \n                                          \n Mcnemar's Test P-Value : 1.049e-05       \n                                          \n            Sensitivity : 0.8417          \n            Specificity : 0.8746          \n         Pos Pred Value : 0.8707          \n         Neg Pred Value : 0.8463          \n              Precision : 0.8707          \n                 Recall : 0.8417          \n                     F1 : 0.8560          \n             Prevalence : 0.5009          \n         Detection Rate : 0.4216          \n   Detection Prevalence : 0.4842          \n      Balanced Accuracy : 0.8581          \n                                          \n       'Positive' Class : neg             \n                                          \n\n\nHm, not doing much better than just the Naive Bayes model.\n\n\nThe Held-Out Set\nNow that we’ve gone through all of these analyses and settled on our approach, it’s a useful time to remember that we set aside 20% of our data waaaay back at the start. Let’s try to evaluate our conclusion – that the Naive Bayes classifier using all of the available features works best — holds up with that old held-out set.\n\n# pull the actual classifications\nactual &lt;- docvars(dfmHeldOut)$polarity\n\n# Naive Bayes\ndfmHeldOutMatched &lt;- dfm_match(dfmHeldOut, features = featnames(dfmTrain))\npredicted.nb &lt;- predict(polarity_NaiveBayes, dfmHeldOutMatched)\nconfusion &lt;- table(predicted.nb, actual)\nconfusionMatrix(confusion, mode = \"everything\")\n\nConfusion Matrix and Statistics\n\n            actual\npredicted.nb  neg  pos\n         neg 5686  443\n         pos  569 3302\n                                          \n               Accuracy : 0.8988          \n                 95% CI : (0.8927, 0.9046)\n    No Information Rate : 0.6255          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7854          \n                                          \n Mcnemar's Test P-Value : 8.518e-05       \n                                          \n            Sensitivity : 0.9090          \n            Specificity : 0.8817          \n         Pos Pred Value : 0.9277          \n         Neg Pred Value : 0.8530          \n              Precision : 0.9277          \n                 Recall : 0.9090          \n                     F1 : 0.9183          \n             Prevalence : 0.6255          \n         Detection Rate : 0.5686          \n   Detection Prevalence : 0.6129          \n      Balanced Accuracy : 0.8954          \n                                          \n       'Positive' Class : neg             \n                                          \n\n\nIt also shows a high accuracy!"
  },
  {
    "objectID": "Tutorial10.html",
    "href": "Tutorial10.html",
    "title": "Tutorial10_Topic Models",
    "section": "",
    "text": "In this tutorial we’ll learn about K-Means and topic models of two different types, the regular vanilla LDA version, and structural topic models."
  },
  {
    "objectID": "Tutorial10.html#introduction",
    "href": "Tutorial10.html#introduction",
    "title": "Tutorial10_Topic Models",
    "section": "Introduction",
    "text": "Introduction\nK-means clustering is one of the simplest and popular unsupervised machine learning algorithms. The objective of K-means is: group similar data points together and discover underlying patterns. To achieve this objective, K-means looks for a fixed number (k) of clusters in a dataset.\nIn this tutorial, we are going to cluster a dataset consisting of health news tweets. These short sentences belong to one of the 16 sources of news considered in the dataset. We are then facing a multi-label classifying problem, with k = 16.\n\ntruth.K &lt;- 16"
  },
  {
    "objectID": "Tutorial10.html#front-end-matters",
    "href": "Tutorial10.html#front-end-matters",
    "title": "Tutorial10_Topic Models",
    "section": "Front-end Matters",
    "text": "Front-end Matters\nFirst, let’s looad the tm package.\n\nlibrary(tm)\n\nLoading required package: NLP\n\n\nWe download the data from the UCI Machine Learning Repository.\n\n# creating the empty dataset with the formatted columns\ndataframe &lt;- data.frame(ID = character(),\n                        datetime = character(),\n                        content = character(),\n                        label = factor())\nsource.url &lt;- 'https://archive.ics.uci.edu/ml/machine-learning-databases/00438/Health-News-Tweets.zip'\n\ntarget.directory &lt;- '/tmp/clustering-r'\ntemporary.file &lt;- tempfile()\ndownload.file(source.url, temporary.file)\nunzip(temporary.file, exdir = target.directory)\n\n# Reading the files\ntarget.directory &lt;- paste(target.directory, 'Health-Tweets', sep=\"/\")\nfiles &lt;- list.files(path = target.directory, pattern = '.txt$')\n\n# filling the dataframe by reading the text content\nfor (f in files){\n  news.filename = paste(target.directory, f, sep = \"/\")\n  news.label &lt;- substr(f, 0, nchar(f) - 4) # removing the 4 last characters (.txt)\n  news.data &lt;- read.csv(news.filename,\n                        encoding = \"UTF-8\",\n                        header = FALSE,\n                        quote = \"\",\n                        sep = \"|\",\n                        col.names = c(\"ID\", \"datetime\", \"content\"))\n  \n  # Trick to ignore last part of tweets which content contains the split character \"|\"\n  # no satisfying solution has been found to split and merging extra-columns with the last one\n  news.data &lt;- news.data[news.data$content != \"\", ]\n  news.data['label'] = news.label # we add the label of the tweet\n  \n  # only considering a little portion of data\n  # because handling sparse matrix for generic usage is a pain\n  news.data &lt;- head(news.data, floor(nrow(news.data) * 0.05))\n  dataframe &lt;- rbind(dataframe, news.data)\n  \n}\n# deleting the temporary directory\nunlink(target.directory, recursive = TRUE)"
  },
  {
    "objectID": "Tutorial10.html#preprocessing",
    "href": "Tutorial10.html#preprocessing",
    "title": "Tutorial10_Topic Models",
    "section": "Preprocessing",
    "text": "Preprocessing\nRemoving urls in the tweets\n\nsentences &lt;- sub(\"http://([[:alnum:]|[:punct:]])+\", '', dataframe$content)\nhead(sentences)\n\n[1] \"Breast cancer risk test devised \"     \n[2] \"GP workload harming care - BMA poll \" \n[3] \"Short people's 'heart risk greater' \" \n[4] \"New approach against HIV 'promising' \"\n[5] \"Coalition 'undermined NHS' - doctors \"\n[6] \"Review of case against NHS manager \"  \n\n\nFor common preprocessing problems, we are going to use tm package.\n\ncorpus &lt;- tm::Corpus(tm::VectorSource(sentences))\n# cleaning up\n# handling utf-8 encoding problem from the dataset\ncorpus.cleaned &lt;- tm::tm_map(corpus, function(x) iconv(x, to = 'UTF-8-MAC', sub = 'byte'))\n\nWarning in tm_map.SimpleCorpus(corpus, function(x) iconv(x, to = \"UTF-8-MAC\", :\ntransformation drops documents\n\ncorpus.cleaned &lt;- tm::tm_map(corpus.cleaned, tm::removeWords, tm::stopwords('english'))\n\nWarning in tm_map.SimpleCorpus(corpus.cleaned, tm::removeWords,\ntm::stopwords(\"english\")): transformation drops documents\n\ncorpus.cleaned &lt;- tm::tm_map(corpus.cleaned, tm::stripWhitespace)\n\nWarning in tm_map.SimpleCorpus(corpus.cleaned, tm::stripWhitespace):\ntransformation drops documents"
  },
  {
    "objectID": "Tutorial10.html#text-representation",
    "href": "Tutorial10.html#text-representation",
    "title": "Tutorial10_Topic Models",
    "section": "Text Representation",
    "text": "Text Representation\nNow, we have a sequence of cleaned sentences that we can use to build our TF-IDF matrix. From this result, we will be able to execute every numerical processes that we want, such as clustering.\n\n# Building the feature matrices\ntfm &lt;- tm::DocumentTermMatrix(corpus.cleaned)\ndim(tfm)\n\n[1] 3159 9410\n\ntfm\n\n&lt;&lt;DocumentTermMatrix (documents: 3159, terms: 9410)&gt;&gt;\nNon-/sparse entries: 26424/29699766\nSparsity           : 100%\nMaximal term length: 62\nWeighting          : term frequency (tf)\n\ntfm.tfidf &lt;- tm::weightTfIdf(tfm)\ndim(tfm.tfidf)\n\n[1] 3159 9410\n\ntfm.tfidf\n\n&lt;&lt;DocumentTermMatrix (documents: 3159, terms: 9410)&gt;&gt;\nNon-/sparse entries: 26424/29699766\nSparsity           : 100%\nMaximal term length: 62\nWeighting          : term frequency - inverse document frequency (normalized) (tf-idf)\n\n# we remove a lot of features. \ntfm.tfidf &lt;- tm::removeSparseTerms(tfm.tfidf, 0.999) # (data,allowed sparsity)\ntfidf.matrix &lt;- as.matrix(tfm.tfidf)\ndim(tfidf.matrix)\n\n[1] 3159 1327\n\n# cosine distance matrix (useful for specific clustering algorithms)\ndist.matrix = proxy::dist(tfidf.matrix, method = \"cosine\")"
  },
  {
    "objectID": "Tutorial10.html#running-the-clustering-algorithms",
    "href": "Tutorial10.html#running-the-clustering-algorithms",
    "title": "Tutorial10_Topic Models",
    "section": "Running the clustering algorithms",
    "text": "Running the clustering algorithms\n\nK-means\nDefine clusters so that the total within-cluster variation is minimized.\n\n\n\n\n\n\nNote\n\n\n\nHartigan-Wong algorithm (Hartigan and Wong 1979) defines the total within-cluster variation as the sum of squared Euclidean distances between items and the corresponding centroid:\n\\(W(C_{k}) = \\sum_{x_{i} \\in C_{k}}(x_{i} - \\mu_{k})^{2}\\)\n\n\\(x_{i}\\): a data point belonging to the cluster \\(C_{k}\\)\n\\(\\mu_{k}\\): the mean value of the points assigned to the cluster \\(C_{k}\\)\n\nTotal within-cluster variation as follows:\ntotal withinness = \\(\\sum^{k}_{k=1}W(C_{k}) = \\sum^{k}_{k=1} \\sum_{x_{i} \\in C_{k}} (x_{i} - \\mu_{k})^{2}\\)\nThe total within-cluster sum of square measures the goodness of the clustering and we want it to be as small as possible.\n\n\n\nclustering.kmeans &lt;- kmeans(tfidf.matrix, truth.K)\nnames(clustering.kmeans)\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\nHierarchical clustering\nDefine a clustering criterion and the pointwise distance matrix. Let’s use the Ward’s methods as the clustering criterion.\n\nclustering.hierarchical &lt;- hclust(dist.matrix, method = \"ward.D2\")\nnames(clustering.hierarchical)\n\n[1] \"merge\"       \"height\"      \"order\"       \"labels\"      \"method\"     \n[6] \"call\"        \"dist.method\"\n\n\n\n\nPlotting\nTo plot the clustering results, as our feature spaces is highly dimensional (TF-IDF representation), we will reduce it to 2 thanks to multi-dimensional scaling. This technique is dependent of our distance metric, but in our case with TF-IDF.\n\npoints &lt;- cmdscale(dist.matrix, k = 2) # running the PCA \npalette &lt;- colorspace::diverge_hcl(truth.K) # creating a color palette\nprevious.par &lt;- par(mfrow = c(1,2))# partitioning the plot space\n\nmaster.cluster &lt;- clustering.kmeans$cluster\nplot(points,\n     main = 'K-Means clustering',\n     col = as.factor(master.cluster),\n     mai = c(0, 0, 0, 0),\n     mar = c(0, 0, 0, 0),\n     xaxt = 'n', yaxt = 'n',\n     xlab = '', ylab = '')\n\nslave.hierarchical &lt;- cutree(clustering.hierarchical, k = truth.K)\nplot(points,\n     main = 'Hierarchical clustering',\n     col = as.factor(slave.hierarchical),\n     mai = c(0, 0, 0, 0),\n     mar = c(0, 0, 0, 0),\n     xaxt = 'n', yaxt = 'n',\n     xlab = '', ylab = '')\n\n\n\npar(previous.par) # recovering the original plot space parameters"
  },
  {
    "objectID": "Tutorial10.html#determining-k",
    "href": "Tutorial10.html#determining-k",
    "title": "Tutorial10_Topic Models",
    "section": "Determining K",
    "text": "Determining K\nIn the previous example, we know sentences belong to one of the 16 sources. Then how to decide the best number of clusters (K)?\nHere we use the “eblow” method. For each given number of clusters, we can calculate how much variance in the data can be explained by the clustering. Typically, this will increase with the number of clusters. However, the increase would slow down at a certain point and that’s where we choose the number of clusters.\n\nk &lt;- 16\nvarper &lt;- NULL\nfor(i in 1:k){\n  clustering.kmeans2 &lt;- kmeans(tfidf.matrix, i)\n  varper &lt;- c(varper, as.numeric(clustering.kmeans2$betweenss)/as.numeric(clustering.kmeans2$totss))\n}\n\nvarper\n\n [1] 3.842354e-12 5.560551e-03 1.916852e-02 2.421237e-02 2.373344e-02\n [6] 2.982853e-02 2.815610e-02 3.071530e-02 3.742710e-02 3.557937e-02\n[11] 4.460016e-02 4.271404e-02 4.308681e-02 4.716389e-02 4.749830e-02\n[16] 5.124432e-02\n\nplot(1:k, varper, xlab = \"# of clusters\", ylab = \"explained variance\")\n\n\n\n\nFrom the plot, after 3 clusters, the increase in the explained variance becomes slower - there is an elbow here. Therefore, we might use 3 clusters here."
  },
  {
    "objectID": "Tutorial10.html#introduction-1",
    "href": "Tutorial10.html#introduction-1",
    "title": "Tutorial10_Topic Models",
    "section": "Introduction",
    "text": "Introduction\nThe general idea with topic models is to identify the topics that characterize a set of documents. The background on this is interesting; a lot of the initial interest came from digital humanities and library science where you had the need to systematically organize the massive thematic content of the huge collections of texts. Importantly, LDA and STM, the two we’ll discuss this week, are both mixed-membership models, meaning documents are characterized as arising from a distribution over topics, rather than coming from a single topic."
  },
  {
    "objectID": "Tutorial10.html#latent-dirichlet-allocation",
    "href": "Tutorial10.html#latent-dirichlet-allocation",
    "title": "Tutorial10_Topic Models",
    "section": "Latent Dirichlet Allocation",
    "text": "Latent Dirichlet Allocation\nFor LDA, we will be using the text2vec package. It is an R package that provides an efficient framework for text analysis and NLP. It’s a fast implementation of word embedding models (which is where it gets it’s name from) but it also has really nice and fast functionality for LDA.\nAlgorithms may classify topics within a text set, and Latent Dirichlet Allocation (LDA) is one of the most popular algorithms for topic modeling. LDA uses two basic principles:\n\nEach document is made up of topics.\nEach word in a document can be attributed to a topic.\n\nLet’s begin!\n\nFront-end Matter\nFirst, let’s load the text2vec package:\n\nlibrary(text2vec)\n\nWe will be using the built in movie reviews dataset that comes with the package. It is labeled and can be called as “movie_review”. Let’s load it in:\n\n# Load in built-in dataset\ndata(\"movie_review\")\n\n# Prints first ten rows of the dtaset:\nhead(movie_review, 10)\n\n        id sentiment\n1   5814_8         1\n2   2381_9         1\n3   7759_3         0\n4   3630_4         0\n5   9495_8         1\n6   8196_8         1\n7   7166_2         0\n8  10633_1         0\n9    319_1         0\n10 8713_10         1\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        review\n1                                                                                                                                               With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.&lt;br /&gt;&lt;br /&gt;Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.&lt;br /&gt;&lt;br /&gt;The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.&lt;br /&gt;&lt;br /&gt;Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.&lt;br /&gt;&lt;br /&gt;Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\\\\"The Classic War of the Worlds\\\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\\\"critics\\\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\\\"critics\\\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\\\"critics\\\\\" perceive to be its shortcomings.\n3  The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like Jurassik Park, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature's most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.&lt;br /&gt;&lt;br /&gt;The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . And it packs a ridiculous final deadly scene. No for small kids by realistic,gory and violent attack scenes . Other films about Sabretooths or Smilodon are the following : Sabretooth(2002)by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better 10.000 BC(2006) by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle. This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films. Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ). Rating : Below average, bottom of barrel.\n4                                                                                                                                                                                                  It must be assumed that those who praised this film (\\\\\"the greatest filmed opera ever,\\\\\" didn't I read somewhere?) either don't care for opera, don't care for Wagner, or don't care about anything except their desire to appear Cultured. Either as a representation of Wagner's swan-song, or as a movie, this strikes me as an unmitigated disaster, with a leaden reading of the score matched to a tricksy, lugubrious realisation of the text.&lt;br /&gt;&lt;br /&gt;It's questionable that people with ideas as to what an opera (or, for that matter, a play, especially one by Shakespeare) is \\\\\"about\\\\\" should be allowed anywhere near a theatre or film studio; Syberberg, very fashionably, but without the smallest justification from Wagner's text, decided that Parsifal is \\\\\"about\\\\\" bisexual integration, so that the title character, in the latter stages, transmutes into a kind of beatnik babe, though one who continues to sing high tenor -- few if any of the actors in the film are the singers, and we get a double dose of Armin Jordan, the conductor, who is seen as the face (but not heard as the voice) of Amfortas, and also appears monstrously in double exposure as a kind of Batonzilla or Conductor Who Ate Monsalvat during the playing of the Good Friday music -- in which, by the way, the transcendant loveliness of nature is represented by a scattering of shopworn and flaccid crocuses stuck in ill-laid turf, an expedient which baffles me. In the theatre we sometimes have to piece out such imperfections with our thoughts, but I can't think why Syberberg couldn't splice in, for Parsifal and Gurnemanz, mountain pasture as lush as was provided for Julie Andrews in Sound of Music...&lt;br /&gt;&lt;br /&gt;The sound is hard to endure, the high voices and the trumpets in particular possessing an aural glare that adds another sort of fatigue to our impatience with the uninspired conducting and paralytic unfolding of the ritual. Someone in another review mentioned the 1951 Bayreuth recording, and Knappertsbusch, though his tempi are often very slow, had what Jordan altogether lacks, a sense of pulse, a feeling for the ebb and flow of the music -- and, after half a century, the orchestral sound in that set, in modern pressings, is still superior to this film.\n5                                                                                                                                                                                                                  Superbly trashy and wondrously unpretentious 80's exploitation, hooray! The pre-credits opening sequences somewhat give the false impression that we're dealing with a serious and harrowing drama, but you need not fear because barely ten minutes later we're up until our necks in nonsensical chainsaw battles, rough fist-fights, lurid dialogs and gratuitous nudity! Bo and Ingrid are two orphaned siblings with an unusually close and even slightly perverted relationship. Can you imagine playfully ripping off the towel that covers your sister's naked body and then stare at her unshaven genitals for several whole minutes? Well, Bo does that to his sister and, judging by her dubbed laughter, she doesn't mind at all. Sick, dude! Anyway, as kids they fled from Russia with their parents, but nasty soldiers brutally slaughtered mommy and daddy. A friendly smuggler took custody over them, however, and even raised and trained Bo and Ingrid into expert smugglers. When the actual plot lifts off, 20 years later, they're facing their ultimate quest as the mythical and incredibly valuable White Fire diamond is coincidentally found in a mine. Very few things in life ever made as little sense as the plot and narrative structure of \\\\\"White Fire\\\\\", but it sure is a lot of fun to watch. Most of the time you have no clue who's beating up who or for what cause (and I bet the actors understood even less) but whatever! The violence is magnificently grotesque and every single plot twist is pleasingly retarded. The script goes totally bonkers beyond repair when suddenly  and I won't reveal for what reason  Bo needs a replacement for Ingrid and Fred Williamson enters the scene with a big cigar in his mouth and his sleazy black fingers all over the local prostitutes. Bo's principal opponent is an Italian chick with big breasts but a hideous accent, the preposterous but catchy theme song plays at least a dozen times throughout the film, there's the obligatory \\\\\"we're-falling-in-love\\\\\" montage and loads of other attractions! My God, what a brilliant experience. The original French title translates itself as \\\\\"Life to Survive\\\\\", which is uniquely appropriate because it makes just as much sense as the rest of the movie: None!\n6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    I dont know why people think this is such a bad movie. Its got a pretty good plot, some good action, and the change of location for Harry does not hurt either. Sure some of its offensive and gratuitous but this is not the only movie like that. Eastwood is in good form as Dirty Harry, and I liked Pat Hingle in this movie as the small town cop. If you liked DIRTY HARRY, then you should see this one, its a lot better than THE DEAD POOL. 4/5\n7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                This movie could have been very good, but comes up way short. Cheesy special effects and so-so acting. I could have looked past that if the story wasn't so lousy. If there was more of a background story, it would have been better. The plot centers around an evil Druid witch who is linked to this woman who gets migraines. The movie drags on and on and never clearly explains anything, it just keeps plodding on. Christopher Walken has a part, but it is completely senseless, as is most of the movie. This movie had potential, but it looks like some really bad made for TV movie. I would avoid this movie.\n8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     I watched this video at a friend's house. I'm glad I did not waste money buying this one. The video cover has a scene from the 1975 movie Capricorn One. The movie starts out with several clips of rocket blow-ups, most not related to manned flight. Sibrel's smoking gun is a short video clip of the astronauts preparing a video broadcast. He edits in his own voice-over instead of letting us listen to what the crew had to say. The video curiously ends with a showing of the Zapruder film. His claims about radiation, shielding, star photography, and others lead me to believe is he extremely ignorant or has some sort of ax to grind against NASA, the astronauts, or American in general. His science is bad, and so is this video.\n9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           A friend of mine bought this film for 1, and even then it was grossly overpriced. Despite featuring big names such as Adam Sandler, Billy Bob Thornton and the incredibly talented Burt Young, this film was about as funny as taking a chisel and hammering it straight through your earhole. It uses tired, bottom of the barrel comedic techniques - consistently breaking the fourth wall as Sandler talks to the audience, and seemingly pointless montages of 'hot girls'.&lt;br /&gt;&lt;br /&gt;Adam Sandler plays a waiter on a cruise ship who wants to make it as a successful comedian in order to become successful with women. When the ship's resident comedian - the shamelessly named 'Dickie' due to his unfathomable success with the opposite gender - is presumed lost at sea, Sandler's character Shecker gets his big break. Dickie is not dead, he's rather locked in the bathroom, presumably sea sick.&lt;br /&gt;&lt;br /&gt;Perhaps from his mouth he just vomited the worst film of all time.\n10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         &lt;br /&gt;&lt;br /&gt;This movie is full of references. Like \\\\\"Mad Max II\\\\\", \\\\\"The wild one\\\\\" and many others. The ladybugs face its a clear reference (or tribute) to Peter Lorre. This movie is a masterpiece. Well talk much more about in the future.\n\n\n\n# checking dimensions of dataset\ndim(movie_review)\n\n[1] 5000    3\n\n\nThe dataset consists of 5000 movie reviews, each of which is marked as positive (1) or negative (0) in the ‘sentiment’ column.\nNow, we need to clean the data up a bit. To make our lives easier and limit the amount of processing power, let’s use the first 3000 reviews. They are located in the ‘review’ column.\n\n\nVectorization\nTexts can take up a lot of memory themselves, but vectorized texts typically do not. To represent documents in vector space, we first have to come to create mappings from terms to term IDs. We call them terms instead of words because they can be arbitrary n-grams not just single words. We represent a set of documents as a sparse matrix, where each row corresponds to a document and each column corresponds to a term. This can be done in two ways: using the vocabulary itself or by feature hashing.\n\nInformation gathered from text2vec creator..\n\nLet’s perform tokenization and lowercase each token:\n\n# creates string of combined lowercased words\ntokens &lt;- tolower(movie_review$review[1:3000])\n\n# performs tokenization\ntokens &lt;- word_tokenizer(tokens)\n\n# prints first two tokenized rows\nhead(tokens, 2)\n\n[[1]]\n  [1] \"with\"         \"all\"          \"this\"         \"stuff\"        \"going\"       \n  [6] \"down\"         \"at\"           \"the\"          \"moment\"       \"with\"        \n [11] \"mj\"           \"i've\"         \"started\"      \"listening\"    \"to\"          \n [16] \"his\"          \"music\"        \"watching\"     \"the\"          \"odd\"         \n [21] \"documentary\"  \"here\"         \"and\"          \"there\"        \"watched\"     \n [26] \"the\"          \"wiz\"          \"and\"          \"watched\"      \"moonwalker\"  \n [31] \"again\"        \"maybe\"        \"i\"            \"just\"         \"want\"        \n [36] \"to\"           \"get\"          \"a\"            \"certain\"      \"insight\"     \n [41] \"into\"         \"this\"         \"guy\"          \"who\"          \"i\"           \n [46] \"thought\"      \"was\"          \"really\"       \"cool\"         \"in\"          \n [51] \"the\"          \"eighties\"     \"just\"         \"to\"           \"maybe\"       \n [56] \"make\"         \"up\"           \"my\"           \"mind\"         \"whether\"     \n [61] \"he\"           \"is\"           \"guilty\"       \"or\"           \"innocent\"    \n [66] \"moonwalker\"   \"is\"           \"part\"         \"biography\"    \"part\"        \n [71] \"feature\"      \"film\"         \"which\"        \"i\"            \"remember\"    \n [76] \"going\"        \"to\"           \"see\"          \"at\"           \"the\"         \n [81] \"cinema\"       \"when\"         \"it\"           \"was\"          \"originally\"  \n [86] \"released\"     \"some\"         \"of\"           \"it\"           \"has\"         \n [91] \"subtle\"       \"messages\"     \"about\"        \"mj's\"         \"feeling\"     \n [96] \"towards\"      \"the\"          \"press\"        \"and\"          \"also\"        \n[101] \"the\"          \"obvious\"      \"message\"      \"of\"           \"drugs\"       \n[106] \"are\"          \"bad\"          \"m'kay\"        \"br\"           \"br\"          \n[111] \"visually\"     \"impressive\"   \"but\"          \"of\"           \"course\"      \n[116] \"this\"         \"is\"           \"all\"          \"about\"        \"michael\"     \n[121] \"jackson\"      \"so\"           \"unless\"       \"you\"          \"remotely\"    \n[126] \"like\"         \"mj\"           \"in\"           \"anyway\"       \"then\"        \n[131] \"you\"          \"are\"          \"going\"        \"to\"           \"hate\"        \n[136] \"this\"         \"and\"          \"find\"         \"it\"           \"boring\"      \n[141] \"some\"         \"may\"          \"call\"         \"mj\"           \"an\"          \n[146] \"egotist\"      \"for\"          \"consenting\"   \"to\"           \"the\"         \n[151] \"making\"       \"of\"           \"this\"         \"movie\"        \"but\"         \n[156] \"mj\"           \"and\"          \"most\"         \"of\"           \"his\"         \n[161] \"fans\"         \"would\"        \"say\"          \"that\"         \"he\"          \n[166] \"made\"         \"it\"           \"for\"          \"the\"          \"fans\"        \n[171] \"which\"        \"if\"           \"true\"         \"is\"           \"really\"      \n[176] \"nice\"         \"of\"           \"him\"          \"br\"           \"br\"          \n[181] \"the\"          \"actual\"       \"feature\"      \"film\"         \"bit\"         \n[186] \"when\"         \"it\"           \"finally\"      \"starts\"       \"is\"          \n[191] \"only\"         \"on\"           \"for\"          \"20\"           \"minutes\"     \n[196] \"or\"           \"so\"           \"excluding\"    \"the\"          \"smooth\"      \n[201] \"criminal\"     \"sequence\"     \"and\"          \"joe\"          \"pesci\"       \n[206] \"is\"           \"convincing\"   \"as\"           \"a\"            \"psychopathic\"\n[211] \"all\"          \"powerful\"     \"drug\"         \"lord\"         \"why\"         \n[216] \"he\"           \"wants\"        \"mj\"           \"dead\"         \"so\"          \n[221] \"bad\"          \"is\"           \"beyond\"       \"me\"           \"because\"     \n[226] \"mj\"           \"overheard\"    \"his\"          \"plans\"        \"nah\"         \n[231] \"joe\"          \"pesci's\"      \"character\"    \"ranted\"       \"that\"        \n[236] \"he\"           \"wanted\"       \"people\"       \"to\"           \"know\"        \n[241] \"it\"           \"is\"           \"he\"           \"who\"          \"is\"          \n[246] \"supplying\"    \"drugs\"        \"etc\"          \"so\"           \"i\"           \n[251] \"dunno\"        \"maybe\"        \"he\"           \"just\"         \"hates\"       \n[256] \"mj's\"         \"music\"        \"br\"           \"br\"           \"lots\"        \n[261] \"of\"           \"cool\"         \"things\"       \"in\"           \"this\"        \n[266] \"like\"         \"mj\"           \"turning\"      \"into\"         \"a\"           \n[271] \"car\"          \"and\"          \"a\"            \"robot\"        \"and\"         \n[276] \"the\"          \"whole\"        \"speed\"        \"demon\"        \"sequence\"    \n[281] \"also\"         \"the\"          \"director\"     \"must\"         \"have\"        \n[286] \"had\"          \"the\"          \"patience\"     \"of\"           \"a\"           \n[291] \"saint\"        \"when\"         \"it\"           \"came\"         \"to\"          \n[296] \"filming\"      \"the\"          \"kiddy\"        \"bad\"          \"sequence\"    \n[301] \"as\"           \"usually\"      \"directors\"    \"hate\"         \"working\"     \n[306] \"with\"         \"one\"          \"kid\"          \"let\"          \"alone\"       \n[311] \"a\"            \"whole\"        \"bunch\"        \"of\"           \"them\"        \n[316] \"performing\"   \"a\"            \"complex\"      \"dance\"        \"scene\"       \n[321] \"br\"           \"br\"           \"bottom\"       \"line\"         \"this\"        \n[326] \"movie\"        \"is\"           \"for\"          \"people\"       \"who\"         \n[331] \"like\"         \"mj\"           \"on\"           \"one\"          \"level\"       \n[336] \"or\"           \"another\"      \"which\"        \"i\"            \"think\"       \n[341] \"is\"           \"most\"         \"people\"       \"if\"           \"not\"         \n[346] \"then\"         \"stay\"         \"away\"         \"it\"           \"does\"        \n[351] \"try\"          \"and\"          \"give\"         \"off\"          \"a\"           \n[356] \"wholesome\"    \"message\"      \"and\"          \"ironically\"   \"mj's\"        \n[361] \"bestest\"      \"buddy\"        \"in\"           \"this\"         \"movie\"       \n[366] \"is\"           \"a\"            \"girl\"         \"michael\"      \"jackson\"     \n[371] \"is\"           \"truly\"        \"one\"          \"of\"           \"the\"         \n[376] \"most\"         \"talented\"     \"people\"       \"ever\"         \"to\"          \n[381] \"grace\"        \"this\"         \"planet\"       \"but\"          \"is\"          \n[386] \"he\"           \"guilty\"       \"well\"         \"with\"         \"all\"         \n[391] \"the\"          \"attention\"    \"i've\"         \"gave\"         \"this\"        \n[396] \"subject\"      \"hmmm\"         \"well\"         \"i\"            \"don't\"       \n[401] \"know\"         \"because\"      \"people\"       \"can\"          \"be\"          \n[406] \"different\"    \"behind\"       \"closed\"       \"doors\"        \"i\"           \n[411] \"know\"         \"this\"         \"for\"          \"a\"            \"fact\"        \n[416] \"he\"           \"is\"           \"either\"       \"an\"           \"extremely\"   \n[421] \"nice\"         \"but\"          \"stupid\"       \"guy\"          \"or\"          \n[426] \"one\"          \"of\"           \"the\"          \"most\"         \"sickest\"     \n[431] \"liars\"        \"i\"            \"hope\"         \"he\"           \"is\"          \n[436] \"not\"          \"the\"          \"latter\"      \n\n[[2]]\n  [1] \"the\"          \"classic\"      \"war\"          \"of\"           \"the\"         \n  [6] \"worlds\"       \"by\"           \"timothy\"      \"hines\"        \"is\"          \n [11] \"a\"            \"very\"         \"entertaining\" \"film\"         \"that\"        \n [16] \"obviously\"    \"goes\"         \"to\"           \"great\"        \"effort\"      \n [21] \"and\"          \"lengths\"      \"to\"           \"faithfully\"   \"recreate\"    \n [26] \"h\"            \"g\"            \"wells\"        \"classic\"      \"book\"        \n [31] \"mr\"           \"hines\"        \"succeeds\"     \"in\"           \"doing\"       \n [36] \"so\"           \"i\"            \"and\"          \"those\"        \"who\"         \n [41] \"watched\"      \"his\"          \"film\"         \"with\"         \"me\"          \n [46] \"appreciated\"  \"the\"          \"fact\"         \"that\"         \"it\"          \n [51] \"was\"          \"not\"          \"the\"          \"standard\"     \"predictable\" \n [56] \"hollywood\"    \"fare\"         \"that\"         \"comes\"        \"out\"         \n [61] \"every\"        \"year\"         \"e.g\"          \"the\"          \"spielberg\"   \n [66] \"version\"      \"with\"         \"tom\"          \"cruise\"       \"that\"        \n [71] \"had\"          \"only\"         \"the\"          \"slightest\"    \"resemblance\" \n [76] \"to\"           \"the\"          \"book\"         \"obviously\"    \"everyone\"    \n [81] \"looks\"        \"for\"          \"different\"    \"things\"       \"in\"          \n [86] \"a\"            \"movie\"        \"those\"        \"who\"          \"envision\"    \n [91] \"themselves\"   \"as\"           \"amateur\"      \"critics\"      \"look\"        \n [96] \"only\"         \"to\"           \"criticize\"    \"everything\"   \"they\"        \n[101] \"can\"          \"others\"       \"rate\"         \"a\"            \"movie\"       \n[106] \"on\"           \"more\"         \"important\"    \"bases\"        \"like\"        \n[111] \"being\"        \"entertained\"  \"which\"        \"is\"           \"why\"         \n[116] \"most\"         \"people\"       \"never\"        \"agree\"        \"with\"        \n[121] \"the\"          \"critics\"      \"we\"           \"enjoyed\"      \"the\"         \n[126] \"effort\"       \"mr\"           \"hines\"        \"put\"          \"into\"        \n[131] \"being\"        \"faithful\"     \"to\"           \"h.g\"          \"wells\"       \n[136] \"classic\"      \"novel\"        \"and\"          \"we\"           \"found\"       \n[141] \"it\"           \"to\"           \"be\"           \"very\"         \"entertaining\"\n[146] \"this\"         \"made\"         \"it\"           \"easy\"         \"to\"          \n[151] \"overlook\"     \"what\"         \"the\"          \"critics\"      \"perceive\"    \n[156] \"to\"           \"be\"           \"its\"          \"shortcomings\"\n\n\nNote that text2vec provides a few tokenizer functions (see ?tokenizers). These are just simple wrappers for the base::gsub() function and are not very fast or flexible. If you need something smarter or faster you can use the tokenizers package.\nWe can create an iterator over each token using itoken(). An iterator is an object that can be iterated upon, meaning that you can traverse through all the values. In our example, we’ll be able to traverse through each token for each row using our newly generated iterator, it. The general thing to note here is that this is a way to make the approach less memory intensive, something that will turn out to be helpful.\n\n# iterates over each token\nit &lt;- itoken(tokens, ids = movie_review$id[1:3000], progressbar = FALSE)\n\n# prints iterator\nit\n\n&lt;itoken&gt;\n  Inherits from: &lt;CallbackIterator&gt;\n  Public:\n    callback: function (x) \n    clone: function (deep = FALSE) \n    initialize: function (x, callback = identity) \n    is_complete: active binding\n    length: active binding\n    move_cursor: function () \n    nextElem: function () \n    x: GenericIterator, iterator, R6\n\n\n\n\nVocabulary-based Vectorization\nAs stated above, we represent our corpus as a document-feature matrix. The process for text2vec is much different than with quanteda, though the intuition is generally aligned. Effectively, the text2vec design is intended to be faster and more memory-efficient; the downside is that it’s a little more obtuse. The first step is to create our vocabulary for the DFM. That is simple since we have already created an iterator; all we need to do is place our iterator as an argument inside create_vocabulary().\n\n# built the vocabulary\nv &lt;- create_vocabulary(it)\n\n# print vocabulary\nv\n\nNumber of docs: 3000 \n0 stopwords:  ... \nngram_min = 1; ngram_max = 1 \nVocabulary: \n        term term_count doc_count\n    1:   0.3          1         1\n    2:  0.48          1         1\n    3:   0.5          1         1\n    4:  0.89          1         1\n    5: 00015          1         1\n   ---                           \n33487:    to      16370      2826\n33488:    of      17409      2829\n33489:   and      19761      2892\n33490:     a      19776      2910\n33491:   the      40246      2975\n\n\n\n# checking dimensions\ndim(v)\n\n[1] 33491     3\n\n\nWe can create stop words or prune our vocabulary with prune_vocabulary(). We will keep the terms that occur at least 10 times.\n\n# prunes vocabulary\nv &lt;- prune_vocabulary(v, term_count_min = 10, doc_proportion_max = 0.2)\n\n# check dimensions\ndim(v)\n\n[1] 5325    3\n\n\nIf we check the dimensions after pruning our vocabulary, we can see that we have less terms. We have removed the very common words so that our vocabulary can contain more high quality and meaningful words.\nBefore we can create our DFM, we’ll need to vectorize our vocabulary with vocab_vectorizer().\n\n# creates a closure that helps transform list of tokens into vector space\nvectorizer &lt;- vocab_vectorizer(v)\n\nWe now have everything we need to create a DFM. We can pass in our iterator of tokens, our vectorized vocabulary, and a type of matrix (either dgCMatrix or dgTMatrix) in create_dtm().\n\n# creates document term matrix\ndtm &lt;- create_dtm(it, vectorizer, type = \"dgTMatrix\")\n\nNow we can create our topic model after we have created our DTM. We create our model using LDA$new().\n\n# create new LDA model\nlda_model &lt;- LDA$new(n_topics = 10, doc_topic_prior = 0.1,\n                     topic_word_prior = 0.01)\n\n# print other methods for LDA\nlda_model\n\n&lt;WarpLDA&gt;\n  Inherits from: &lt;LDA&gt;\n  Public:\n    clone: function (deep = FALSE) \n    components: active binding\n    fit_transform: function (x, n_iter = 1000, convergence_tol = 0.001, n_check_convergence = 10, \n    get_top_words: function (n = 10, topic_number = 1L:private$n_topics, lambda = 1) \n    initialize: function (n_topics = 10L, doc_topic_prior = 50/n_topics, topic_word_prior = 1/n_topics, \n    plot: function (lambda.step = 0.1, reorder.topics = FALSE, doc_len = private$doc_len, \n    topic_word_distribution: active binding\n    transform: function (x, n_iter = 1000, convergence_tol = 0.001, n_check_convergence = 10, \n  Private:\n    calc_pseudo_loglikelihood: function (ptr = private$ptr) \n    check_convert_input: function (x) \n    components_: NULL\n    doc_len: NULL\n    doc_topic_distribution: function () \n    doc_topic_distribution_with_prior: function () \n    doc_topic_matrix: NULL\n    doc_topic_prior: 0.1\n    fit_transform_internal: function (model_ptr, n_iter, convergence_tol, n_check_convergence, \n    get_c_all: function () \n    get_c_all_local: function () \n    get_doc_topic_matrix: function (prt, nr) \n    get_topic_word_count: function () \n    init_model_dtm: function (x, ptr = private$ptr) \n    internal_matrix_formats: list\n    is_initialized: FALSE\n    n_iter_inference: 10\n    n_topics: 10\n    ptr: NULL\n    reset_c_local: function () \n    run_iter_doc: function (update_topics = TRUE, ptr = private$ptr) \n    run_iter_word: function (update_topics = TRUE, ptr = private$ptr) \n    seeds: 154317648.356281 1121893281.95516\n    set_c_all: function (x) \n    set_internal_matrix_formats: function (sparse = NULL, dense = NULL) \n    topic_word_distribution_with_prior: function () \n    topic_word_prior: 0.01\n    transform_internal: function (x, n_iter = 1000, convergence_tol = 0.001, n_check_convergence = 10, \n    vocabulary: NULL\n\n\nAfter printing lda_model, we can see there are other methods we can use with the model.\nNote: the only accessible methods are the ones under ‘Public’. Documentation for all methods and arguments are available here on page 22.\n\n\nFitting\nWe can fit our model with $fit_transform:\n\n# fitting model\ndoc_topic_distr &lt;- \n  lda_model$fit_transform(x = dtm, n_iter = 1000,\n                          convergence_tol = 0.001, n_check_convergence = 25,\n                          progressbar = FALSE)\n\nINFO  [11:52:41.420] early stopping at 225 iteration\nINFO  [11:52:45.972] early stopping at 50 iteration\n\n\nThe doc_topic_distr object is a matrix where each row is a document, each column is a topic, and the cell entry is the proportion of the document estimated to be of the topic. That is, each row is the topic attention distribution for a document.\nFor example, here’s the topic distribution for the very first document:\n\nbarplot(doc_topic_distr[1, ], xlab = \"topic\",\n        ylab = \"proportion\", ylim = c(0,1),\n        names.arg = 1:ncol(doc_topic_distr))\n\n\n\n\n\n\nDescribing Topics: Top Words\nWe can also use $get_top_words as a method to get the top words for each topic.\n\n# get top n words for topics 1, 5, and 10\nlda_model$get_top_words(n = 10, topic_number = c(1L, 5L, 10L),\n                        lambda = 1)\n\n      [,1]     [,2]          [,3]    \n [1,] \"ever\"   \"performance\" \"now\"   \n [2,] \"i'm\"    \"scenes\"      \"game\"  \n [3,] \"didn't\" \"director\"    \"old\"   \n [4,] \"know\"   \"makes\"       \"still\" \n [5,] \"worst\"  \"may\"         \"joe\"   \n [6,] \"say\"    \"new\"         \"he's\"  \n [7,] \"your\"   \"these\"       \"man\"   \n [8,] \"got\"    \"here\"        \"played\"\n [9,] \"funny\"  \"both\"        \"name\"  \n[10,] \"thing\"  \"main\"        \"star\"  \n\n\nAlso top-words could be stored by “relevance” which also takes into account frequency of word in the corpus (0 &lt; lambda &lt; 1).\nThe creator recommends setting lambda to be between 0.2 and 0.4. Here’s the difference compared to a lambda of 1:\n\nlda_model$get_top_words(n = 10, topic_number = c(1L, 5L, 10L),\n                        lambda = 0.2)\n\n      [,1]        [,2]        [,3]       \n [1,] \"awful\"     \"thriller\"  \"joe\"      \n [2,] \"stupid\"    \"brings\"    \"game\"     \n [3,] \"cheesy\"    \"che\"       \"cowboy\"   \n [4,] \"waste\"     \"match\"     \"opera\"    \n [5,] \"worst\"     \"davis\"     \"johnny\"   \n [6,] \"porn\"      \"detective\" \"faces\"    \n [7,] \"budget\"    \"sullivan\"  \"don\"      \n [8,] \"screaming\" \"walter\"    \"invisible\"\n [9,] \"talking\"   \"perfectly\" \"race\"     \n[10,] \"bomb\"      \"wwe\"       \"pilot\"    \n\n\n\n\nApply Learned Model to New Data\nOne thing we occasionally may be interested in doing is understanding how well our model fits the data. Therefore, we can rely on our supervised learning insights and apply the estimated model to new data. From that, we’ll obtain a document-topic distribution that we can:\n\n# creating iterator\nit2 &lt;- itoken(movie_review$review[3001:5000], tolower,\n              word_tokenizer, ids = movie_review$id[3001:5000])\n# creating new DFM\nnew_dtm &lt;- create_dtm(it2, vectorizer, type = \"dgTMatrix\")\n\nWe will have to use $transform instead of $fit_transform since we don’t have to fit the new model (we are attempting to predict the last 2000).\n\nnew_doc_topiic_distr = lda_model$transform(new_dtm)\n\nINFO  [11:52:51.225] early stopping at 40 iteration\n\n\nOne widely used approach for model hyper-parameter tuning is validation of per-word perplexity on hold-out set. This is quite easy with text2vec.\nRemember that we’ve fit the model on only the first 3000 reviews and predicted the last 2000. Therefore, we will calculate the held-out perplexity on these 2000 docs as follows:\n\n# calculates perplexity between new and old topic word distribution\nperplexity(new_dtm, topic_word_distribution = lda_model$topic_word_distribution,\n           doc_topic_distribution = new_doc_topiic_distr)\n\n[1] 2298.228\n\n\nThe lower perplexity the better. We can imagine adapting our hyperparameters and re-estimating across perplexity to try to evaluate our model performance. Still, perplexity as a measure has it’s own concerns: it doesn’t directly provide insight on whether or not the topics make sense, and tends to prefer bigger models than smaller ones.\n\n\nVisualization\nNormally it would take one line to run the visualization for the LDA model, using the method $plot().\nLet’s download and load in the required library the visuals depend on:\n\n#install.packages('LDAvis')\nlibrary(LDAvis)\n\n\n# creating plot\nlda_model$plot()\n\nLoading required namespace: servr"
  },
  {
    "objectID": "Tutorial10.html#structural-topic-model",
    "href": "Tutorial10.html#structural-topic-model",
    "title": "Tutorial10_Topic Models",
    "section": "Structural Topic Model",
    "text": "Structural Topic Model\nImagine you are interested in the topics that are explored in political speeches, and specifically whether Republicans and Democrats focus on different topics. One approach would be to–after estimating an LDA model like above–average the topic proportions by the speaker.\nOf course, that seems inefficient. We might want to instead leverage the information on the speech itself as part of the estimation of the topics. That is, we are estimating topical prevalence, and we know that there’s a different speaker, so we should be incorporating that information in estimating the topics. That’s the fundamental idea with Structural Topic Models (STM).\n\nFront-end Matters\nSTM has really fantastic documentation and a host of related packages for added functionality. You can find the STM website here. Let’s load the package. Note that this will almost certainly take a few minutes given all of the dependencies.\n\n#install.packages(\"stm\")\nlibrary(stm)\n\nstm v1.3.6 successfully loaded. See ?stm for help. \n Papers, resources, and other materials at structuraltopicmodel.com\n\nlibrary(quanteda)\n\nPackage version: 3.3.1\nUnicode version: 14.0\nICU version: 70.1\n\n\nParallel computing: 4 of 4 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\n\n\n\nCreating the DFM\nWe’ll continue to use the movie reviews dataset. Now, we’ll leverage the sentiment variable included in the dataset as a covariate in our estimates of topical prevalence; that is, we expect some topics to be more prevalent in positive reviews as opposed to negative reviews, and vice versa. The variable is coded [0,1], with 0 indicating a negative review and 1 indicating a positive review.\n\ntable(movie_review$sentiment)\n\n\n   0    1 \n2483 2517 \n\n\nSTM works differently than the text2vec, so we’ll need to have our data in a different format now.\n\nmyDfm &lt;- dfm(tokens(movie_review$review),\n             tolower = TRUE,\n             remove = stopwords(\"en\"),\n             remove_punct = TRUE)\n\nWarning: '...' should not be used for tokens() arguments; use 'tokens()' first.\n\n\nWarning: 'remove' is deprecated; use dfm_remove() instead\n\ndim(myDfm)\n\n[1]  5000 47719\n\n\n\n\nCorrelated Topic Model\nNow that we have our corpus, we can prep for a structural topic model that incorporates covariates. Recall, however, that the STM without covariates reduces to a very fast implementation of Correlated Topic Models (i.e., a version of the vanilla LDA model but where the topic proportions can be positively correlated with one another).\n\ncor_topic_model &lt;- stm(myDfm, K = 5,\n                       verbose = FALSE, init.type = \"Spectral\")\ncor_topic_model\n\nA topic model with 5 topics, 5000 documents and a 47719 word dictionary.\n\nsummary(cor_topic_model)\n\nA topic model with 5 topics, 5000 documents and a 47719 word dictionary.\n\n\nTopic 1 Top Words:\n     Highest Prob: film, one, horror, just, even, like, bad \n     FREX: slasher, zombie, zombies, scarecrows, halloween, kornbluth, horror \n     Lift: boogey, btk, cheapo, copycat, dornwinkle, faat, impaled \n     Score: zombie, slasher, horror, scarecrows, kornbluth, zombies, scarecrow \nTopic 2 Top Words:\n     Highest Prob: &gt;, &lt;, br, film, one, movie, like \n     FREX: &gt;, &lt;, br, zizek, miya, |, aztec \n     Lift: 102, acre, bolkan, brull, charis, cortes, florinda \n     Score: &gt;, &lt;, br, miya, zizek, slugs, oshii \nTopic 3 Top Words:\n     Highest Prob: film, one, story, life, films, also, love \n     FREX: bettie, widmark, sidney, mathieu, vargas, macarthur, chavez \n     Lift: _by, --almost, --five-out-of-ten, --not, --who, --you, -1940s \n     Score: film, bettie, mathieu, widmark, macarthur, chavez, vargas \nTopic 4 Top Words:\n     Highest Prob: one, show, good, best, also, film, like \n     FREX: wwe, rochester, triple, blandings, kolchak, spock, taker \n     Lift: _ex_executives, --another, --but, -erica, -filled, -juan, -kelly \n     Score: wwe, taker, bubba, benoit, rochester, booker, kolchak \nTopic 5 Top Words:\n     Highest Prob: movie, like, just, one, good, film, really \n     FREX: movie, watched, movies, liked, funny, think, loved \n     Lift: _____, ______, _real_, _tried, --an, --locate, --polarisdib \n     Score: movie, movies, bad, stupid, funny, think, like \n\n\nOnce we’ve estimated the model, we’ll want to take a look at the topics. Importantly, we don’t get nice, neat topic names. What we do have are the words that are most frequent, probable, or that otherwise characterize a topic. STM provides handy functionality to extract those words with the labelTopics() function.\n\nlabelTopics(cor_topic_model)\n\nTopic 1 Top Words:\n     Highest Prob: film, one, horror, just, even, like, bad \n     FREX: slasher, zombie, zombies, scarecrows, halloween, kornbluth, horror \n     Lift: boogey, btk, cheapo, copycat, dornwinkle, faat, impaled \n     Score: zombie, slasher, horror, scarecrows, kornbluth, zombies, scarecrow \nTopic 2 Top Words:\n     Highest Prob: &gt;, &lt;, br, film, one, movie, like \n     FREX: &gt;, &lt;, br, zizek, miya, |, aztec \n     Lift: 102, acre, bolkan, brull, charis, cortes, florinda \n     Score: &gt;, &lt;, br, miya, zizek, slugs, oshii \nTopic 3 Top Words:\n     Highest Prob: film, one, story, life, films, also, love \n     FREX: bettie, widmark, sidney, mathieu, vargas, macarthur, chavez \n     Lift: _by, --almost, --five-out-of-ten, --not, --who, --you, -1940s \n     Score: film, bettie, mathieu, widmark, macarthur, chavez, vargas \nTopic 4 Top Words:\n     Highest Prob: one, show, good, best, also, film, like \n     FREX: wwe, rochester, triple, blandings, kolchak, spock, taker \n     Lift: _ex_executives, --another, --but, -erica, -filled, -juan, -kelly \n     Score: wwe, taker, bubba, benoit, rochester, booker, kolchak \nTopic 5 Top Words:\n     Highest Prob: movie, like, just, one, good, film, really \n     FREX: movie, watched, movies, liked, funny, think, loved \n     Lift: _____, ______, _real_, _tried, --an, --locate, --polarisdib \n     Score: movie, movies, bad, stupid, funny, think, like \n\n\nWe can also look at the top documents associated with each topic using findThoughts(). Here, we’ll look at the top document (n=1) for each of the 5 topics (topics = c(1:5)).\n\nfindThoughts(cor_topic_model,\n             texts = movie_review$review,\n             topics = c(1:5),\n             n = 1)\n\n\n Topic 1: \n     Need a lesson in pure, abject failure?? Look no further than \\\"Wizards of the Lost Kingdom\\\", an abysmal, dirt-poor, disgrace of a flick. As we all know, decent moovies tend to sprout horrible, horrible offspring: \\\"Halloween\\\" begat many, many bad 80's slasher flicks; \\\"Mad Max\\\" begat many, many bad 80's \\\"futuristic wasteland fantasy\\\" flicks; and \\\"Conan the Barbarian\\\" begat a whole slew of terrible, horrible, incredibly bad 80's sword-and-sorcery flicks. \\\"Wizards of the Lost Kingdom\\\" scrapes the bottom of that 80's barrel, in a way that's truly insulting to barrels. A young runt named Simon recaptured his \\\"good kingdom\\\" from an evil sorcerer with the help of a mangy rug, a garden gnome, a topless bimbo mermaid, and a tired-looking, pudgy Bo Svenson. Svenson(\\\"North Dallas Forty\\\", \\\"Inglorious Bastards\\\", \\\"Delta Force\\\"), a long-time b-moovie muscleman, looks barely able to swing his aluminum foil sword. However, he manages to defeat the forces of evil, which consist of the evil sorcerer, \\\"Shurka\\\", and his army of badly costumed monsters, giants, and midgets. At one point, a paper mache bat on a string attacks, but is eaten by a 1/2 hidden sock puppet, pitifully presented as some sort of dragon. The beginning of the film consists of what can only politely be described as bits of scenes scooped up from the cutting-room floor of udder bad moovies, stitched together in the vain hope of setting the scene for the film, and over-earnestly narrated by some guy who never appears again. Words cannot properly convey the jaw-dropping cheapness of this film; the producers probably spent moore moolah feeding Svenson's ever expanding gullet than on the cheesy fx of this flick. And we're talkin' Brie here, folks... :=8P Director Hector Olivera(\\\"Barbarian Queen\\\") presents this mish-mash in a hopelessly confused, confuddled, and cliched manner, destroying any possible hint of clear, linear storytelling. The acting is dreadful, the production levels below shoe-string, and the plot is one tired cliche after another paraded before our weary eyes. That they actually made a sequel(!!!) makes the MooCow's brain whirl. James Horner's(\\\"Braveheart\\\", \\\"Titanic\\\",\\\"The Rock\\\") cheesy moosic from \\\"Battle Beyond the Stars\\\" was lifted, screaming and kicking, and mercilessly grafted onto this turkey - bet this one doesn't pop up on his resume. Folks, you gotta see this to believe it. The MooCow says as a cheapo rent when there is NOTHING else to watch, well, it's moore fun than watching dust bunnies mate. Barely. :=8P \n Topic 2: \n     David Arquette is a young and naive home security alarm&lt;br /&gt;&lt;br /&gt;salesman taken under the wing of Stanley Tucci. Arquette is a&lt;br /&gt;&lt;br /&gt;golden boy, scoring a big sale on his first call- to widow Kate&lt;br /&gt;&lt;br /&gt;Capshaw and her dopey son Ryan Reynolds. Things are going&lt;br /&gt;&lt;br /&gt;well for Arquette, he is appearing in commercials for the security&lt;br /&gt;&lt;br /&gt;firm and he is falling in love with Capshaw.&lt;br /&gt;&lt;br /&gt;Then Tucci and his right hand woman Mary McCormack let him in&lt;br /&gt;&lt;br /&gt;on a little secret- they sometimes break into the houses of their&lt;br /&gt;&lt;br /&gt;clients in order to scare them and to get their neighbors to buy&lt;br /&gt;&lt;br /&gt;security systems from the firm. Arquette decides not to get&lt;br /&gt;&lt;br /&gt;involved, taking Capshaw to meet his family, and going through life&lt;br /&gt;&lt;br /&gt;with a goofy smile on his face. Then, someone breaks into&lt;br /&gt;&lt;br /&gt;Capshaw's home and murders her and her son. Arquette suspects Tucci, and sets a series of traps, resulting in a gun to his&lt;br /&gt;&lt;br /&gt;boss' head as Tucci pleads his innocence.&lt;br /&gt;&lt;br /&gt;Based on a stage play, \\\"The Alarmist\\\" is not opened up well. The&lt;br /&gt;&lt;br /&gt;scenes where Arquette takes the Capshaw to meet his parents&lt;br /&gt;&lt;br /&gt;are badly played and completely unfunny. They are also out of line&lt;br /&gt;&lt;br /&gt;with the character Capshaw is playing, as she gets drunk and tells&lt;br /&gt;&lt;br /&gt;sexually explicit stories to Arquette's mom Michael Learned. Other&lt;br /&gt;&lt;br /&gt;than these scenes, Capshaw is not given much to do, but she&lt;br /&gt;&lt;br /&gt;does a lot with the little she is given.&lt;br /&gt;&lt;br /&gt;Stanley Tucci, looking just like Terry O'Quinn, is a riot as the&lt;br /&gt;&lt;br /&gt;security firm owner. He is a creep who really does not understand&lt;br /&gt;&lt;br /&gt;Arquette's moral revulsion. However, when he turns into a&lt;br /&gt;&lt;br /&gt;sniveling whiner after Arquette kidnaps him, he is hilarious. Mary&lt;br /&gt;&lt;br /&gt;McCormack seems to have been groomed for a bigger role, but&lt;br /&gt;&lt;br /&gt;she mostly stands around and agrees with Tucci. Ryan Reynolds&lt;br /&gt;&lt;br /&gt;is too old to play a dumb teenager, but he is funny, especially&lt;br /&gt;&lt;br /&gt;telling his own explicit sexual story to Arquette.&lt;br /&gt;&lt;br /&gt;The screenplay lurches from romantic comedy to dark comedy too&lt;br /&gt;&lt;br /&gt;soon. Capshaw meeting the parents is completely unmotivated,&lt;br /&gt;&lt;br /&gt;except to give her a reason to get out of town so someone can&lt;br /&gt;&lt;br /&gt;break into her house. Capshaw and Reynolds are in the film just&lt;br /&gt;&lt;br /&gt;to give Arquette a reason to take revenge on Tucci.&lt;br /&gt;&lt;br /&gt;Arquette, who has proven he is a good actor, is awful here. He&lt;br /&gt;&lt;br /&gt;relies on the constipated mugging that got him through those&lt;br /&gt;&lt;br /&gt;AT&T ads, and he is not a strong enough presence to build this&lt;br /&gt;&lt;br /&gt;weak film around. Actually, Reynolds might have been a better&lt;br /&gt;&lt;br /&gt;choice in the role.&lt;br /&gt;&lt;br /&gt;Dunsky's direction is good, nothing that will win an Oscar soon.&lt;br /&gt;&lt;br /&gt;Christophe Beck's light jazzy score recalls the type of film noir this&lt;br /&gt;&lt;br /&gt;film tries to be, and it is really catchy on top of that.&lt;br /&gt;&lt;br /&gt;Despite the pluses, Arquette's failure as a lead and the script's&lt;br /&gt;&lt;br /&gt;schizophrenic quality sinks the film. I do not recommend it.&lt;br /&gt;&lt;br /&gt;This is rated (R) for physical violence, gun violence, some gore,&lt;br /&gt;&lt;br /&gt;strong profanity, brief female nudity, sexual content, strong sexual&lt;br /&gt;&lt;br /&gt;references, and adult situations. \n Topic 3: \n     Since most review's of this film are of screening's seen decade's ago I'd like to add a more recent one, the film open's with stock footage of B-17's bombing Germany, the film cut's to Oskar Werner's Hauptmann (captain) Wust character and his aide running for cover while making their way to Hitler's Fuehrer Bunker, once inside, they are debriefed by bunker staff personnel, the film then cut's to one of many conference scene's with Albin Skoda giving a decent impression of Adolf Hitler rallying his officer's to \\\"Ultimate Victory\\\" while Werner's character is shown as slowly coming to realize the bunker denizen's are caught up in a fantasy world-some non-bunker event's are depicted, most notable being the flooding of the subway system to prevent a Russian advance through them and a minor subplot involving a young member of the Flak unit's and his family's difficulty in surviving-this film suffer's from a number of detail inaccuracies that a German film made only 10 year's after WW2 should not have included; the actor portraying Goebbels (Willy Krause) wear's the same uniform as Hitler, including arm eagle- Goebbels wore a brown Nazi Party uniform with swastika armband-the \\\"SS\\\" soldier's wear German army camouflage, the well documented scene of Hitler awarding the iron cross to boy's of the Hitler Youth is shown as having taken place INSIDE the bunker (it was done outside in the courtyard) and lastly, Hitler's suicide weapon is clearly shown as a Belgian browning model 1922-most account's agree it was a Walther PPK-some bit's of acting also seem wholly inaccurate with the drunken dance scene near the end of the film being notable, this bit is shown as a cabaret skit, with a intoxicated wounded soldier (his arm in a splint) maniacally goose-stepping to music while a nurse does a combination striptease/belly dance, all by candlelight... this is actually embarrassing to watch-the most incredible bit is when Werner's Captain Wust gain's an audience alone with Skoda's Hitler, Hitler is shown as slumped on a wall bench, drugged and delirious, when Werner's character begin's to question him, Hitler start's screaming which bring's in a SS guard who mortally wound's Werner's character in the back with a gunshot-this fabricated scene is not based on any true historic account-Werner's character is then hauled off to die in a anteroom while Hitler prepare's his own ending, Hitler's farewell to his staff is shown but the suicide is off-screen, the final second's of the movie show Hitler's funeral pyre smoke slowly forming into a ghostly image of the face of the dead Oskar Werner/Hauptmann Wust-this film is more allegorical than historical and anyone interested in this period would do better to check out more recent film's such as the 1973 remake \\\"Hitler: the last 10 day's\\\" or the German film \\\"Downfall\\\" (Der Untergang) if they wish a more true accounting of this dramatic story, these last two film's are based on first person eyewitness account's, with \\\"Hitler: the last 10 day's\\\" being compiled from Gerhard Boldt's autobiography as a staff officer in the Fuehrer Bunker and \\\"Downfall\\\" being done from Hitler's secretary's recollection's, the screen play for \\\"Der Letzte Akte\\\" is taken from American Nuremberg war crime's trial judge Michael Musmanno's book \\\"Ten day's to die\\\", which is more a compilation of event's (many obviously fanciful) than eyewitness history-it is surprising that Hugh Trevor Roper's account,\\\"The last day's of Hitler\\\" was never made into a film. \n Topic 4: \n     This happy-go-luck 1939 military swashbuckler, based rather loosely on Rudyard Kipling's memorable poem as well as his novel \\\"Soldiers Three,\\\" qualifies as first-rate entertainment about the British Imperial Army in India in the 1880s. Cary Grant delivers more knock-about blows with his knuckled-up fists than he did in all of his movies put together. Set in faraway India, this six-fisted yarn dwells on the exploits of three rugged British sergeants and their native water bearer Gunga Din (Sam Jaffe) who contend with a bloodthirsty cult of murderous Indians called the Thuggee. Sergeant Archibald Cutter (Cary Grant of \\\"The Last Outpost\\\"), Sergeant MacChesney (Oscar-winner Victor McLaglen of \\\"The Informer\\\"), and Sergeant Ballantine (Douglas Fairbanks, Jr. of \\\"The Dawn Patrol\\\"), are a competitive trio of hard-drinking, hard-brawling, and fun-loving Alpha males whose years of frolic are about to become history because Ballantine plans to marry Emmy Stebbins (Joan Fontaine) and enter the tea business. Naturally, Cutter and MacChesney drum up assorted schemes to derail Ballentine's plans. When their superiors order them back into action with Sgt. Bertie Higginbotham (Robert Coote of \\\"The Sheik Steps Out\\\"), Cutter and MacChesney drug Higginbotham so that he cannot accompany them and Ballantine has to replace him. Half of the fun here is watching the principals trying to outwit each other without hating themselves. Director George Stevens celebrates the spirit of adventure in grand style and scope as our heroes tangle with an army of Thuggees. Lenser Joseph H. August received an Oscar nomination for his outstanding black & white cinematography. \n Topic 5: \n     Full House is a great show. I am still today growing up on it. I started watching it when i was 8 and now i am 12 and still watching it. i fell in love with all of the characters, especially Stephanie. she is my favorite. she had such a sense of humor. in case there are people on this sight that hardly watch the show, you should because you will get hooked on it. i became hooked on it after the first show i saw, which just happened to be the first episode, in 2002. it really is a good show. i really think that this show should go down to many generations in families. and it's great too because it is an appropriate show for all ages. and for all parents, it teaches kids lessons on how to go on with their life. nothing terrible happens, like violence or swearing. it is just a really great sit-com. i give it 5 out of 5 stars. what do you think? OH and the best time to watch it is when you are home sick from school or even the old office. It will make you feel a lot better. Trust me i am hardly home sick but i still know that it will make you feel better. and to everybody that thinks the show is stupid, well that's too bad for you because you won't get as far in life even if you are happy with your life. you really should watch it and you will get hooked on it. i am just telling you what happened to me and everybody else that started watching this awesome show. well i need must go to have some lunch. remember you must start watching full house and soon!\n\n\n\n\nStructural Topic model\nLet’s go ahead and estimate our structural topic model now. We’ll incorporate the sentiment variable as a predictor on prevalence.\n\n# choose our number of topics\nk &lt;- 5\n\n# specify model\nmyModel &lt;- stm(myDfm,\n               K = k,\n               prevalence = ~ sentiment,\n               data = movie_review,\n               max.em.its = 1000,\n               seed = 1234,\n               init.type = \"Spectral\")\n\nBeginning Spectral Initialization \n     Calculating the gram matrix...\n     Using only 10000 most frequent terms during initialization...\n     Finding anchor words...\n    .....\n     Recovering initialization...\n    ....................................................................................................\nInitialization complete.\n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 1 (approx. per word bound = -8.615) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 2 (approx. per word bound = -8.043, relative change = 6.646e-02) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 3 (approx. per word bound = -8.008, relative change = 4.390e-03) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 4 (approx. per word bound = -7.999, relative change = 1.042e-03) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 5 (approx. per word bound = -7.995, relative change = 4.933e-04) \nTopic 1: film, br, &lt;, &gt;, movie \n Topic 2: &gt;, &lt;, br, film, movie \n Topic 3: film, br, &lt;, &gt;, one \n Topic 4: one, br, &lt;, &gt;, show \n Topic 5: movie, like, one, just, br \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 6 (approx. per word bound = -7.993, relative change = 2.928e-04) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 7 (approx. per word bound = -7.991, relative change = 1.982e-04) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 8 (approx. per word bound = -7.990, relative change = 1.496e-04) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 9 (approx. per word bound = -7.989, relative change = 1.181e-04) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 10 (approx. per word bound = -7.988, relative change = 9.822e-05) \nTopic 1: film, one, just, like, movie \n Topic 2: &gt;, &lt;, br, film, one \n Topic 3: film, br, &lt;, &gt;, one \n Topic 4: one, show, like, good, film \n Topic 5: movie, like, just, one, good \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 11 (approx. per word bound = -7.988, relative change = 8.465e-05) \n....................................................................................................\nCompleted E-Step (2 seconds). \nCompleted M-Step. \nCompleting Iteration 12 (approx. per word bound = -7.987, relative change = 7.740e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 13 (approx. per word bound = -7.987, relative change = 6.696e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 14 (approx. per word bound = -7.986, relative change = 5.219e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 15 (approx. per word bound = -7.986, relative change = 4.341e-05) \nTopic 1: film, one, just, like, even \n Topic 2: &gt;, &lt;, br, film, one \n Topic 3: film, one, story, br, &lt; \n Topic 4: one, show, good, like, film \n Topic 5: movie, like, just, one, good \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 16 (approx. per word bound = -7.986, relative change = 3.805e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 17 (approx. per word bound = -7.985, relative change = 3.305e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 18 (approx. per word bound = -7.985, relative change = 2.854e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 19 (approx. per word bound = -7.985, relative change = 2.753e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 20 (approx. per word bound = -7.985, relative change = 2.927e-05) \nTopic 1: film, one, just, even, like \n Topic 2: &gt;, &lt;, br, film, one \n Topic 3: film, one, story, life, films \n Topic 4: one, show, good, film, best \n Topic 5: movie, like, just, one, good \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 21 (approx. per word bound = -7.984, relative change = 3.115e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 22 (approx. per word bound = -7.984, relative change = 2.915e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 23 (approx. per word bound = -7.984, relative change = 2.367e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 24 (approx. per word bound = -7.984, relative change = 2.057e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 25 (approx. per word bound = -7.984, relative change = 2.056e-05) \nTopic 1: film, one, just, even, like \n Topic 2: &gt;, &lt;, br, film, one \n Topic 3: film, one, story, life, films \n Topic 4: one, show, good, best, film \n Topic 5: movie, like, just, one, good \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 26 (approx. per word bound = -7.983, relative change = 2.021e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 27 (approx. per word bound = -7.983, relative change = 1.789e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 28 (approx. per word bound = -7.983, relative change = 1.564e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 29 (approx. per word bound = -7.983, relative change = 1.315e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 30 (approx. per word bound = -7.983, relative change = 1.195e-05) \nTopic 1: film, one, just, even, like \n Topic 2: &gt;, &lt;, br, film, one \n Topic 3: film, one, story, life, films \n Topic 4: one, show, good, best, film \n Topic 5: movie, like, just, one, good \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 31 (approx. per word bound = -7.983, relative change = 1.065e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 32 (approx. per word bound = -7.983, relative change = 1.092e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 33 (approx. per word bound = -7.983, relative change = 1.172e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 34 (approx. per word bound = -7.983, relative change = 1.203e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 35 (approx. per word bound = -7.983, relative change = 1.252e-05) \nTopic 1: film, one, just, even, like \n Topic 2: &gt;, &lt;, br, film, one \n Topic 3: film, one, story, life, films \n Topic 4: one, show, good, film, best \n Topic 5: movie, like, just, one, good \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 36 (approx. per word bound = -7.982, relative change = 1.298e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 37 (approx. per word bound = -7.982, relative change = 1.139e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nModel Converged \n\n\nNote what’s significantly different from before is added the prevalence formula. As we discuss in lecture, you can also include variables as content predictors.\n\nlabelTopics(myModel)\n\nTopic 1 Top Words:\n     Highest Prob: film, one, just, even, like, bad, horror \n     FREX: slasher, scarecrows, zombie, zombies, kornbluth, scarecrow, seagal \n     Lift: -overall, 2600, addy, antichrist, aranoa, ba, bale \n     Score: slasher, zombie, scarecrows, kornbluth, zombies, horror, bad \nTopic 2 Top Words:\n     Highest Prob: &gt;, &lt;, br, film, one, movie, like \n     FREX: &gt;, &lt;, br, zizek, miya, |, aztec \n     Lift: ------------, --------------, ----------------, -------------------, ----------------------------------------, -buster, -contest \n     Score: &gt;, &lt;, br, zizek, miya, slugs, oshii \nTopic 3 Top Words:\n     Highest Prob: film, one, story, life, films, also, love \n     FREX: bettie, widmark, sidney, mathieu, macarthur, chavez, israel \n     Lift: lumet, _by, --five-out-of-ten, --not, --who, --you, -1940s \n     Score: film, bettie, mathieu, widmark, macarthur, aids, antwone \nTopic 4 Top Words:\n     Highest Prob: one, show, film, good, best, also, like \n     FREX: wwe, rochester, blandings, kolchak, spock, taker, dominick \n     Lift: 1692, 1931, absurdist, adrien, adversaries, affirmative, alaric \n     Score: wwe, taker, bubba, benoit, booker, rochester, kolchak \nTopic 5 Top Words:\n     Highest Prob: movie, like, just, one, good, film, really \n     FREX: movie, watched, movies, liked, funny, loved, kids \n     Lift: affiliated, ai, ammo, anka, ardala, b5, bauraki \n     Score: movie, movies, bad, stupid, funny, think, like \n\n\nThe topics again look reasonable, and are generally similar to the topics we estimated earlier. We can go a step further by plotting out the top topics (as groups of words associated with that topic) and their estimated frequency across the corpus.\n\nplot(myModel, type = \"summary\")\n\n\n\n\nOne thing we might want to do is to extract the topics and to assign them to the vector of document proportions; this is often useful if we’re using those topic proportions in any sort of downstream analysis, including just a visualization. The following extracts the top words (here, by frex, though you can update that to any of the other three top word sets). Then it iterates through the extracted sets and collapses the strings so the tokens are separated by an underscore; this is useful as a variable name for those downstream analyses.\n\n# get the words\nmyTopicNames &lt;- labelTopics(myModel, n=4)$frex\n\n# set up an empty vector\nmyTopicLabels &lt;- rep(NA, k)\n\n# set up a loop to go through the topics and collapse the words to a single name\nfor (i in 1:k){\n  myTopicLabels[i] &lt;- paste(myTopicNames[i,], collapse = \"_\")\n}\n\n# print the names\nmyTopicLabels\n\n[1] \"slasher_scarecrows_zombie_zombies\" \"&gt;_&lt;_br_zizek\"                     \n[3] \"bettie_widmark_sidney_mathieu\"     \"wwe_rochester_blandings_kolchak\"  \n[5] \"movie_watched_movies_liked\"       \n\n\n\n\nEstimate Effect\nRecall that we included sentiment as a predictor variable on topical prevalence. We can extract the effect of the predictor here using the estimateEffect() function, which takes as arguments a formula, the stm model object, and the metadata containing the predictor variable.\nOnce we’ve run the function, we can plot the estimated effects of sentiment on topic prevalence for each of the estimated topics. With a dichotomous predictor variable, we’ll plot these out solely as the difference (method = \"difference\") in topic prevalence across the values of the predictor. Here, our estimate indicates how much more (or less) the topic is discussed when the sentiment of the post is positive.\n\n# estimate effects\nmodelEffects &lt;- estimateEffect(formula = 1:k ~ sentiment,\n                               stmobj = myModel,\n                               metadata = movie_review)\n\n# plot effects\nmyRows &lt;- 2\npar(mfrow = c(myRows, 3), bty = \"n\", lwd = 2)\nfor (i in 1:k){\n  plot.estimateEffect(modelEffects,\n                      covariate = \"sentiment\",\n                      xlim = c(-.25, .25),\n                      model = myModel,\n                      topics = modelEffects$topics[i],\n                      method = \"difference\",\n                      cov.value1 = 1,\n                      cov.value2 = 0, \n                      main = myTopicLabels[i],\n                      printlegend = F,\n                      linecol = \"grey26\",\n                      labeltype = \"custom\",\n                      verbose.labels = F,\n                      custom.labels = c(\"\"))\n  par(new = F)\n}\n\n\n\n\n\n\nChoosing K\nI’m sure you were thinking “How did she select 5 topics?” Well, the answer is that it was just a random number that I selected out of thin air. The choice of the number of topics, typically denoted K, is one of the areas where the design of topic models let’s us as researchers down a bit. While some approaches have been proposed, none have really gained traction. STM includes an approach that we won’t explore based on work by David Mimno that automatically identifies a topic; in reality, it normally results in far more topics than a human would be likely to choose.\nWith all that said, there is some functionality included with STM to explore different specifications and to try to at least get some idea of how different approaches perform. searchK() lets you estimate a series of different models, then you can plot a series of different evaluation metrics across those choices.\n\ndifferentKs &lt;- searchK(myDfm,\n                       K = c(5, 25, 50),\n                       prevalence = ~ sentiment,\n                       N = 250,\n                       data = movie_review,\n                       max.em.its = 1000,\n                       init.type = \"Spectral\")\n\nBeginning Spectral Initialization \n     Calculating the gram matrix...\n     Using only 10000 most frequent terms during initialization...\n     Finding anchor words...\n    .....\n     Recovering initialization...\n    ....................................................................................................\nInitialization complete.\n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 1 (approx. per word bound = -8.614) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 2 (approx. per word bound = -8.038, relative change = 6.687e-02) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 3 (approx. per word bound = -8.002, relative change = 4.386e-03) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 4 (approx. per word bound = -7.995, relative change = 9.712e-04) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 5 (approx. per word bound = -7.991, relative change = 4.555e-04) \nTopic 1: &gt;, br, &lt;, one, film \n Topic 2: movie, like, just, one, film \n Topic 3: &gt;, &lt;, br, film, movie \n Topic 4: film, &lt;, br, &gt;, one \n Topic 5: &gt;, br, &lt;, one, show \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 6 (approx. per word bound = -7.989, relative change = 2.730e-04) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 7 (approx. per word bound = -7.987, relative change = 1.882e-04) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 8 (approx. per word bound = -7.986, relative change = 1.442e-04) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 9 (approx. per word bound = -7.985, relative change = 1.026e-04) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 10 (approx. per word bound = -7.985, relative change = 7.521e-05) \nTopic 1: one, film, like, just, characters \n Topic 2: movie, like, film, just, one \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, one, &lt;, br, &gt; \n Topic 5: &gt;, br, &lt;, one, show \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 11 (approx. per word bound = -7.984, relative change = 5.662e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 12 (approx. per word bound = -7.984, relative change = 4.903e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 13 (approx. per word bound = -7.984, relative change = 4.331e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 14 (approx. per word bound = -7.983, relative change = 3.716e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 15 (approx. per word bound = -7.983, relative change = 3.185e-05) \nTopic 1: film, one, like, just, characters \n Topic 2: movie, film, like, just, one \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, one, story, &lt;, br \n Topic 5: &gt;, br, &lt;, one, show \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 16 (approx. per word bound = -7.983, relative change = 2.919e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 17 (approx. per word bound = -7.983, relative change = 2.689e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 18 (approx. per word bound = -7.982, relative change = 2.472e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 19 (approx. per word bound = -7.982, relative change = 2.209e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 20 (approx. per word bound = -7.982, relative change = 2.178e-05) \nTopic 1: film, one, like, just, characters \n Topic 2: movie, film, like, just, one \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, one, story, life, also \n Topic 5: &gt;, br, &lt;, one, show \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 21 (approx. per word bound = -7.982, relative change = 2.088e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 22 (approx. per word bound = -7.982, relative change = 2.045e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 23 (approx. per word bound = -7.982, relative change = 2.134e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 24 (approx. per word bound = -7.981, relative change = 1.825e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 25 (approx. per word bound = -7.981, relative change = 1.355e-05) \nTopic 1: film, one, like, just, characters \n Topic 2: movie, film, like, just, one \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, one, story, life, also \n Topic 5: one, &gt;, br, &lt;, show \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 26 (approx. per word bound = -7.981, relative change = 1.156e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nCompleting Iteration 27 (approx. per word bound = -7.981, relative change = 1.014e-05) \n....................................................................................................\nCompleted E-Step (1 seconds). \nCompleted M-Step. \nModel Converged \nBeginning Spectral Initialization \n     Calculating the gram matrix...\n     Using only 10000 most frequent terms during initialization...\n     Finding anchor words...\n    .........................\n     Recovering initialization...\n    ....................................................................................................\nInitialization complete.\n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 1 (approx. per word bound = -8.589) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 2 (approx. per word bound = -7.792, relative change = 9.278e-02) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 3 (approx. per word bound = -7.708, relative change = 1.084e-02) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 4 (approx. per word bound = -7.691, relative change = 2.256e-03) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 5 (approx. per word bound = -7.683, relative change = 9.445e-04) \nTopic 1: film, &lt;, br, &gt;, characters \n Topic 2: movie, film, good, actors, one \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, one, story, br, &lt; \n Topic 5: show, one, episode, best, series \n Topic 6: movie, great, one, film, made \n Topic 7: movie, film, good, one, interesting \n Topic 8: film, &lt;, &gt;, br, one \n Topic 9: br, &lt;, &gt;, film, one \n Topic 10: movie, film, got, like, one \n Topic 11: br, &lt;, &gt;, good, time \n Topic 12: br, &gt;, &lt;, film, one \n Topic 13: br, &gt;, &lt;, film, one \n Topic 14: movie, &gt;, &lt;, br, just \n Topic 15: br, &gt;, &lt;, film, one \n Topic 16: br, &lt;, &gt;, movie, film \n Topic 17: movie, bad, film, good, just \n Topic 18: movie, like, show, br, &lt; \n Topic 19: &gt;, br, &lt;, movie, film \n Topic 20: &gt;, &lt;, br, one, film \n Topic 21: &lt;, br, one, &gt;, movie \n Topic 22: film, &gt;, br, &lt;, just \n Topic 23: &lt;, br, &gt;, new, one \n Topic 24: br, &gt;, &lt;, movie, film \n Topic 25: &gt;, br, &lt;, documentary, film \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 6 (approx. per word bound = -7.679, relative change = 5.183e-04) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 7 (approx. per word bound = -7.677, relative change = 3.323e-04) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 8 (approx. per word bound = -7.675, relative change = 2.341e-04) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 9 (approx. per word bound = -7.674, relative change = 1.762e-04) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 10 (approx. per word bound = -7.673, relative change = 1.404e-04) \nTopic 1: film, characters, one, &lt;, br \n Topic 2: movie, film, good, actors, story \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, one, story, films, excellent \n Topic 5: show, one, episode, best, series \n Topic 6: movie, great, one, film, made \n Topic 7: movie, film, good, one, interesting \n Topic 8: film, &lt;, &gt;, br, one \n Topic 9: film, br, &lt;, &gt;, one \n Topic 10: movie, film, got, like, one \n Topic 11: good, time, film, one, br \n Topic 12: br, &gt;, &lt;, film, one \n Topic 13: br, &lt;, &gt;, film, one \n Topic 14: movie, just, bad, like, see \n Topic 15: br, &gt;, &lt;, film, one \n Topic 16: movie, film, br, &lt;, &gt; \n Topic 17: film, movie, bad, good, just \n Topic 18: movie, like, show, just, one \n Topic 19: &gt;, br, &lt;, movie, film \n Topic 20: &gt;, &lt;, br, one, film \n Topic 21: one, movie, &lt;, br, &gt; \n Topic 22: film, just, one, &gt;, br \n Topic 23: &lt;, br, &gt;, new, film \n Topic 24: movie, br, &gt;, &lt;, film \n Topic 25: documentary, war, &gt;, film, br \n....................................................................................................\nCompleted E-Step (2 seconds). \nCompleted M-Step. \nCompleting Iteration 11 (approx. per word bound = -7.672, relative change = 1.166e-04) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 12 (approx. per word bound = -7.671, relative change = 1.016e-04) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 13 (approx. per word bound = -7.670, relative change = 8.924e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 14 (approx. per word bound = -7.670, relative change = 8.016e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 15 (approx. per word bound = -7.669, relative change = 7.067e-05) \nTopic 1: film, characters, one, like, just \n Topic 2: movie, film, good, actors, story \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, one, story, films, excellent \n Topic 5: show, one, episode, best, series \n Topic 6: movie, great, one, film, good \n Topic 7: movie, film, good, one, interesting \n Topic 8: film, one, &lt;, &gt;, br \n Topic 9: film, br, &lt;, &gt;, one \n Topic 10: movie, film, got, like, one \n Topic 11: good, time, film, one, story \n Topic 12: br, &gt;, &lt;, film, one \n Topic 13: br, &lt;, &gt;, film, one \n Topic 14: movie, just, bad, like, see \n Topic 15: film, one, much, like, br \n Topic 16: movie, film, one, just, like \n Topic 17: film, bad, movie, just, good \n Topic 18: movie, like, show, just, tv \n Topic 19: &gt;, br, &lt;, film, movie \n Topic 20: one, film, &gt;, br, &lt; \n Topic 21: one, movie, film, movies, seen \n Topic 22: film, just, one, like, films \n Topic 23: &lt;, br, &gt;, new, film \n Topic 24: movie, film, love, br, &gt; \n Topic 25: war, documentary, film, people, &gt; \n....................................................................................................\nCompleted E-Step (2 seconds). \nCompleted M-Step. \nCompleting Iteration 16 (approx. per word bound = -7.669, relative change = 6.158e-05) \n....................................................................................................\nCompleted E-Step (2 seconds). \nCompleted M-Step. \nCompleting Iteration 17 (approx. per word bound = -7.668, relative change = 5.699e-05) \n....................................................................................................\nCompleted E-Step (2 seconds). \nCompleted M-Step. \nCompleting Iteration 18 (approx. per word bound = -7.668, relative change = 5.321e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 19 (approx. per word bound = -7.667, relative change = 5.145e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 20 (approx. per word bound = -7.667, relative change = 4.434e-05) \nTopic 1: film, characters, one, like, just \n Topic 2: movie, film, good, story, actors \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, one, story, films, woman \n Topic 5: show, one, best, episode, series \n Topic 6: movie, great, one, film, good \n Topic 7: movie, film, good, one, interesting \n Topic 8: film, one, like, story, also \n Topic 9: film, br, &lt;, &gt;, one \n Topic 10: movie, film, got, like, one \n Topic 11: good, time, film, one, story \n Topic 12: film, br, &gt;, &lt;, one \n Topic 13: film, &lt;, br, &gt;, one \n Topic 14: movie, just, bad, like, see \n Topic 15: film, one, much, like, even \n Topic 16: film, movie, one, just, like \n Topic 17: film, bad, movie, just, one \n Topic 18: movie, like, show, just, tv \n Topic 19: &gt;, br, &lt;, film, movie \n Topic 20: one, film, match, &gt;, br \n Topic 21: one, movie, film, movies, seen \n Topic 22: film, just, one, films, like \n Topic 23: new, &lt;, br, &gt;, one \n Topic 24: movie, film, love, one, s \n Topic 25: war, documentary, film, people, world \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 21 (approx. per word bound = -7.667, relative change = 4.375e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 22 (approx. per word bound = -7.666, relative change = 4.277e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 23 (approx. per word bound = -7.666, relative change = 4.140e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 24 (approx. per word bound = -7.666, relative change = 3.821e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 25 (approx. per word bound = -7.665, relative change = 3.679e-05) \nTopic 1: film, characters, one, like, just \n Topic 2: movie, film, good, story, actors \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, one, story, films, woman \n Topic 5: show, one, best, episode, series \n Topic 6: movie, great, one, film, also \n Topic 7: movie, film, good, one, interesting \n Topic 8: film, one, like, story, also \n Topic 9: film, br, &lt;, &gt;, one \n Topic 10: movie, film, got, like, one \n Topic 11: good, film, time, one, story \n Topic 12: film, one, life, br, &gt; \n Topic 13: film, one, films, &lt;, br \n Topic 14: movie, just, bad, like, see \n Topic 15: film, one, much, like, zombie \n Topic 16: film, movie, one, just, like \n Topic 17: film, bad, movie, just, one \n Topic 18: movie, show, like, just, tv \n Topic 19: film, movie, one, &gt;, much \n Topic 20: one, film, match, man, also \n Topic 21: one, movie, film, movies, seen \n Topic 22: film, just, one, films, like \n Topic 23: new, one, film, movie, &lt; \n Topic 24: movie, film, love, one, s \n Topic 25: war, documentary, people, film, world \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 26 (approx. per word bound = -7.665, relative change = 3.407e-05) \n....................................................................................................\nCompleted E-Step (2 seconds). \nCompleted M-Step. \nCompleting Iteration 27 (approx. per word bound = -7.665, relative change = 3.341e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 28 (approx. per word bound = -7.665, relative change = 3.305e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 29 (approx. per word bound = -7.664, relative change = 3.277e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 30 (approx. per word bound = -7.664, relative change = 3.064e-05) \nTopic 1: film, characters, one, like, just \n Topic 2: movie, film, good, story, actors \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, one, story, films, woman \n Topic 5: one, show, best, episode, series \n Topic 6: movie, great, one, film, also \n Topic 7: movie, film, good, one, interesting \n Topic 8: film, one, like, story, also \n Topic 9: film, one, br, &lt;, family \n Topic 10: movie, film, got, like, one \n Topic 11: good, film, time, one, even \n Topic 12: film, one, life, like, show \n Topic 13: film, one, films, like, movie \n Topic 14: movie, bad, just, like, even \n Topic 15: film, one, much, like, zombie \n Topic 16: film, movie, one, just, like \n Topic 17: film, bad, movie, just, one \n Topic 18: show, movie, like, just, tv \n Topic 19: film, movie, one, much, &gt; \n Topic 20: one, film, match, man, also \n Topic 21: one, movie, film, movies, seen \n Topic 22: film, just, one, films, like \n Topic 23: new, one, film, movie, like \n Topic 24: movie, film, love, one, s \n Topic 25: war, documentary, people, world, film \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 31 (approx. per word bound = -7.664, relative change = 2.779e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 32 (approx. per word bound = -7.664, relative change = 2.152e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 33 (approx. per word bound = -7.664, relative change = 1.999e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 34 (approx. per word bound = -7.663, relative change = 1.927e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 35 (approx. per word bound = -7.663, relative change = 2.126e-05) \nTopic 1: film, characters, one, like, just \n Topic 2: movie, film, good, story, one \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, one, story, films, woman \n Topic 5: one, show, best, episode, character \n Topic 6: movie, great, one, film, also \n Topic 7: movie, film, good, one, interesting \n Topic 8: film, one, like, story, also \n Topic 9: film, one, family, br, &lt; \n Topic 10: movie, film, got, like, one \n Topic 11: good, film, time, one, even \n Topic 12: film, one, life, like, show \n Topic 13: film, one, films, like, movie \n Topic 14: movie, bad, just, like, even \n Topic 15: film, one, much, like, zombie \n Topic 16: film, movie, one, just, like \n Topic 17: film, bad, movie, just, one \n Topic 18: show, movie, like, just, tv \n Topic 19: film, movie, one, much, game \n Topic 20: one, film, match, man, also \n Topic 21: one, movie, film, movies, seen \n Topic 22: film, just, one, films, like \n Topic 23: new, one, film, movie, like \n Topic 24: movie, film, love, one, s \n Topic 25: war, documentary, people, us, world \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 36 (approx. per word bound = -7.663, relative change = 2.180e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 37 (approx. per word bound = -7.663, relative change = 2.064e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 38 (approx. per word bound = -7.663, relative change = 1.567e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 39 (approx. per word bound = -7.663, relative change = 1.952e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 40 (approx. per word bound = -7.663, relative change = 1.663e-05) \nTopic 1: film, characters, one, like, just \n Topic 2: movie, film, good, story, one \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, one, story, woman, films \n Topic 5: one, show, best, character, episode \n Topic 6: movie, great, one, film, also \n Topic 7: movie, film, good, one, interesting \n Topic 8: film, one, like, story, just \n Topic 9: film, one, family, br, &lt; \n Topic 10: movie, film, got, like, one \n Topic 11: good, film, time, one, even \n Topic 12: film, one, life, like, young \n Topic 13: film, one, films, like, movie \n Topic 14: movie, bad, just, like, even \n Topic 15: film, one, much, like, zombie \n Topic 16: film, movie, one, just, like \n Topic 17: film, bad, movie, just, one \n Topic 18: show, movie, like, just, tv \n Topic 19: film, movie, one, much, game \n Topic 20: one, film, match, man, also \n Topic 21: one, movie, film, movies, seen \n Topic 22: film, just, one, films, like \n Topic 23: new, one, film, movie, like \n Topic 24: movie, film, love, one, s \n Topic 25: war, people, documentary, us, film \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 41 (approx. per word bound = -7.662, relative change = 1.739e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 42 (approx. per word bound = -7.662, relative change = 1.755e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 43 (approx. per word bound = -7.662, relative change = 2.082e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 44 (approx. per word bound = -7.662, relative change = 2.056e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 45 (approx. per word bound = -7.662, relative change = 1.820e-05) \nTopic 1: film, characters, one, like, just \n Topic 2: movie, film, good, story, one \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, one, story, woman, films \n Topic 5: one, best, show, character, episode \n Topic 6: movie, great, one, film, also \n Topic 7: movie, film, good, one, interesting \n Topic 8: film, one, like, story, just \n Topic 9: film, one, family, story, love \n Topic 10: movie, film, got, like, one \n Topic 11: good, film, time, one, even \n Topic 12: film, one, life, like, young \n Topic 13: film, one, films, like, movie \n Topic 14: movie, bad, just, like, even \n Topic 15: film, one, much, like, zombie \n Topic 16: film, movie, one, just, like \n Topic 17: film, bad, movie, just, one \n Topic 18: show, movie, like, just, tv \n Topic 19: film, movie, one, much, game \n Topic 20: one, film, match, man, also \n Topic 21: one, movie, film, movies, seen \n Topic 22: film, just, one, films, like \n Topic 23: new, one, film, movie, like \n Topic 24: movie, film, love, one, s \n Topic 25: war, people, us, documentary, film \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 46 (approx. per word bound = -7.662, relative change = 1.720e-05) \n....................................................................................................\nCompleted E-Step (2 seconds). \nCompleted M-Step. \nCompleting Iteration 47 (approx. per word bound = -7.662, relative change = 1.593e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 48 (approx. per word bound = -7.661, relative change = 1.451e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 49 (approx. per word bound = -7.661, relative change = 1.298e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 50 (approx. per word bound = -7.661, relative change = 1.353e-05) \nTopic 1: film, characters, one, like, just \n Topic 2: movie, film, good, story, one \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, one, story, woman, films \n Topic 5: one, best, show, character, series \n Topic 6: movie, great, one, film, also \n Topic 7: movie, film, good, one, interesting \n Topic 8: film, one, like, story, just \n Topic 9: film, one, family, story, love \n Topic 10: movie, film, got, like, one \n Topic 11: good, film, time, one, even \n Topic 12: film, one, life, like, young \n Topic 13: film, one, films, like, movie \n Topic 14: movie, bad, just, like, even \n Topic 15: film, one, much, like, zombie \n Topic 16: film, movie, one, just, like \n Topic 17: film, bad, movie, just, one \n Topic 18: show, like, movie, just, tv \n Topic 19: film, movie, one, much, game \n Topic 20: one, film, match, man, also \n Topic 21: one, movie, film, movies, seen \n Topic 22: film, just, one, films, like \n Topic 23: new, one, film, movie, like \n Topic 24: movie, film, love, one, s \n Topic 25: war, people, us, documentary, film \n....................................................................................................\nCompleted E-Step (2 seconds). \nCompleted M-Step. \nCompleting Iteration 51 (approx. per word bound = -7.661, relative change = 1.297e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 52 (approx. per word bound = -7.661, relative change = 1.293e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 53 (approx. per word bound = -7.661, relative change = 1.279e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 54 (approx. per word bound = -7.661, relative change = 1.184e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 55 (approx. per word bound = -7.661, relative change = 1.212e-05) \nTopic 1: film, characters, one, like, just \n Topic 2: movie, film, good, story, great \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, one, story, woman, films \n Topic 5: one, best, show, character, series \n Topic 6: movie, great, one, film, also \n Topic 7: movie, film, good, one, interesting \n Topic 8: film, one, like, story, just \n Topic 9: film, one, family, love, story \n Topic 10: movie, film, got, like, one \n Topic 11: good, film, time, one, even \n Topic 12: film, one, life, like, young \n Topic 13: film, one, films, like, movie \n Topic 14: movie, bad, just, like, even \n Topic 15: film, one, much, like, zombie \n Topic 16: film, movie, one, just, like \n Topic 17: film, bad, movie, just, one \n Topic 18: show, like, movie, just, tv \n Topic 19: film, movie, one, much, game \n Topic 20: one, film, match, man, also \n Topic 21: one, movie, film, movies, seen \n Topic 22: film, just, one, films, like \n Topic 23: new, one, film, movie, like \n Topic 24: movie, film, love, one, s \n Topic 25: war, people, us, documentary, film \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 56 (approx. per word bound = -7.661, relative change = 1.230e-05) \n....................................................................................................\nCompleted E-Step (2 seconds). \nCompleted M-Step. \nCompleting Iteration 57 (approx. per word bound = -7.661, relative change = 1.332e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 58 (approx. per word bound = -7.660, relative change = 1.211e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 59 (approx. per word bound = -7.660, relative change = 1.078e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 60 (approx. per word bound = -7.660, relative change = 1.069e-05) \nTopic 1: film, characters, one, like, just \n Topic 2: movie, film, good, story, great \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, one, story, woman, films \n Topic 5: one, best, character, show, series \n Topic 6: movie, great, one, film, also \n Topic 7: movie, film, good, one, interesting \n Topic 8: film, one, like, story, just \n Topic 9: film, one, family, love, story \n Topic 10: movie, film, got, like, one \n Topic 11: good, film, time, one, even \n Topic 12: film, one, life, like, young \n Topic 13: film, one, films, like, movie \n Topic 14: movie, bad, just, like, even \n Topic 15: film, one, much, like, zombie \n Topic 16: film, movie, one, just, like \n Topic 17: film, bad, movie, just, one \n Topic 18: show, like, movie, just, tv \n Topic 19: film, movie, one, much, game \n Topic 20: one, film, match, man, also \n Topic 21: one, movie, film, movies, seen \n Topic 22: film, just, one, films, like \n Topic 23: new, one, film, movie, like \n Topic 24: movie, film, love, one, s \n Topic 25: war, people, us, film, documentary \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 61 (approx. per word bound = -7.660, relative change = 1.086e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 62 (approx. per word bound = -7.660, relative change = 1.030e-05) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nModel Converged \nBeginning Spectral Initialization \n     Calculating the gram matrix...\n     Using only 10000 most frequent terms during initialization...\n     Finding anchor words...\n    ..................................................\n     Recovering initialization...\n    ....................................................................................................\nInitialization complete.\n....................................................................................................\nCompleted E-Step (11 seconds). \nCompleted M-Step. \nCompleting Iteration 1 (approx. per word bound = -8.567) \n....................................................................................................\nCompleted E-Step (9 seconds). \nCompleted M-Step. \nCompleting Iteration 2 (approx. per word bound = -7.656, relative change = 1.064e-01) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 3 (approx. per word bound = -7.526, relative change = 1.699e-02) \n....................................................................................................\nCompleted E-Step (7 seconds). \nCompleted M-Step. \nCompleting Iteration 4 (approx. per word bound = -7.491, relative change = 4.614e-03) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 5 (approx. per word bound = -7.475, relative change = 2.157e-03) \nTopic 1: film, characters, br, &lt;, &gt; \n Topic 2: movie, actors, director, disappointed, good \n Topic 3: &lt;, &gt;, br, film, one \n Topic 4: film, excellent, woman, makes, one \n Topic 5: best, character, episode, one, show \n Topic 6: movie, great, one, made, story \n Topic 7: movie, film, interesting, good, really \n Topic 8: film, think, br, one, &lt; \n Topic 9: film, &lt;, br, &gt;, family \n Topic 10: movie, film, never, story, got \n Topic 11: time, good, one, film, actually \n Topic 12: br, &gt;, &lt;, film, one \n Topic 13: br, &gt;, &lt;, film, lost \n Topic 14: movie, &gt;, br, &lt;, just \n Topic 15: &gt;, br, &lt;, film, one \n Topic 16: movie, just, one, like, film \n Topic 17: movie, bad, get, one, just \n Topic 18: movie, like, just, tv, good \n Topic 19: &gt;, br, &lt;, movie, much \n Topic 20: br, &gt;, &lt;, match, one \n Topic 21: movie, one, movies, &gt;, br \n Topic 22: film, just, &gt;, &lt;, br \n Topic 23: &lt;, br, &gt;, new, one \n Topic 24: movie, &gt;, br, &lt;, love \n Topic 25: &gt;, br, &lt;, movie, like \n Topic 26: film, can, good, like, see \n Topic 27: br, &gt;, &lt;, series, original \n Topic 28: &gt;, &lt;, br, one, movie \n Topic 29: film, br, &gt;, &lt;, films \n Topic 30: one, like, movie, &lt;, br \n Topic 31: good, movie, bad, film, &lt; \n Topic 32: movie, just, like, really, one \n Topic 33: &gt;, br, &lt;, movie, see \n Topic 34: film, horror, br, &gt;, &lt; \n Topic 35: &lt;, &gt;, br, people, like \n Topic 36: &gt;, br, &lt;, like, one \n Topic 37: movie, director, actors, &gt;, br \n Topic 38: show, &gt;, br, &lt;, one \n Topic 39: book, movie, &lt;, br, &gt; \n Topic 40: film, show, br, &gt;, &lt; \n Topic 41: br, &lt;, &gt;, film, story \n Topic 42: film, br, &gt;, &lt;, one \n Topic 43: movie, one, just, br, &gt; \n Topic 44: br, &gt;, &lt;, show, one \n Topic 45: film, br, &gt;, &lt;, one \n Topic 46: &lt;, br, &gt;, film, one \n Topic 47: film, &lt;, &gt;, br, one \n Topic 48: &gt;, br, &lt;, life, movie \n Topic 49: film, &gt;, br, &lt;, one \n Topic 50: church, movie, joseph, smith, wife \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 6 (approx. per word bound = -7.466, relative change = 1.232e-03) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 7 (approx. per word bound = -7.460, relative change = 7.705e-04) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 8 (approx. per word bound = -7.456, relative change = 4.905e-04) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 9 (approx. per word bound = -7.454, relative change = 3.398e-04) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 10 (approx. per word bound = -7.452, relative change = 2.431e-04) \nTopic 1: film, characters, script, just, even \n Topic 2: movie, actors, director, film, disappointed \n Topic 3: &lt;, &gt;, br, film, one \n Topic 4: film, excellent, woman, story, one \n Topic 5: best, character, one, film, episode \n Topic 6: movie, great, one, made, film \n Topic 7: movie, film, good, interesting, really \n Topic 8: film, one, think, also, plot \n Topic 9: film, &lt;, br, &gt;, family \n Topic 10: movie, film, never, story, got \n Topic 11: time, good, film, one, actually \n Topic 12: br, &gt;, &lt;, film, one \n Topic 13: br, &gt;, &lt;, film, lost \n Topic 14: movie, &gt;, br, &lt;, see \n Topic 15: film, one, much, br, &gt; \n Topic 16: movie, just, one, film, like \n Topic 17: movie, bad, get, one, like \n Topic 18: movie, like, just, good, tv \n Topic 19: &gt;, br, &lt;, movie, film \n Topic 20: match, br, &gt;, &lt;, one \n Topic 21: movie, one, movies, film, seen \n Topic 22: film, just, time, one, like \n Topic 23: new, one, &lt;, br, &gt; \n Topic 24: movie, love, film, br, &gt; \n Topic 25: &gt;, br, &lt;, movie, like \n Topic 26: film, can, good, see, like \n Topic 27: series, br, &gt;, &lt;, original \n Topic 28: one, movie, comedy, film, funny \n Topic 29: film, films, br, &gt;, &lt; \n Topic 30: one, like, movie, film, see \n Topic 31: good, movie, bad, film, scenes \n Topic 32: movie, just, like, really, one \n Topic 33: &gt;, br, &lt;, movie, film \n Topic 34: film, horror, just, one, films \n Topic 35: &lt;, &gt;, br, people, documentary \n Topic 36: &gt;, br, &lt;, like, one \n Topic 37: movie, director, good, actors, one \n Topic 38: show, one, episode, like, season \n Topic 39: book, movie, novel, read, &lt; \n Topic 40: film, show, like, one, even \n Topic 41: film, br, &lt;, &gt;, story \n Topic 42: film, one, br, &gt;, &lt; \n Topic 43: movie, one, just, film, like \n Topic 44: br, &gt;, &lt;, show, one \n Topic 45: film, one, make, heart, story \n Topic 46: &lt;, br, &gt;, film, one \n Topic 47: film, one, man, just, cast \n Topic 48: &gt;, br, &lt;, life, movie \n Topic 49: film, one, &gt;, br, &lt; \n Topic 50: church, smith, movie, joseph, much \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 11 (approx. per word bound = -7.450, relative change = 1.826e-04) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 12 (approx. per word bound = -7.449, relative change = 1.400e-04) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 13 (approx. per word bound = -7.449, relative change = 1.069e-04) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 14 (approx. per word bound = -7.448, relative change = 8.472e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 15 (approx. per word bound = -7.447, relative change = 7.078e-05) \nTopic 1: film, characters, script, just, even \n Topic 2: movie, film, actors, director, one \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, excellent, story, one, woman \n Topic 5: best, character, one, film, great \n Topic 6: movie, great, one, film, made \n Topic 7: movie, film, good, interesting, really \n Topic 8: film, one, think, also, plot \n Topic 9: film, &lt;, br, &gt;, family \n Topic 10: film, movie, never, story, got \n Topic 11: time, good, film, one, actually \n Topic 12: film, one, life, show, may \n Topic 13: film, lost, br, one, &gt; \n Topic 14: movie, see, just, bad, like \n Topic 15: film, one, much, 2, even \n Topic 16: movie, just, film, one, like \n Topic 17: bad, movie, get, one, like \n Topic 18: movie, like, just, good, tv \n Topic 19: &gt;, br, &lt;, movie, film \n Topic 20: match, one, br, &lt;, &gt; \n Topic 21: movie, one, movies, film, seen \n Topic 22: film, just, time, one, like \n Topic 23: new, one, film, joe, movie \n Topic 24: movie, love, film, s, one \n Topic 25: &gt;, br, &lt;, movie, now \n Topic 26: film, can, good, see, like \n Topic 27: series, original, like, one, film \n Topic 28: one, movie, comedy, film, funny \n Topic 29: film, films, one, best, br \n Topic 30: one, like, movie, film, see \n Topic 31: good, movie, film, bad, scenes \n Topic 32: movie, just, like, really, one \n Topic 33: movie, &gt;, br, &lt;, film \n Topic 34: film, horror, one, just, films \n Topic 35: &lt;, &gt;, br, people, documentary \n Topic 36: &gt;, br, &lt;, like, one \n Topic 37: movie, director, film, good, one \n Topic 38: show, episode, one, like, season \n Topic 39: book, movie, novel, read, film \n Topic 40: film, show, like, one, even \n Topic 41: film, story, br, &lt;, &gt; \n Topic 42: film, one, get, just, real \n Topic 43: movie, one, just, film, like \n Topic 44: show, one, film, br, &gt; \n Topic 45: film, one, heart, make, story \n Topic 46: &lt;, br, &gt;, film, one \n Topic 47: film, one, man, just, cast \n Topic 48: &gt;, br, &lt;, life, movie \n Topic 49: film, one, like, hero, films \n Topic 50: church, movie, smith, joseph, life \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 16 (approx. per word bound = -7.447, relative change = 5.617e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 17 (approx. per word bound = -7.447, relative change = 4.848e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 18 (approx. per word bound = -7.446, relative change = 5.124e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 19 (approx. per word bound = -7.446, relative change = 5.808e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 20 (approx. per word bound = -7.445, relative change = 5.292e-05) \nTopic 1: film, characters, script, even, just \n Topic 2: movie, film, actors, director, one \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, excellent, story, one, woman \n Topic 5: best, character, film, one, well \n Topic 6: movie, great, one, film, story \n Topic 7: movie, film, good, interesting, really \n Topic 8: film, one, think, also, plot \n Topic 9: film, &lt;, br, &gt;, family \n Topic 10: film, movie, never, story, got \n Topic 11: time, good, film, one, actually \n Topic 12: film, one, life, may, show \n Topic 13: film, lost, one, films, like \n Topic 14: movie, see, bad, just, people \n Topic 15: film, one, much, 2, even \n Topic 16: movie, film, just, one, like \n Topic 17: bad, movie, get, one, like \n Topic 18: movie, like, just, good, tv \n Topic 19: &gt;, br, &lt;, film, movie \n Topic 20: match, one, rock, wwe, ring \n Topic 21: one, movie, movies, film, seen \n Topic 22: film, just, time, one, like \n Topic 23: new, one, film, joe, movie \n Topic 24: movie, love, film, s, one \n Topic 25: &gt;, br, &lt;, movie, now \n Topic 26: film, can, good, like, see \n Topic 27: series, original, like, film, one \n Topic 28: one, comedy, film, movie, funny \n Topic 29: film, films, one, best, much \n Topic 30: one, like, movie, film, see \n Topic 31: good, movie, film, bad, scenes \n Topic 32: movie, just, like, really, one \n Topic 33: movie, film, see, also, good \n Topic 34: film, horror, one, just, films \n Topic 35: &lt;, &gt;, br, people, documentary \n Topic 36: &gt;, br, &lt;, like, one \n Topic 37: movie, director, film, good, one \n Topic 38: show, episode, one, like, episodes \n Topic 39: book, movie, novel, read, film \n Topic 40: film, show, like, one, even \n Topic 41: film, story, one, role, br \n Topic 42: film, one, get, real, just \n Topic 43: movie, one, just, film, like \n Topic 44: show, one, film, stage, can \n Topic 45: film, one, heart, make, story \n Topic 46: &lt;, br, &gt;, film, one \n Topic 47: film, one, man, cast, just \n Topic 48: &gt;, br, &lt;, life, movie \n Topic 49: film, one, like, hero, films \n Topic 50: movie, church, smith, joseph, recommend \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 21 (approx. per word bound = -7.445, relative change = 4.422e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 22 (approx. per word bound = -7.445, relative change = 4.272e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 23 (approx. per word bound = -7.444, relative change = 3.972e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 24 (approx. per word bound = -7.444, relative change = 3.983e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 25 (approx. per word bound = -7.444, relative change = 4.210e-05) \nTopic 1: film, characters, script, even, just \n Topic 2: movie, film, actors, director, one \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, excellent, one, story, woman \n Topic 5: best, character, film, one, well \n Topic 6: movie, great, one, film, story \n Topic 7: movie, film, good, interesting, really \n Topic 8: film, one, think, also, plot \n Topic 9: film, &lt;, br, &gt;, family \n Topic 10: film, movie, never, story, got \n Topic 11: time, good, film, one, actually \n Topic 12: film, one, life, may, young \n Topic 13: film, lost, one, films, like \n Topic 14: movie, see, bad, just, people \n Topic 15: film, one, much, 2, even \n Topic 16: movie, film, one, just, like \n Topic 17: bad, movie, get, one, like \n Topic 18: movie, like, just, tv, good \n Topic 19: film, movie, one, much, &gt; \n Topic 20: match, one, rock, wwe, ring \n Topic 21: one, movie, movies, film, seen \n Topic 22: film, just, time, one, like \n Topic 23: new, one, film, joe, movie \n Topic 24: movie, love, film, s, one \n Topic 25: &gt;, br, &lt;, movie, now \n Topic 26: film, can, good, way, like \n Topic 27: series, original, film, one, like \n Topic 28: one, comedy, film, movie, funny \n Topic 29: film, films, one, best, also \n Topic 30: one, like, movie, film, see \n Topic 31: good, film, movie, bad, scenes \n Topic 32: movie, just, like, really, one \n Topic 33: movie, film, see, also, good \n Topic 34: film, horror, one, just, films \n Topic 35: people, documentary, like, film, see \n Topic 36: like, film, one, &gt;, br \n Topic 37: movie, director, film, good, one \n Topic 38: show, episode, one, like, episodes \n Topic 39: book, movie, novel, read, film \n Topic 40: film, show, like, one, even \n Topic 41: film, story, one, role, even \n Topic 42: film, one, get, real, just \n Topic 43: movie, one, just, film, like \n Topic 44: show, one, film, stage, can \n Topic 45: film, one, heart, make, story \n Topic 46: &lt;, br, &gt;, film, one \n Topic 47: film, one, man, cast, many \n Topic 48: &gt;, br, &lt;, life, movie \n Topic 49: film, one, like, hero, films \n Topic 50: movie, church, smith, joseph, film \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 26 (approx. per word bound = -7.444, relative change = 3.630e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 27 (approx. per word bound = -7.443, relative change = 3.169e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 28 (approx. per word bound = -7.443, relative change = 2.985e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 29 (approx. per word bound = -7.443, relative change = 2.568e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 30 (approx. per word bound = -7.443, relative change = 2.562e-05) \nTopic 1: film, characters, script, even, just \n Topic 2: movie, film, actors, director, one \n Topic 3: &gt;, &lt;, br, film, one \n Topic 4: film, excellent, one, story, woman \n Topic 5: best, character, film, one, well \n Topic 6: movie, great, one, film, story \n Topic 7: movie, film, good, interesting, really \n Topic 8: film, one, think, also, plot \n Topic 9: film, family, &lt;, br, &gt; \n Topic 10: film, movie, never, story, got \n Topic 11: time, good, film, one, actually \n Topic 12: film, one, life, may, young \n Topic 13: film, lost, one, films, like \n Topic 14: movie, see, bad, just, people \n Topic 15: film, one, much, 2, even \n Topic 16: movie, film, one, just, like \n Topic 17: bad, movie, get, one, like \n Topic 18: movie, like, just, tv, good \n Topic 19: film, movie, one, much, like \n Topic 20: match, one, rock, wwe, back \n Topic 21: one, movie, movies, film, seen \n Topic 22: film, just, time, one, like \n Topic 23: new, one, film, joe, movie \n Topic 24: movie, love, film, s, one \n Topic 25: &gt;, br, &lt;, movie, now \n Topic 26: film, can, good, way, comedy \n Topic 27: series, original, film, one, like \n Topic 28: one, comedy, film, movie, funny \n Topic 29: film, films, one, best, also \n Topic 30: one, like, movie, film, people \n Topic 31: good, film, bad, movie, scenes \n Topic 32: movie, just, like, really, one \n Topic 33: movie, film, see, also, good \n Topic 34: film, horror, one, just, films \n Topic 35: people, like, documentary, film, see \n Topic 36: like, film, one, &gt;, br \n Topic 37: movie, director, film, good, one \n Topic 38: show, episode, one, like, episodes \n Topic 39: book, movie, novel, read, film \n Topic 40: film, show, like, one, even \n Topic 41: film, story, one, role, even \n Topic 42: film, one, get, real, just \n Topic 43: one, movie, just, film, like \n Topic 44: show, one, film, stage, can \n Topic 45: film, one, heart, make, story \n Topic 46: &lt;, br, &gt;, film, one \n Topic 47: film, one, man, cast, many \n Topic 48: life, &gt;, br, &lt;, movie \n Topic 49: film, one, like, hero, films \n Topic 50: movie, church, smith, joseph, film \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 31 (approx. per word bound = -7.443, relative change = 2.257e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 32 (approx. per word bound = -7.442, relative change = 2.386e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 33 (approx. per word bound = -7.442, relative change = 2.667e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 34 (approx. per word bound = -7.442, relative change = 2.594e-05) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 35 (approx. per word bound = -7.442, relative change = 2.436e-05) \nTopic 1: film, characters, script, even, just \n Topic 2: movie, film, actors, director, one \n Topic 3: &gt;, &lt;, br, film, story \n Topic 4: film, excellent, one, story, woman \n Topic 5: best, character, film, one, well \n Topic 6: great, movie, one, film, story \n Topic 7: movie, film, good, interesting, one \n Topic 8: film, one, think, also, plot \n Topic 9: film, family, one, br, &lt; \n Topic 10: film, movie, never, story, got \n Topic 11: time, good, film, one, actually \n Topic 12: film, one, life, may, young \n Topic 13: film, lost, one, films, like \n Topic 14: movie, see, bad, just, people \n Topic 15: film, one, much, 2, even \n Topic 16: movie, film, one, just, like \n Topic 17: bad, movie, get, one, like \n Topic 18: movie, like, just, tv, lot \n Topic 19: film, movie, one, much, like \n Topic 20: match, one, rock, wwe, back \n Topic 21: one, movie, movies, film, seen \n Topic 22: film, just, time, one, like \n Topic 23: new, one, film, joe, movie \n Topic 24: movie, love, film, s, one \n Topic 25: &gt;, br, &lt;, movie, now \n Topic 26: film, can, good, way, comedy \n Topic 27: series, original, film, one, like \n Topic 28: one, comedy, film, funny, movie \n Topic 29: film, films, one, best, also \n Topic 30: one, like, film, movie, people \n Topic 31: good, film, bad, movie, scenes \n Topic 32: movie, just, like, really, one \n Topic 33: movie, film, see, also, good \n Topic 34: film, horror, one, just, films \n Topic 35: people, like, film, documentary, see \n Topic 36: like, film, one, &gt;, br \n Topic 37: movie, director, film, good, one \n Topic 38: show, episode, one, like, episodes \n Topic 39: book, movie, novel, read, film \n Topic 40: film, show, like, one, even \n Topic 41: film, story, role, one, even \n Topic 42: film, one, get, real, just \n Topic 43: one, movie, just, film, david \n Topic 44: show, one, film, stage, can \n Topic 45: film, one, heart, make, story \n Topic 46: &lt;, br, &gt;, film, one \n Topic 47: film, one, man, cast, many \n Topic 48: life, &gt;, movie, br, &lt; \n Topic 49: film, one, hero, like, films \n Topic 50: church, movie, smith, joseph, film \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 36 (approx. per word bound = -7.442, relative change = 2.246e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 37 (approx. per word bound = -7.442, relative change = 2.028e-05) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 38 (approx. per word bound = -7.441, relative change = 1.628e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 39 (approx. per word bound = -7.441, relative change = 2.086e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 40 (approx. per word bound = -7.441, relative change = 1.658e-05) \nTopic 1: film, characters, script, even, just \n Topic 2: movie, film, actors, director, one \n Topic 3: &gt;, &lt;, br, film, story \n Topic 4: film, excellent, one, story, woman \n Topic 5: best, character, film, one, well \n Topic 6: great, movie, one, film, story \n Topic 7: film, movie, interesting, good, one \n Topic 8: film, one, think, also, plot \n Topic 9: film, family, one, love, br \n Topic 10: film, movie, never, story, got \n Topic 11: time, good, film, one, actually \n Topic 12: film, one, life, may, young \n Topic 13: film, lost, one, films, like \n Topic 14: movie, see, bad, just, people \n Topic 15: film, one, much, 2, even \n Topic 16: movie, film, one, just, like \n Topic 17: bad, movie, get, one, like \n Topic 18: movie, like, just, tv, lot \n Topic 19: film, movie, one, much, like \n Topic 20: match, one, rock, wwe, back \n Topic 21: one, movie, movies, film, seen \n Topic 22: film, just, time, one, like \n Topic 23: new, one, film, joe, movie \n Topic 24: movie, love, film, s, one \n Topic 25: &gt;, br, &lt;, movie, now \n Topic 26: film, can, good, way, comedy \n Topic 27: series, original, film, one, like \n Topic 28: one, comedy, film, funny, movie \n Topic 29: film, films, one, best, also \n Topic 30: one, like, film, movie, people \n Topic 31: good, film, bad, movie, scenes \n Topic 32: movie, just, like, really, one \n Topic 33: movie, film, see, also, good \n Topic 34: film, horror, one, just, films \n Topic 35: people, film, like, documentary, see \n Topic 36: like, film, one, just, game \n Topic 37: movie, director, film, good, one \n Topic 38: show, episode, one, like, episodes \n Topic 39: book, movie, novel, read, film \n Topic 40: film, show, like, one, even \n Topic 41: film, story, role, one, even \n Topic 42: film, one, get, real, just \n Topic 43: one, movie, just, film, david \n Topic 44: show, one, film, stage, can \n Topic 45: film, one, heart, make, story \n Topic 46: &lt;, br, &gt;, film, one \n Topic 47: film, one, man, cast, many \n Topic 48: life, movie, film, one, &gt; \n Topic 49: film, one, hero, like, films \n Topic 50: church, movie, smith, joseph, film \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 41 (approx. per word bound = -7.441, relative change = 1.891e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 42 (approx. per word bound = -7.441, relative change = 1.799e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 43 (approx. per word bound = -7.441, relative change = 1.672e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 44 (approx. per word bound = -7.441, relative change = 1.742e-05) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 45 (approx. per word bound = -7.440, relative change = 1.596e-05) \nTopic 1: film, characters, script, even, like \n Topic 2: movie, film, actors, director, one \n Topic 3: &gt;, &lt;, br, film, story \n Topic 4: film, excellent, one, story, woman \n Topic 5: best, character, film, one, well \n Topic 6: great, movie, one, film, story \n Topic 7: film, movie, interesting, good, one \n Topic 8: film, one, think, also, plot \n Topic 9: film, family, one, love, story \n Topic 10: film, movie, never, story, got \n Topic 11: time, good, film, one, actually \n Topic 12: film, one, life, may, young \n Topic 13: film, lost, one, films, like \n Topic 14: movie, see, bad, people, just \n Topic 15: film, one, much, 2, even \n Topic 16: movie, film, one, just, like \n Topic 17: bad, movie, get, one, like \n Topic 18: movie, like, just, tv, lot \n Topic 19: film, movie, one, much, like \n Topic 20: match, one, rock, wwe, back \n Topic 21: one, movies, movie, film, seen \n Topic 22: film, just, time, one, like \n Topic 23: new, one, film, joe, time \n Topic 24: movie, love, film, s, one \n Topic 25: &gt;, br, &lt;, movie, now \n Topic 26: film, can, good, way, comedy \n Topic 27: series, original, film, one, like \n Topic 28: one, comedy, film, funny, movie \n Topic 29: film, films, one, best, also \n Topic 30: one, like, film, movie, people \n Topic 31: good, film, bad, movie, scenes \n Topic 32: movie, just, like, really, one \n Topic 33: film, movie, see, also, good \n Topic 34: film, horror, one, just, films \n Topic 35: people, film, like, documentary, see \n Topic 36: like, film, one, just, game \n Topic 37: movie, director, film, good, one \n Topic 38: show, episode, one, like, episodes \n Topic 39: book, novel, movie, read, film \n Topic 40: film, show, like, one, even \n Topic 41: film, story, role, one, even \n Topic 42: film, one, get, real, just \n Topic 43: one, movie, just, film, david \n Topic 44: show, one, film, stage, can \n Topic 45: film, one, heart, make, story \n Topic 46: film, &lt;, br, &gt;, one \n Topic 47: film, one, man, cast, many \n Topic 48: life, movie, film, one, story \n Topic 49: film, one, hero, like, films \n Topic 50: church, movie, smith, joseph, film \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 46 (approx. per word bound = -7.440, relative change = 1.206e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 47 (approx. per word bound = -7.440, relative change = 2.095e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 48 (approx. per word bound = -7.440, relative change = 2.575e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 49 (approx. per word bound = -7.440, relative change = 1.747e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 50 (approx. per word bound = -7.440, relative change = 1.878e-05) \nTopic 1: film, characters, script, even, character \n Topic 2: movie, film, director, actors, one \n Topic 3: &gt;, &lt;, br, film, well \n Topic 4: film, excellent, one, story, woman \n Topic 5: best, character, film, one, well \n Topic 6: great, movie, one, film, story \n Topic 7: film, movie, interesting, good, one \n Topic 8: film, one, think, also, plot \n Topic 9: film, family, one, love, story \n Topic 10: film, movie, never, story, got \n Topic 11: time, good, film, one, actually \n Topic 12: film, one, life, may, young \n Topic 13: film, lost, one, films, like \n Topic 14: movie, see, bad, people, just \n Topic 15: film, one, much, 2, even \n Topic 16: movie, film, one, just, like \n Topic 17: bad, movie, get, one, like \n Topic 18: movie, like, just, tv, lot \n Topic 19: film, movie, one, much, like \n Topic 20: match, one, rock, wwe, back \n Topic 21: one, movies, movie, film, seen \n Topic 22: film, just, time, one, like \n Topic 23: new, one, film, joe, time \n Topic 24: movie, love, film, s, one \n Topic 25: &gt;, br, &lt;, movie, now \n Topic 26: film, can, good, way, comedy \n Topic 27: series, original, film, one, like \n Topic 28: one, comedy, film, funny, laugh \n Topic 29: film, films, one, best, also \n Topic 30: one, like, film, movie, people \n Topic 31: good, film, bad, movie, scenes \n Topic 32: movie, just, like, really, one \n Topic 33: film, movie, also, see, good \n Topic 34: film, horror, one, just, films \n Topic 35: people, film, like, documentary, see \n Topic 36: like, film, one, just, game \n Topic 37: movie, director, film, good, one \n Topic 38: show, episode, one, like, episodes \n Topic 39: book, novel, movie, read, film \n Topic 40: film, show, like, one, even \n Topic 41: film, story, role, one, even \n Topic 42: film, one, get, real, just \n Topic 43: one, movie, just, film, david \n Topic 44: show, one, film, stage, can \n Topic 45: film, one, heart, make, story \n Topic 46: film, &lt;, br, &gt;, one \n Topic 47: film, one, man, cast, many \n Topic 48: life, movie, film, one, story \n Topic 49: film, one, hero, films, like \n Topic 50: movie, church, smith, joseph, film \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 51 (approx. per word bound = -7.440, relative change = 1.428e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 52 (approx. per word bound = -7.440, relative change = 1.561e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 53 (approx. per word bound = -7.439, relative change = 1.422e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 54 (approx. per word bound = -7.439, relative change = 1.317e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 55 (approx. per word bound = -7.439, relative change = 1.148e-05) \nTopic 1: film, characters, script, even, character \n Topic 2: movie, film, director, actors, one \n Topic 3: &gt;, &lt;, br, film, well \n Topic 4: film, excellent, one, story, woman \n Topic 5: best, character, film, one, great \n Topic 6: great, movie, one, film, story \n Topic 7: film, movie, interesting, good, one \n Topic 8: film, one, think, also, plot \n Topic 9: film, family, one, love, story \n Topic 10: film, movie, never, story, got \n Topic 11: time, good, film, one, actually \n Topic 12: film, one, life, may, young \n Topic 13: film, lost, one, films, like \n Topic 14: movie, see, bad, people, just \n Topic 15: film, one, much, 2, even \n Topic 16: movie, film, one, just, like \n Topic 17: bad, movie, get, one, like \n Topic 18: movie, like, just, tv, lot \n Topic 19: film, one, movie, much, like \n Topic 20: match, one, rock, wwe, back \n Topic 21: one, movies, movie, film, seen \n Topic 22: film, just, time, one, like \n Topic 23: new, one, film, joe, time \n Topic 24: movie, love, film, s, one \n Topic 25: &gt;, br, &lt;, movie, now \n Topic 26: film, can, way, comedy, good \n Topic 27: series, original, film, one, like \n Topic 28: one, comedy, film, funny, laugh \n Topic 29: film, films, one, best, also \n Topic 30: one, like, film, movie, people \n Topic 31: good, film, bad, movie, scenes \n Topic 32: movie, just, like, really, one \n Topic 33: film, movie, also, see, good \n Topic 34: film, horror, one, just, films \n Topic 35: people, film, like, documentary, see \n Topic 36: film, like, one, just, game \n Topic 37: movie, director, film, good, one \n Topic 38: show, episode, one, like, episodes \n Topic 39: book, novel, movie, read, film \n Topic 40: film, show, like, one, even \n Topic 41: film, story, role, one, acting \n Topic 42: film, one, get, real, just \n Topic 43: one, movie, just, film, david \n Topic 44: show, one, film, stage, can \n Topic 45: film, one, heart, make, story \n Topic 46: film, &lt;, br, &gt;, one \n Topic 47: film, one, man, cast, many \n Topic 48: life, movie, film, one, story \n Topic 49: film, one, hero, films, like \n Topic 50: movie, church, smith, people, joseph \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 56 (approx. per word bound = -7.439, relative change = 1.079e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 57 (approx. per word bound = -7.439, relative change = 1.084e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nModel Converged \n\nplot(differentKs)\n\n\n\n\nThe plot is a mixed bag for us. Higher values of the held-out likelihood and semantic coherence both indicate better models, while lower values of residuals indicates a better model. It’s also important to note that it’s artificially easy to get more semantic coherence by having fewer topics (semantic coherence is a measure based on how well the top topic words identify the topics). If it was me, I’d probably settle at the midpoint here (25 topics). But there’s no magic solution. Instead, the decision is largely left up to you. That flexibility is nice, but it also means that *you need to be able to defend your choice of K**, because external audiences are going to want to know why you chose the number you did."
  }
]